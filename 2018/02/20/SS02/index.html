<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SS02 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="Transformations on DStreams 123updateStateByKey：先看一个案例  123[double_happy@hadoop101 ~]$ nc -lk 9999a,a,a,d,da,a,a,d,d  1234567891011121314151617181920212223242526272829303132333435363738394041424344454">
<meta property="og:type" content="article">
<meta property="og:title" content="SS02">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;20&#x2F;SS02&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="Transformations on DStreams 123updateStateByKey：先看一个案例  123[double_happy@hadoop101 ~]$ nc -lk 9999a,a,a,d,da,a,a,d,d  1234567891011121314151617181920212223242526272829303132333435363738394041424344454">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031192844428.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019103120380049.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031205307726.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031210724578.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031211641822.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031213004342.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031220146737.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031220239500.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:11:52.069Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031192844428.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SS02" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SS02
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/20/SS02/" class="article-date">
  <time datetime="2018-02-20T12:11:27.000Z" itemprop="datePublished">2018-02-20</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams" target="_blank" rel="noopener">Transformations on DStreams</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">updateStateByKey：</span><br><span class="line"></span><br><span class="line">先看一个案例</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,a,d,d</span><br><span class="line">a,a,a,d,d</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572519050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572519060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572519070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">这个计算 只计算当前批次的 之后批次 没有数据 </span><br><span class="line"></span><br><span class="line">需求：</span><br><span class="line">	统计 从现在时间点 到 10分钟之后的 a出现的次数  ？对于</span><br><span class="line">	上面的代码是无法满足 的    (也可以满足 存起来 再加 也可以)</span><br><span class="line"></span><br><span class="line">对于累计的需求该这么办呢？</span><br><span class="line"></span><br><span class="line">这就引出一个有没有状态的问题。</span><br><span class="line"></span><br><span class="line">状态：State</span><br><span class="line">    无状态的        只与当前批次有关的 叫无状态</span><br><span class="line">    有状态的        前后批次是有关系的   eg：需要把之前的历史到当前的时间点 需要累计起来</span><br><span class="line"></span><br><span class="line">实现有状态的 需求 使用updateStateByKey算子***</span><br><span class="line"></span><br><span class="line">updateStateByKey ：更新你的状态 通过key 来更新   怎么更新 传入一个function 即可 eg:累加 还是别的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">updateStateByKey(func)	：</span><br><span class="line">	Return a new &quot;state&quot; DStream where the state for each key is updated </span><br><span class="line">	by applying the given function on the previous state of the key </span><br><span class="line">	and the new values for the key. </span><br><span class="line">	This can be used to maintain arbitrary state data for each key.</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#updatestatebykey-operation" target="_blank" rel="noopener">UpdateStateByKey Operation</a></p>
<p>The updateStateByKey operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.</p>
<p>   1.Define the state - The state can be an arbitrary data type.</p>
<p>   2.Define the state update function - Specify with a function how to update the state<br>     using the <strong>previous state and the new values from an input stream.</strong></p>
<p>In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p>
<p>Let’s illustrate this with an example. Say you want to maintain a running count of each word seen in a text data stream. Here, the running count is the state and it is an integer. We define the update function as:</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">updateStateByKey operation ：</span><br><span class="line">	1.Define the state</span><br><span class="line">	2.Define the state update function</span><br><span class="line"></span><br><span class="line">对于上面给的wc例子 ：</span><br><span class="line">哪个东西是state    应该是 value </span><br><span class="line"></span><br><span class="line">updateStateByKey  通过key 来更新谁 ( 你可以这么理解)</span><br></pre></td></tr></table></figure></div>
<p>案例：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. </span><br><span class="line"> val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .reduceByKey(_+_)</span><br><span class="line">      </span><br><span class="line">reduceByKey(_+_)  是对当前批次的累计 所以这里不能这么写</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *</span><br><span class="line">    * 1批次：  a a a d d</span><br><span class="line">    * 2批次：  b b b c c a</span><br><span class="line">    *</span><br><span class="line">    *newValues : 当前批次的值</span><br><span class="line">    *           key对应的新值(或者有新的key)  可能有多个 所以是一个Seq</span><br><span class="line">    * preValues : 以前批次的累加值</span><br><span class="line">    *             key已经存在的值  有可能没有 有可能有  所以定义成Option  有就返回some  没有返回none</span><br><span class="line">    *</span><br><span class="line">    */</span><br><span class="line">  def updateFunction(newValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    //newValues : (b,1)(b,1)(b,1)(c,1)(c,1) (a,1)</span><br><span class="line"></span><br><span class="line">    val curr = newValues.sum // 当前批次</span><br><span class="line">    val pre = preValues.getOrElse(0)   //老的值   (a,3) (d,2)   拿出值  key没有的  就赋值为0</span><br><span class="line">    Some(curr + pre)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/10/31 19:21:07 ERROR StreamingContext: Error starting the context, marking it as stopped</span><br><span class="line">java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().</span><br><span class="line">	at scala.Predef$.require(Predef.scala:224)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:243)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at scala.collection.immutable.List.foreach(List.scala:381)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph.start(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.startFirstTime(JobGenerator.scala:194)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:100)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:103)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:583)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01$.main(StreamingWCApp01.scala:19)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01.main(StreamingWCApp01.scala)</span><br><span class="line">19/10/31 19:21:08 WARN ReceiverSupervisorImpl: Skip stopping receiver because it has not yet stared</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().</span><br><span class="line">	at scala.Predef$.require(Predef.scala:224)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:243)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at scala.collection.immutable.List.foreach(List.scala:381)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph.start(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.startFirstTime(JobGenerator.scala:194)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:100)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:103)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:583)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01$.main(StreamingWCApp01.scala:19)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01.main(StreamingWCApp01.scala)</span><br><span class="line"></span><br><span class="line">Process finished with exit code 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Please set it by StreamingContext.checkpoint().</span><br></pre></td></tr></table></figure></div>
<p>修改代码</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a         第一次输入</span><br><span class="line"></span><br><span class="line">a,a,b,b,a         第二次输入</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line">    </span><br><span class="line">    ssc.checkpoint(&quot;file:///C:/IdeaProjects/spark/checkponit&quot;)</span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *</span><br><span class="line">    * 1批次：  a a a d d</span><br><span class="line">    * 2批次：  b b b c c a</span><br><span class="line">    *</span><br><span class="line">    *newValues : 当前批次的值</span><br><span class="line">    *           key对应的新值(或者有新的key)  可能有多个 所以是一个Seq</span><br><span class="line">    * preValues : 以前批次的累加值</span><br><span class="line">    *             key已经存在的值  有可能没有 有可能有  所以定义成Option  有就返回some  没有返回none</span><br><span class="line">    *</span><br><span class="line">    */</span><br><span class="line">  def updateFunction(newValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    //newValues : (b,1)(b,1)(b,1)(c,1)(c,1) (a,1)</span><br><span class="line"></span><br><span class="line">    val curr = newValues.sum // 当前批次</span><br><span class="line">    val pre = preValues.getOrElse(0)   //老的值   (a,3) (d,2)   拿出值  key没有的  就赋值为0</span><br><span class="line">    Some(curr + pre)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 19:24:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:24:13 WARN BlockManager: Block input-0-1572521053200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,2)</span><br><span class="line">(a,3)</span><br><span class="line"></span><br><span class="line">19/10/31 19:24:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:24:25 WARN BlockManager: Block input-0-1572521064800 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521080000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">为什么要checkpoint呢？</span><br><span class="line">之前的代码都是没有设置checkpoint 的 为什么之前不需要设置 呢？</span><br><span class="line">因为之前的是没有状态的 没有状态 就是当前批次处理完就ok了 </span><br><span class="line"></span><br><span class="line">但是现在 需要把当前批次 和 以前批次累加起来的  这个东西在哪里呢？下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031192844428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ok 现在我把程序关掉 重启以后 是多少呢？  </span><br><span class="line">之前值是：</span><br><span class="line">	(b,4)</span><br><span class="line">    (a,6)</span><br><span class="line">重启之后的值是：空的 </span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521460000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521470000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">也就是说 ：</span><br><span class="line">	如果你的作业 中途挂掉了 重启之后 什么都没有了 </span><br><span class="line">为什么呢？</span><br><span class="line">	因为之前的结果写到 checkponit里了 ，而且当前批次 也没有数据输入进来</span><br><span class="line">那么：</span><br><span class="line">	我们有什么办法 把 checkponit里的数据读取出来呢？</span><br><span class="line">	看官网</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">Checkpointing</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">最好直接看官网：我只是截取我认为重要的</span><br><span class="line">Spark Streaming needs to checkpoint enough information </span><br><span class="line">to a fault- tolerant storage system such that it can recover from failures.</span><br><span class="line"> There are two types of data that are checkpointed.</span><br><span class="line"></span><br><span class="line">1. a fault- tolerant storage system    可以选用HDFS</span><br><span class="line">2. two types of data that are checkpointed</span><br><span class="line">      1.Metadata checkpointing</span><br><span class="line">      			Configuration     配置文件</span><br><span class="line">      			DStream operations     算子 </span><br><span class="line">      			Incomplete batches    未完成的</span><br><span class="line">      2.Data checkpointing    就是你真正传过来的数据</span><br><span class="line"></span><br><span class="line">When to enable Checkpointing？</span><br><span class="line">   1.Usage of stateful transformations </span><br><span class="line">   2.Recovering from failures of the driver running the application </span><br><span class="line">   		driver挂了 你的作业就挂了 当你作业挂了 从Checkpoint中恢复</span><br><span class="line"></span><br><span class="line">How to configure Checkpointing？</span><br><span class="line">	看代码   就是说什么代码得改动 不能像之前那样写</span><br><span class="line">1.需要定义一个函数 这个函数就是 创建StreamingContext</span><br><span class="line">2.之后 再 val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _) </span><br><span class="line">才可以解决 重启之后能够拿到之前的值 </span><br><span class="line"></span><br><span class="line">这个就是利用了 ：</span><br><span class="line">	从Checkpoint中恢复 StreamingContext思想(driver 里的 )</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp02 &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  val checkpointDirectory = &quot;file:///C:/IdeaProjects/spark/checkponit&quot;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // 当作业挂了时，从checkpoint中去获取StreamingContext</span><br><span class="line">    val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def functionToCreateContext(): StreamingContext = &#123;</span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line">    ssc.checkpoint(checkpointDirectory)</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *</span><br><span class="line">    * 1)  a a a d d</span><br><span class="line">    * 2)  b b b c c a</span><br><span class="line">    *</span><br><span class="line">    * @param newValues  当前批次的值</span><br><span class="line">    *        key对应的新值  可能有多个 所以是一个Seq</span><br><span class="line">    * @param preValues  以前批次的累加值</span><br><span class="line">    *        key已经存在的值  有可能没有 有可能有  所以定义成Option</span><br><span class="line">    * @return</span><br><span class="line">    */</span><br><span class="line">  def updateFunction(newValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    val curr = newValues.sum // 当前</span><br><span class="line">    val pre = preValues.getOrElse(0)</span><br><span class="line">    Some(curr + pre)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">	-------------------------------------------</span><br><span class="line">Time: 1572521710000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521720000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521730000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521740000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521750000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521760000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">为什么呢？因为 我改动代码了 虽然 checkpoint目录没有变 </span><br><span class="line">先把之前的 checkpoint 目录删掉 再测试 (第一次 之后关闭程序 再重启)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a    第一次输入 </span><br><span class="line">a,a,b,b,a</span><br><span class="line"></span><br><span class="line">a,a,b,b,a   第二次输入</span><br><span class="line">a,a,b,b,a</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523040000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line">19/10/31 19:57:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:57:25 WARN BlockManager: Block input-0-1572523045400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 19:57:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:57:26 WARN BlockManager: Block input-0-1572523046200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">重启后的结果：</span><br><span class="line">	-------------------------------------------</span><br><span class="line">Time: 1572523070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523080000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523090000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523100000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523110000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523120000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523130000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523140000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523150000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523160000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">ok啦</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Stream + Kafka == CP</span><br><span class="line">Kafka 的offset肯定是需要手工维护：有哪些呢？很多的 </span><br><span class="line">    1.checkpoint： 就是把offset维护在checkponit里面的    </span><br><span class="line">    	(代码不能发生任何的变化   只要你代码发生了变化 就意味着 checkpoint 的 matadata 发生了变化  )</span><br><span class="line">    2.Kafka     </span><br><span class="line">    3.ZK   </span><br><span class="line">    4.MySQL    </span><br><span class="line">    5.Redis</span><br><span class="line"></span><br><span class="line">所以生产上 checkpoint 根本没法用  (你的代码怎么可能不变呢？或者不修改呢？所以用不了 )</span><br></pre></td></tr></table></figure></div>

<p><strong>把数据写出去： ****</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">foreachRDD：</span><br><span class="line"></span><br><span class="line">foreachRDD(func)：</span><br><span class="line">	The most generic output operator that applies a function, func, to each RDD </span><br><span class="line">	generated from the stream. </span><br><span class="line">	This function should push the data in each RDD to an external system, </span><br><span class="line">	such as saving the RDD to files, </span><br><span class="line">	or writing it over the network to a database. Note that the function func</span><br><span class="line">	 is executed in the driver process running the streaming application,</span><br><span class="line">	 and will usually have RDD actions in it</span><br><span class="line">	  that will force the computation of the streaming RDDs.</span><br><span class="line"></span><br><span class="line">1. such as saving the RDD to files, </span><br><span class="line">	or writing it over the network to a database.</span><br><span class="line">2.闭包  优雅的方式写出去</span><br><span class="line">3.the function func</span><br><span class="line">	 is executed in the driver process </span><br><span class="line">	 running the streaming application</span><br><span class="line">	 func是运行在driver process的</span><br><span class="line"></span><br><span class="line">driver端到executor端 必然涉及到一个序列化的问题</span><br></pre></td></tr></table></figure></div>
<p><strong>把数据写到MySQL</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MySQL底层引擎有几种？各自什么区别？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019103120380049.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>咱们一步一步来 由劣到优</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    result.foreachRDD( rdd =&gt;&#123;</span><br><span class="line">      val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line"></span><br><span class="line">      rdd.foreach(pair =&gt;&#123;</span><br><span class="line">        val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">        connection.createStatement().execute(sql)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      MySQLUtils.closeResource(connection)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/10/31 20:43:30 ERROR JobScheduler: Error running job streaming job 1572525810000 ms.0</span><br><span class="line">org.apache.spark.SparkException: Task not serializable</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:403)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:393)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)</span><br><span class="line">	at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:926)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:925)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:32)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:29)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.NotSerializableException: java.lang.Object</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: java.lang.Object, value: java.lang.Object@4ffd7c3f)</span><br><span class="line">	- writeObject data (class: java.util.HashMap)</span><br><span class="line">	- object (class java.util.HashMap, &#123;UTF-8=java.lang.Object@4ffd7c3f, US-ASCII=com.mysql.jdbc.SingleByteCharsetConverter@53c22208, WINDOWS-1252=com.mysql.jdbc.SingleByteCharsetConverter@77cd4c6d&#125;)</span><br><span class="line">	- field (class: com.mysql.jdbc.ConnectionImpl, name: charsetConverterMap, type: interface java.util.Map)</span><br><span class="line">	- object (class com.mysql.jdbc.JDBC4Connection, com.mysql.jdbc.JDBC4Connection@65b0d4df)</span><br><span class="line">	- field (class: com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, name: connection$1, type: interface java.sql.Connection)</span><br><span class="line">	- object (class com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, &lt;function1&gt;)</span><br><span class="line">	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:400)</span><br><span class="line">	... 30 more</span><br><span class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Task not serializable</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:403)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:393)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)</span><br><span class="line">	at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:926)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:925)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:32)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:29)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.NotSerializableException: java.lang.Object</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: java.lang.Object, value: java.lang.Object@4ffd7c3f)</span><br><span class="line">	- writeObject data (class: java.util.HashMap)</span><br><span class="line">	- object (class java.util.HashMap, &#123;UTF-8=java.lang.Object@4ffd7c3f, US-ASCII=com.mysql.jdbc.SingleByteCharsetConverter@53c22208, WINDOWS-1252=com.mysql.jdbc.SingleByteCharsetConverter@77cd4c6d&#125;)</span><br><span class="line">	- field (class: com.mysql.jdbc.ConnectionImpl, name: charsetConverterMap, type: interface java.util.Map)</span><br><span class="line">	- object (class com.mysql.jdbc.JDBC4Connection, com.mysql.jdbc.JDBC4Connection@65b0d4df)</span><br><span class="line">	- field (class: com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, name: connection$1, type: interface java.sql.Connection)</span><br><span class="line">	- object (class com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, &lt;function1&gt;)</span><br><span class="line">	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:400)</span><br><span class="line">	... 30 more</span><br><span class="line">19/10/31 20:43:30 WARN SocketReceiver: Error receiving data</span><br><span class="line">java.net.SocketException: Socket closed</span><br><span class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)</span><br><span class="line">	at java.io.InputStreamReader.read(InputStreamReader.java:184)</span><br><span class="line">	at java.io.BufferedReader.fill(BufferedReader.java:161)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:324)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:389)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)</span><br><span class="line">	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)</span><br><span class="line">19/10/31 20:43:30 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data</span><br><span class="line">java.net.SocketException: Socket closed</span><br><span class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)</span><br><span class="line">	at java.io.InputStreamReader.read(InputStreamReader.java:184)</span><br><span class="line">	at java.io.BufferedReader.fill(BufferedReader.java:161)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:324)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:389)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)</span><br><span class="line">	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)</span><br><span class="line">19/10/31 20:43:30 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver</span><br><span class="line">19/10/31 20:43:30 WARN ReceiverSupervisorImpl: Receiver has been stopped</span><br><span class="line">Exception in thread &quot;receiver-supervisor-future-0&quot; java.lang.Error: java.lang.InterruptedException: sleep interrupted</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.lang.InterruptedException: sleep interrupted</span><br><span class="line">	at java.lang.Thread.sleep(Native Method)</span><br><span class="line">	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply$mcV$sp(ReceiverSupervisor.scala:196)</span><br><span class="line">	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)</span><br><span class="line">	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)</span><br><span class="line">	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)</span><br><span class="line">	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	... 2 more</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	org.apache.spark.SparkException: Task not serializable</span><br><span class="line">	ClosureCleaner    Closure 闭包的意思</span><br><span class="line">根本原因是：</span><br><span class="line">	Caused by: java.io.NotSerializableException: java.lang.Object</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: java.lang.Object, value: java.lang.Object@4ffd7c3f)</span><br><span class="line">	- writeObject data (class: java.util.HashMap)</span><br><span class="line">	- object (class java.util.HashMap, &#123;UTF-8=java.lang.Object@4ffd7c3f, US-ASCII=com.mysql.jdbc.SingleByteCharsetConverter@53c22208, WINDOWS-1252=com.mysql.jdbc.SingleByteCharsetConverter@77cd4c6d&#125;)</span><br><span class="line">	- field (class: com.mysql.jdbc.ConnectionImpl, name: charsetConverterMap, type: interface java.util.Map)</span><br><span class="line">	- object (class com.mysql.jdbc.JDBC4Connection, com.mysql.jdbc.JDBC4Connection@65b0d4df)</span><br><span class="line">	- field (class: com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, name: connection$1, type: interface java.sql.Connection)</span><br><span class="line"></span><br><span class="line">就是 object not serializable ：com.mysql.jdbc.SingleByteCharsetConverter </span><br><span class="line">MySQL的驱动不能序列化    但是事实上 MySQL驱动就是序列化不了 </span><br><span class="line"></span><br><span class="line">该怎么办呢？ 看官网  下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031205307726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线————————————————————————————————————</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">上面的错误明白之后 那么什么叫做闭包？</span><br><span class="line">先看一下官网  RDD篇介绍的</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-" target="_blank" rel="noopener">Understanding closures</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">闭包：在函数内部 引用了一个外部的变量 </span><br><span class="line">eg： 这两行代码</span><br><span class="line"></span><br><span class="line">     val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line">      rdd.foreach(pair =&gt;&#123;</span><br><span class="line">        val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">        connection.createStatement().execute(sql)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">foreach 内部使用了 connection  而connection 是在foreach的外部</span><br><span class="line"></span><br><span class="line">如果 假设哈 connection 可以序列化 的  上面这种写法是没有问题的！！！</span><br><span class="line">很不幸 connection objects are rarely transferable across machines</span><br></pre></td></tr></table></figure></div>
<p><strong>修改：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">    result.foreachRDD( rdd =&gt;&#123;</span><br><span class="line">      rdd.foreach(pair =&gt;&#123;</span><br><span class="line">        val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line">        val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">        connection.createStatement().execute(sql)</span><br><span class="line">        MySQLUtils.closeResource(connection)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">connection放到里面去  那么还涉及闭包的问题么？</span><br><span class="line">一定没有闭包的问题了 避免了上次测试 出现的闭包问题</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">没有日志的 因为 foreachRDD 是没有返回值的  只能查看MySQL数据了</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------------------</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">Empty set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |    8 |</span><br><span class="line">| a    |   12 |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line">说明写也是ok的 </span><br><span class="line"></span><br><span class="line">但是也有个问题的？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031210724578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>优化</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">        val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line"></span><br><span class="line">        partition.foreach(pair =&gt; &#123;</span><br><span class="line">          val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">          connection.createStatement().execute(sql)</span><br><span class="line">        &#125;)</span><br><span class="line">        MySQLUtils.closeResource(connection)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |    8 |</span><br><span class="line">| a    |   12 |</span><br><span class="line">| b    |    4 |</span><br><span class="line">| a    |    6 |</span><br><span class="line">+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line">这种方式比前面的好多了 但是也不行 </span><br><span class="line">分区多了  connection也会多  </span><br><span class="line">那么最好的方式是什么呢？拿一个连接池 用完之后返回回去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031211641822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">正确的写法会写了 但是 </span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |    8 |</span><br><span class="line">| a    |   12 |</span><br><span class="line">| b    |    4 |</span><br><span class="line">| a    |    6 |</span><br><span class="line">+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line">结果咋整 写了两次就这样了  怎么解决呢？这是数据问题</span><br></pre></td></tr></table></figure></div>
<p><strong>还有一种写法  建议使用它</strong></p>
<p>scalikejdbc 自带Connection Pool<br><img src="https://img-blog.csdnimg.cn/20191031213004342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    </span><br><span class="line">    DBs.setupAll() //这样就把配置文件解析出来了</span><br><span class="line">    result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">      </span><br><span class="line">        partition.foreach(pair =&gt; &#123;</span><br><span class="line">          DB.autoCommit &#123; implicit session =&gt; &#123;</span><br><span class="line">            SQL(&quot;insert into wc(word,cnt) values(?, ?)&quot;)</span><br><span class="line">              .bind(pair._1,pair._2)</span><br><span class="line">              .update().apply()</span><br><span class="line">          &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |   20 |</span><br><span class="line">| a    |   30 |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">你确定scalikejdbc 默认就使用 连接池么？？？ 留一个坑</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">之前 我们用state 进行累计的  </span><br><span class="line">因为用state累加 会用到checkpoint   checkpoint自己生成小文件一大堆  等等</span><br><span class="line"></span><br><span class="line">那么 不用state 能不能累加？</span><br><span class="line">用redis </span><br><span class="line"> /**</span><br><span class="line">      * WC这种统计维度来说</span><br><span class="line">      * Redis的使用关键点：如何选择合适的数据类型</span><br><span class="line">      */</span><br><span class="line">这里我们选hash</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    /**</span><br><span class="line">      * WC这种统计维度来说</span><br><span class="line">      * Redis的使用关键点：如何选择合适的数据类型</span><br><span class="line">      */</span><br><span class="line">        result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">          rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">            val jedis = RedisUtils.getJedis  // 获取Redis连接</span><br><span class="line">            partition.foreach(pair =&gt; &#123;</span><br><span class="line">              jedis.hincrBy(&quot;doublehappy_redis_wc&quot;, pair._1, pair._2)   //String key, String field, long value</span><br><span class="line">            &#125;)</span><br><span class="line">            jedis.close() // free</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">扩展：</span><br><span class="line">	这里是连接redis  ，那么连接 phoneix 、Cassandra   都一样的 </span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;doublehappy_redis_wc&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031220146737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">再放一些数据</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031220239500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>说明结果ok的哈</p>
<p><strong>transform</strong><br><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#transform-operation" target="_blank" rel="noopener">transform</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">transform(func)	;</span><br><span class="line">Return a new DStream by applying a RDD-to-RDD function </span><br><span class="line">to every RDD of the source DStream.</span><br><span class="line"> This can be used to do arbitrary RDD operations on the DStream.</span><br><span class="line"></span><br><span class="line">之前的编程都是基于DStream</span><br><span class="line">    /**</span><br><span class="line">      * 现在的编程都是基于DStream    生产上绝大多数是DStream</span><br><span class="line">      *</span><br><span class="line">      但是 </span><br><span class="line">      * DStream与RDD互操作咋整？ 使用transform</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">好处就是 把DStream  的RDD 跟我们的RDD进行操作</span><br><span class="line"></span><br><span class="line">需求：</span><br><span class="line">     * 流处理的时候，有一个数据来源于文本或者是其他的   这部分数据是 RDD</span><br><span class="line">      * 另外一个数据是来自Kafka、或者其他的数据源 这部分数据是 DStream</span><br><span class="line">      </span><br><span class="line">做这两个关联  你需要用到 transform</span><br></pre></td></tr></table></figure></div>
<p><strong>例子</strong><br>黑名单<br>目的：<br>只要由黑名单里的东西 把黑名单的数据全部过滤掉 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">先用core的方式;</span><br><span class="line">object CoreBlackListApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(&quot;CoreBlackListApp&quot;)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 构建黑名单  (xx, true) 或者  (xx, 1)</span><br><span class="line">      */</span><br><span class="line">    val blacks = new ListBuffer[(String,Boolean)]()</span><br><span class="line">    blacks.append((&quot;苍老师&quot;,true))  // 鉴黄</span><br><span class="line">    val blacksRDD = sc.parallelize(blacks)  // 把数据转成RDD</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 构建访问日志</span><br><span class="line">      */</span><br><span class="line">    val input = new ListBuffer[(String,String)]</span><br><span class="line">    input.append((&quot;历史第一人&quot;,&quot;被小卡干了，000000&quot;))</span><br><span class="line">    input.append((&quot;日天&quot;,&quot;也被小卡干了，111111&quot;))</span><br><span class="line">    input.append((&quot;苍老师&quot;,&quot;我们敬爱的老师，111111&quot;))</span><br><span class="line">    val inputRDD = sc.parallelize(input)</span><br><span class="line"></span><br><span class="line">    //TODO... 想从访问日志中过滤掉“苍老师”的数据</span><br><span class="line">    inputRDD.leftOuterJoin(blacksRDD)</span><br><span class="line">      .filter(_._2._2.getOrElse(false) != true)</span><br><span class="line">      .map(x =&gt;(x._1, x._2._1))</span><br><span class="line">      .printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(日天,也被小卡干了，111111)</span><br><span class="line">(历史第一人,被小卡干了，000000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">ssc：很重要 </span><br><span class="line">生产上用的很多  生产上统计结果有些数据 有些是MySQL里的直接拿的 </span><br><span class="line"></span><br><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 构建黑名单  (xx, true)  (xx, 1)</span><br><span class="line">      */</span><br><span class="line">    val blacks = new ListBuffer[(String,Boolean)]()</span><br><span class="line">    blacks.append((&quot;canglaoshi&quot;,true))  // 鉴黄</span><br><span class="line">    val blacksRDD = ssc.sparkContext.parallelize(blacks)  // 把数据转成RDD</span><br><span class="line"></span><br><span class="line">    // &quot;日天&quot;,&quot;也被小卡干了，111111&quot;</span><br><span class="line">    lines.map(x =&gt; (x.split(&quot;,&quot;)(0), x))</span><br><span class="line">      .transform(rdd =&gt; &#123;</span><br><span class="line">        rdd.leftOuterJoin(blacksRDD)</span><br><span class="line">          .filter(_._2._2.getOrElse(false) != true)</span><br><span class="line">          .map(x=&gt;x._2._1)</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">canglaoshi,xxooll</span><br><span class="line">longlaoshi,11oooxxx</span><br><span class="line">james,xxxxx</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572533110000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572533120000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">james,xxxxx</span><br><span class="line">longlaoshi,11oooxxx</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572533130000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果正确 过滤掉 canglaoshi的数据</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/21/SS03/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            SS03
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/02/17/SS01/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">SS01</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
          <li>
            <a href="/2019/11/20/tmp/">tmp</a>
          </li>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>