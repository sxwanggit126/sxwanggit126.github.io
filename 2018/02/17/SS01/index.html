<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SS01 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="123456789101112131415Fault Tolerance：Stateful exactly-once semantics out of the box.Spark Streaming recovers both lost work and operator state (e.g. sliding windows) out of the box, without any extra">
<meta property="og:type" content="article">
<meta property="og:title" content="SS01">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;17&#x2F;SS01&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="123456789101112131415Fault Tolerance：Stateful exactly-once semantics out of the box.Spark Streaming recovers both lost work and operator state (e.g. sliding windows) out of the box, without any extra">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031103920758.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031104829799.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031110110617.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;201910311101467.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031112414892.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031131615454.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031133816240.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031132936682.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031133418106.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031141750538.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:11:09.992Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191031103920758.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SS01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SS01
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/17/SS01/" class="article-date">
  <time datetime="2018-02-17T12:10:43.000Z" itemprop="datePublished">2018-02-17</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Fault Tolerance：</span><br><span class="line">Stateful exactly-once semantics out of the box.</span><br><span class="line">Spark Streaming recovers both lost work and </span><br><span class="line">operator state (e.g. sliding windows) out of the box, </span><br><span class="line">without any extra code on your part.</span><br><span class="line"></span><br><span class="line">注意：容错机制</span><br><span class="line">1.recovers  lost executor</span><br><span class="line">2.operator state</span><br><span class="line"></span><br><span class="line">Spark Integration：整合</span><br><span class="line">By running on Spark, Spark Streaming lets you reuse the same code </span><br><span class="line">for batch processing, join streams against historical data,</span><br><span class="line"> or run ad-hoc queries on stream state. </span><br><span class="line"> Build powerful interactive applications, not just analytics.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">流处理</span><br><span class="line">  实时：Storm Flink   event (就是来一条数据处理一个  这是真实时)</span><br><span class="line">   近实时：Spark Streaming   mini-batch  </span><br><span class="line">   		Spark Streaming把过来的数据切割成5s 一个批次  (是小微批次处理 不是真的实时 )</span><br><span class="line">   		Spark Streaming对数据的处理是使用小批处理</span><br><span class="line"></span><br><span class="line">批处理：一次性处理某个批次的数据     数据是有始有终(有开始有结束 有头有尾的)</span><br><span class="line">	eg：处理某个文件夹下面数据 处理完就ok了  不可能跑到别的文件夹下面  (可以这么理解)</span><br><span class="line"></span><br><span class="line">流处理 ： 流氓的流 流是一直不断的 </span><br><span class="line">	eg：水龙头打开了 水一直流     不流水了说明 水龙头坏了 或者 没水了 </span><br><span class="line">	</span><br><span class="line">你们的生产上面的实时性是多高呢？</span><br><span class="line">Spark Streaming 可以做到0.5s  </span><br><span class="line">你要注意 0.5s 能进来多少数据</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#overview" target="_blank" rel="noopener">官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1. Spark Streaming is an extension of the core Spark API </span><br><span class="line"></span><br><span class="line">2.Data can be ingested from many sources </span><br><span class="line">like Kafka, Flume, Kinesis, or TCP sockets,</span><br><span class="line"> and can be processed using complex algorithms expressed </span><br><span class="line"> with high-level functions like map, reduce, join and window.</span><br><span class="line"></span><br><span class="line">数据源：</span><br><span class="line"> Kafka *****  流处理引擎+Kafka  CP</span><br><span class="line"> Flume ==&gt; 流处理引擎   可以的用的  但是 没有缓冲 </span><br><span class="line"> HDFS</span><br><span class="line"> TCP sockets ==&gt; 测试  + 电信运营商(他们用 早期的时候 15年)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031103920758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ss：</span><br><span class="line">	Input： Kafka Socket</span><br><span class="line">    Transform：业务逻辑处理</span><br><span class="line">    Output：</span><br><span class="line"></span><br><span class="line">Spark Streaming ：他有几件事情</span><br><span class="line">    1）receives live input data streams   接受数据</span><br><span class="line">    2）divides the data into batches         把接受到的数据 拆分成batches</span><br><span class="line">比如说 ：</span><br><span class="line">	1.Spark Streaming 5秒中处理一次数据   5s时间到了  </span><br><span class="line">	2. 那么会把5s中接受的数据 把它切成 batch </span><br><span class="line">    3. 之后 把batch 交给 sparkEngine 处理  </span><br><span class="line">    4. 处理完的结果也是 batch</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031104829799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming 的编程模型：</span><br><span class="line">	DStream</span><br><span class="line">        which represents a continuous stream of data</span><br><span class="line"></span><br><span class="line">理解不了看源码：</span><br><span class="line">/**</span><br><span class="line"> * A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous</span><br><span class="line"> * sequence of RDDs (of the same type) representing a continuous stream of data (see</span><br><span class="line"> * org.apache.spark.rdd.RDD in the Spark core documentation for more details on RDDs).</span><br><span class="line"> * DStreams can either be created from live data (such as, data from TCP sockets, Kafka, Flume,</span><br><span class="line"> * etc.) using a [[org.apache.spark.streaming.StreamingContext]] or it can be generated by</span><br><span class="line"> * transforming existing DStreams using operations such as `map`,</span><br><span class="line"> * `window` and `reduceByKeyAndWindow`. While a Spark Streaming program is running, each DStream</span><br><span class="line"> * periodically generates a RDD, either from live data or by transforming the RDD generated by a</span><br><span class="line"> * parent DStream.</span><br><span class="line"> *</span><br><span class="line"> * This class contains the basic operations available on all DStreams, such as `map`, `filter` and</span><br><span class="line"> * `window`. In addition, [[org.apache.spark.streaming.dstream.PairDStreamFunctions]] contains</span><br><span class="line"> * operations available only on DStreams of key-value pairs, such as `groupByKeyAndWindow` and</span><br><span class="line"> * `join`. These operations are automatically available on any DStream of pairs</span><br><span class="line"> * (e.g., DStream[(Int, Int)] through implicit conversions.</span><br><span class="line"> *</span><br><span class="line"> * A DStream internally is characterized by a few basic properties:</span><br><span class="line"> *  - A list of other DStreams that the DStream depends on</span><br><span class="line"> *  - A time interval at which the DStream generates an RDD</span><br><span class="line"> *  - A function that is used to generate an RDD after each time interval</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">abstract class DStream[T: ClassTag] (</span><br><span class="line">    @transient private[streaming] var ssc: StreamingContext</span><br><span class="line">  ) extends Serializable with Logging &#123;</span><br><span class="line"></span><br><span class="line">注意：跟RDD 差不多 </span><br><span class="line">StreamingContext：就是流处理的上下文模型</span><br><span class="line">DStream ： is a continuous sequence of RDDs</span><br><span class="line">  就是一个流进来 按照时间批次(就是几秒一批次) 被拆成一个一个的RDD</span><br><span class="line">  DStream 由一串RDD构成  我们处理的时候 是以 RDD为单位进行处理的 </span><br><span class="line">  底层就是sparkcore </span><br><span class="line"></span><br><span class="line">DStream 这么来的呢？ 跟RDD一样 (看注释)</span><br><span class="line">	1.live data</span><br><span class="line">	2.别的DStream 转换来的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This class contains the basic operations available on all DStreams：</span><br><span class="line">看看有多少operations</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031110110617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/201910311101467.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">所以RDD算子一点要熟练掌握</span><br><span class="line"></span><br><span class="line">特性三个：</span><br><span class="line">	1.A list of other DStreams that the DStream depends on    </span><br><span class="line">		</span><br><span class="line">	2.A time interval at which the DStream generates an RDD</span><br><span class="line">		   时间间隔产生rdd     也就是  每隔多少时间处理一次</span><br><span class="line">	3. A function that is used to generate an RDD after each time interval</span><br><span class="line">	       	因为你一个DStream 由一堆RDD构成 是有顺序的</span><br><span class="line">	       	最终 你对DStream 做操作 其实就是对RDD做操作 </span><br><span class="line">	       	对RDD做操作 就是对 RDD里的每一个元素做操作</span><br></pre></td></tr></table></figure></div>

<p><strong>案列代码准备</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">StreamingContext  有好多附属构造器的 </span><br><span class="line">class StreamingContext private[streaming] (</span><br><span class="line">    _sc: SparkContext,</span><br><span class="line">    _cp: Checkpoint,</span><br><span class="line">    _batchDur: Duration</span><br><span class="line">  ) extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create a StreamingContext using an existing SparkContext.</span><br><span class="line">   * @param sparkContext existing SparkContext</span><br><span class="line">   * @param batchDuration the time interval at which streaming data will be divided into batches</span><br><span class="line">   */</span><br><span class="line">  def this(sparkContext: SparkContext, batchDuration: Duration) = &#123;</span><br><span class="line">    this(sparkContext, null, batchDuration)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create a StreamingContext by providing the configuration necessary for a new SparkContext.</span><br><span class="line">   * @param conf a org.apache.spark.SparkConf object specifying Spark parameters</span><br><span class="line">   * @param batchDuration the time interval at which streaming data will be divided into batches</span><br><span class="line">   */</span><br><span class="line">  def this(conf: SparkConf, batchDuration: Duration) = &#123;</span><br><span class="line">    this(StreamingContext.createNewSparkContext(conf), null, batchDuration)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">我们选择 this(conf: SparkConf, batchDuration: Duration)   </span><br><span class="line"></span><br><span class="line">case class Duration (private val millis: Long)    单位是millis  </span><br><span class="line"></span><br><span class="line">你传Duration 太死板了想穿个秒数 还得算  看看有没有封装好的 </span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Helper object that creates instance of [[org.apache.spark.streaming.Duration]] representing</span><br><span class="line"> * a given number of seconds.</span><br><span class="line"> */</span><br><span class="line">object Seconds &#123;</span><br><span class="line">  def apply(seconds: Long): Duration = new Duration(seconds * 1000)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">封装一个工具类：</span><br><span class="line">object ContextUtils &#123;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 获取sc</span><br><span class="line">    */</span><br><span class="line">  def getSparkContext(appname:String,defalut:String = &quot;local[2]&quot;): SparkContext = &#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">    new SparkContext(sparkConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 获取ssc</span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">  def getStreamingContext(appname:String,batch:Int,defalut:String = &quot;local&quot;) =&#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">    new StreamingContext(sparkConf,Seconds(batch))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">object AppName &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    println(this.getClass.getName)    //包名+类名</span><br><span class="line">    println(this.getClass.getSimpleName)    //类名</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">com.ruozedata.spark.ss01.AppName$         </span><br><span class="line">AppName$</span><br></pre></td></tr></table></figure></div>
<p><strong>案例</strong><br>socket：<br><img src="https://img-blog.csdnimg.cn/20191031112414892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有三个 ：用哪个呢？有什么区别呢？看下面</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">数据源：socket </span><br><span class="line"> /**</span><br><span class="line">   * Creates an input stream from TCP source hostname:port. Data is received using</span><br><span class="line">   * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\n` delimited</span><br><span class="line">   * lines.</span><br><span class="line">   * @param hostname      Hostname to connect to for receiving data</span><br><span class="line">   * @param port          Port to connect to for receiving data</span><br><span class="line">   * @param storageLevel  Storage level to use for storing the received objects</span><br><span class="line">   *                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)</span><br><span class="line">   * @see [[socketStream]]</span><br><span class="line">   */</span><br><span class="line">  def socketTextStream(</span><br><span class="line">      hostname: String,</span><br><span class="line">      port: Int,</span><br><span class="line">      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2</span><br><span class="line">    ): ReceiverInputDStream[String] = withNamedScope(&quot;socket text stream&quot;) &#123;</span><br><span class="line">    socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Creates an input stream from TCP source hostname:port. Data is received using</span><br><span class="line">   * a TCP socket and the receive bytes it interpreted as object using the given</span><br><span class="line">   * converter.</span><br><span class="line">   * @param hostname      Hostname to connect to for receiving data</span><br><span class="line">   * @param port          Port to connect to for receiving data</span><br><span class="line">   * @param converter     Function to convert the byte stream to objects</span><br><span class="line">   * @param storageLevel  Storage level to use for storing the received objects</span><br><span class="line">   * @tparam T            Type of the objects received (after converting bytes to objects)</span><br><span class="line">   */</span><br><span class="line">  def socketStream[T: ClassTag](</span><br><span class="line">      hostname: String,</span><br><span class="line">      port: Int,</span><br><span class="line">      converter: (InputStream) =&gt; Iterator[T],</span><br><span class="line">      storageLevel: StorageLevel</span><br><span class="line">    ): ReceiverInputDStream[T] = &#123;</span><br><span class="line">    new SocketInputDStream[T](this, hostname, port, converter, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	socketTextStream </span><br><span class="line">		底层调用的是 socketStream</span><br><span class="line">		socketStream底层调用的是 SocketInputDStream</span><br><span class="line">    socketStream</span><br><span class="line">	底层调用的是 SocketInputDStream</span><br><span class="line"></span><br><span class="line">socketTextStream 和socketStream 就是入参不一样 用起来是一样的 </span><br><span class="line">那么SocketInputDStream：</span><br><span class="line"></span><br><span class="line">class SocketInputDStream[T: ClassTag](</span><br><span class="line">    _ssc: StreamingContext,</span><br><span class="line">    host: String,</span><br><span class="line">    port: Int,</span><br><span class="line">    bytesToObjects: InputStream =&gt; Iterator[T],</span><br><span class="line">    storageLevel: StorageLevel</span><br><span class="line">  ) extends ReceiverInputDStream[T]</span><br><span class="line"></span><br><span class="line">都是ReceiverInputDStream 这个 ******</span><br><span class="line"></span><br><span class="line">StorageLevel默认的是 MEMORY_AND_DISK_SER_2  </span><br><span class="line">跟sparkcore里是不一样的  为什么是2呢 ？</span><br></pre></td></tr></table></figure></div>

<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">A Quick Example</a></p>
<p>测试：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b</span><br></pre></td></tr></table></figure></div>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input DStream</span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">   //transformation</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    result.print()</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">19/10/31 11:35:31 WARN StreamingContext: spark.master should be set as local[n], n &gt; 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.</span><br><span class="line">19/10/31 11:35:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:35:40 WARN BlockManager: Block input-0-1572492940600 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:35:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:35:44 WARN BlockManager: Block input-0-1572492944000 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为什么没有数据？</span><br><span class="line">先把 master local  改成 local[2] 再测试</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b        这是我第一次测试</span><br><span class="line">                  第二次测试   第一批次</span><br><span class="line">a,a,a,a         </span><br><span class="line">b,b,b,b</span><br><span class="line"></span><br><span class="line">                   第二批次 </span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b</span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b</span><br><span class="line"></span><br><span class="line">结果是 ：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493040000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 11:37:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:24 WARN BlockManager: Block input-0-1572493044200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:37:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:25 WARN BlockManager: Block input-0-1572493045000 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,4)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 11:37:42 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:42 WARN BlockManager: Block input-0-1572493062600 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:37:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:43 WARN BlockManager: Block input-0-1572493062800 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:37:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:43 WARN BlockManager: Block input-0-1572493063400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.</span><br><span class="line"> 上面的代码是处理  当前批次的</span><br><span class="line"> 不是求累加批次的 累加是另外的算子</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">After a context is defined, you have to do the following.</span><br><span class="line"></span><br><span class="line">    1.Define the input sources by creating input DStreams.</span><br><span class="line">	2.Define the streaming computations by applying transformation and output operations to DStreams.</span><br><span class="line">	3.Start receiving data and processing it using streamingContext.start().</span><br><span class="line">	4.Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().</span><br><span class="line">	5.The processing can be manually stopped using streamingContext.stop().</span><br><span class="line">Points to remember:</span><br><span class="line">	1.Once a context has been started, no new streaming computations can be set up or added to it.</span><br><span class="line">		就是说：</span><br><span class="line">			 ssc.start()</span><br><span class="line">			 在这加入逻辑处理是没有用的</span><br><span class="line">            ssc.awaitTermination()</span><br><span class="line">            </span><br><span class="line">	2.Once a context has been stopped, it cannot be restarted.</span><br><span class="line">	3.Only one StreamingContext can be active in a JVM at the same time.</span><br><span class="line">	4.stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext</span><br><span class="line">		 set the optional parameter of stop() called stopSparkContext to false.</span><br><span class="line">	5.A SparkContext can be re-used to create multiple StreamingContexts,</span><br><span class="line">	 as long as the previous StreamingContext is stopped (without stopping the SparkContext)</span><br><span class="line">	  before the next StreamingContext is created.</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191031131615454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">上面案例讲解：</span><br><span class="line"></span><br><span class="line">1.既然是通过上下文ssc 去拿数据去 接收数据</span><br><span class="line">会有一个  接收器   在里面 </span><br><span class="line"></span><br><span class="line">socket 起在 9999 端口  需要一个接收器 把数据接收回来 </span><br><span class="line"></span><br><span class="line">下面</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="noopener">Input DStreams and Receivers</a></p>
<p><strong>Input DStreams</strong> are DStreams representing <strong>the stream of input data</strong> <strong>received from streaming sources.</strong> In the quick example, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (<strong>except file stream</strong>, discussed later in this section) is associated with a <strong>Receiver</strong> (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. lines was an input DStream</span><br><span class="line">2. Receiver :receives the data from a source and stores it in Spark’s memory for processing.</span><br><span class="line"></span><br><span class="line">所以 你假如不知道 哪个算子 里有接收器   看它返回值 返回值里是带Receiver 的 </span><br><span class="line">只要有返回值里是带Receiver   必然是有接收器的</span><br><span class="line">eg：</span><br><span class="line">			 val lines: ReceiverInputDStream[String]</span><br><span class="line"> </span><br><span class="line"> 不是所有的接收数据都需要接收器的 ***</span><br><span class="line"> 为什么呢？</span><br><span class="line"> 	eg：HDFS上的数据  直接通过API读进来就可以了 不需要接收器</span><br><span class="line"></span><br><span class="line">所以：</span><br><span class="line">上面的 master 设置 local  不是local[2]</span><br><span class="line">为什么1 不行呢？1的话 你的 jobid 0  就占用一个线程  后面没有资源线程处理了呀</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031133816240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">where &gt; n  因为 有些业务是需要多个流处理的   </span><br><span class="line">eg：你一套代码里面 有多个 socket  就有多个reciver了 明白吗？</span><br><span class="line"></span><br><span class="line">所以你 core的数量 要大于 recivers的数量   否则 你的程序只能接收数据 不能处理数据</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191031132936682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">active job   ： receiver  是接收数据用的 一直在跑</span><br><span class="line">这个是永远存在的 因为 对于 socket模式 </span><br><span class="line">返回值是 ReceiverInputDStream 所有第一个Job是一直running在那的，职责就是接收数据</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031133418106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这幅图 调优的时候详细讲解</span><br></pre></td></tr></table></figure></div>

<p>*<em>操作讲解 *</em><br><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams" target="_blank" rel="noopener">Transformations on DStreams</a></p>
<p>只有最后两个和RDD算子不一样 其他的都一样</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,c,b,b,b</span><br><span class="line">a,a,a,a,a</span><br><span class="line">b,b,b,b,b</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//    result.print()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    //1.统计一个批次出现了多少个单词</span><br><span class="line">    lines.count().print() //一个批次有多少条数据</span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).count().print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501470000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501470000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">19/10/31 13:57:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 13:57:55 WARN BlockManager: Block input-0-1572501475400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 13:57:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 13:57:59 WARN BlockManager: Block input-0-1572501479200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501480000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501480000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">15</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501490000 ms</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">b,b,b,b,b</span><br><span class="line">a,a,a,a,a</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//    result.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //1.统计一个批次出现了多少个单词</span><br><span class="line">    lines.count().print() //一个批次有多少条数据</span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).countByValue().print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501630000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501630000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 14:00:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 14:00:31 WARN BlockManager: Block input-0-1572501631200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 14:00:38 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 14:00:38 WARN BlockManager: Block input-0-1572501638400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501640000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501640000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,5)</span><br><span class="line">(a,5)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501650000 ms</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure></div>

<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#output-operations-on-dstreams" target="_blank" rel="noopener">Output Operations on DStreams</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Save each RDD in this DStream as at text file, using string representation</span><br><span class="line">   * of elements. The file name at each batch interval is generated based on</span><br><span class="line">   * `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.</span><br><span class="line">   */</span><br><span class="line">  def saveAsTextFiles(prefix: String, suffix: String = &quot;&quot;): Unit = ssc.withScope &#123;</span><br><span class="line">    val saveFunc = (rdd: RDD[T], time: Time) =&gt; &#123;</span><br><span class="line">      val file = rddToFileName(prefix, suffix, time)</span><br><span class="line">      rdd.saveAsTextFile(file)</span><br><span class="line">    &#125;</span><br><span class="line">    this.foreachRDD(saveFunc, displayInnerRDDOps = false)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">这个方法生产上能用么？</span><br><span class="line">假如你1s 处理一次  1s写一次 你hdfs很容易写爆掉的 </span><br><span class="line">如果要用 把写出去的文件 使用追加的方式写   或者 定期合并生成的文件 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output opearation  和rdd 大部分都类似 </span><br><span class="line">foreachRDD  这个算子 之后讲解</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="noopener">Input DStreams and Receivers</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming provides two categories of built-in streaming sources.</span><br><span class="line"></span><br><span class="line">	1.Basic sources:</span><br><span class="line">		 Sources directly available in the StreamingContext API. </span><br><span class="line">		 Examples: file systems, and socket connections.</span><br><span class="line">	2.Advanced sources: </span><br><span class="line">		Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. </span><br><span class="line">		These require linking against extra dependencies as discussed in the linking section.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">流处理系统 一般对接的是 kafka  读文件用的少 </span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create an input stream that monitors a Hadoop-compatible filesystem</span><br><span class="line">   * for new files and reads them as text files (using key as LongWritable, value</span><br><span class="line">   * as Text and input format as TextInputFormat). Files must be written to the</span><br><span class="line">   * monitored directory by &quot;moving&quot; them from another location within the same</span><br><span class="line">   * file system. File names starting with . are ignored.</span><br><span class="line">   * @param directory HDFS directory to monitor for new file</span><br><span class="line">   */</span><br><span class="line">  def textFileStream(directory: String): DStream[String] = withNamedScope(&quot;text file stream&quot;) &#123;</span><br><span class="line">    fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString)</span><br><span class="line">  &#125;</span><br><span class="line">底层fileStream  跟一下源码有兴趣的 前面文章讲过</span><br><span class="line"></span><br><span class="line">返回值是 DStream  所以 可以local1来处理 </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line"> Files must be written to the</span><br><span class="line">   * monitored directory by &quot;moving&quot; them from another location within the same</span><br><span class="line">   * file system.</span><br><span class="line"></span><br><span class="line">All files must be in the same data format.  看官网</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031141750538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/20/SS02/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            SS02
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/02/15/Kafka01/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Kafka01</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>