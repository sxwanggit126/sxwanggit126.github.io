<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SparkSQL03 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384在object LogApp &amp;#123;  def main(args: Arr">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL03">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;05&#x2F;SparkSQL03&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384在object LogApp &amp;#123;  def main(args: Arr">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102822020751.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028220302437.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102821264517.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028213942626.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028223316966.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191029101322751.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191029103139260.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191029103558943.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102910505030.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191029104917905.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191029105551860.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:08:56.175Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102822020751.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SparkSQL03" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SparkSQL03
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/05/SparkSQL03/" class="article-date">
  <time datetime="2018-02-05T12:08:32.000Z" itemprop="datePublished">2018-02-05</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">在object LogApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local[4]&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    // ETL: 一定保留原有的数据   最完整</span><br><span class="line">    var inputDF = spark.read.json(&quot;data/data-test.json&quot;)</span><br><span class="line">   inputDF = inputDF.withColumn(&quot;province&quot;, MydataUDF.getProvince(inputDF.col(&quot;ip&quot;)))</span><br><span class="line">   inputDF = inputDF.withColumn(&quot;city&quot;, MydataUDF.getCity(inputDF.col(&quot;ip&quot;)))</span><br><span class="line"></span><br><span class="line">    // ETL==&gt;ODS</span><br><span class="line">  //  inputDF.coalesce(1).write.format(&quot;parquet&quot;)     //orc /parquet</span><br><span class="line"> //     .option(&quot;compression&quot;,&quot;snappy&quot;).save(&quot; path&quot;)    //别使用snappy 用lzo</span><br><span class="line"></span><br><span class="line">    inputDF.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line"></span><br><span class="line">    spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;,&quot;400&quot;) // --conf  这个东西不建议写在代码里 建议写在 spark-submit --conf 那块 明白吗？</span><br><span class="line">    </span><br><span class="line">    val areaSQL01 = &quot;select province,city, &quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode &gt;=1 then 1 else 0 end) origin_request,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode &gt;=2 then 1 else 0 end) valid_request,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode =3 then 1 else 0 end) ad_request,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and isbid=1 and adorderid!=0 then 1 else 0 end) bid_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 then 1 else 0 end) bid_success_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=2 and iseffective=1 then 1 else 0 end) ad_display_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=3 and processnode=1 then 1 else 0 end) ad_click_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=2 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_display_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=3 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_click_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*winprice/1000 else 0 end) ad_consumption,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*adpayment/1000 else 0 end) ad_cost &quot; +</span><br><span class="line">      &quot;from log group by province,city&quot;</span><br><span class="line">    spark.sql(areaSQL01).show(false)//.createOrReplaceTempView(&quot;area_tmp&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val areaSQL02 = &quot;select province,city, &quot; +</span><br><span class="line">      &quot;origin_request,&quot; +</span><br><span class="line">      &quot;valid_request,&quot; +</span><br><span class="line">      &quot;ad_request,&quot; +</span><br><span class="line">      &quot;bid_cnt,&quot; +</span><br><span class="line">      &quot;bid_success_cnt,&quot; +</span><br><span class="line">      &quot;bid_success_cnt/bid_cnt bid_success_rate,&quot; +</span><br><span class="line">      &quot;ad_display_cnt,&quot; +</span><br><span class="line">      &quot;ad_click_cnt,&quot; +</span><br><span class="line">      &quot;ad_click_cnt/ad_display_cnt ad_click_rate,&quot; +</span><br><span class="line">      &quot;ad_consumption,&quot; +</span><br><span class="line">      &quot;ad_cost from area_tmp &quot; +</span><br><span class="line">      &quot;where bid_cnt!=0 and ad_display_cnt!=0&quot;</span><br><span class="line"></span><br><span class="line">    Thread.sleep(10000)</span><br><span class="line"></span><br><span class="line">    spark.sql(areaSQL02).show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object MydataUDF &#123;</span><br><span class="line"></span><br><span class="line">  import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">  def getProvince = udf((ip:String)=&gt;&#123;</span><br><span class="line">    IPUtil.getInstance().getInfos(ip)(1)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  def getCity = udf((ip:String)=&gt;&#123;</span><br><span class="line">    IPUtil.getInstance().getInfos(ip)(2)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">问题：</span><br><span class="line">有什么问题？</span><br><span class="line">1.spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;,&quot;400&quot;) // --conf  </span><br><span class="line">这个东西不建议写在代码里 建议写在 spark-submit --conf 那块 明白吗？ 或者通过代码判断输入值 eg:400</span><br><span class="line"></span><br><span class="line">2. inputDF.coalesce(1).write.format(&quot;parquet&quot;)     //orc /parquet</span><br><span class="line">.option(&quot;compression&quot;,&quot;snappy&quot;).save(&quot; path&quot;)    //别使用snappy 用lzo</span><br><span class="line">spark默认是snappy 别用哈 看压缩篇</span><br><span class="line"></span><br><span class="line">coalesce(1) 这个值 看下面给的建议  处理小文件的</span><br></pre></td></tr></table></figure></div>

<p>演示上面代码可能的问题<br><img src="https://img-blog.csdnimg.cn/2019102822020751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028220302437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">这200 哪里来的 ？</span><br><span class="line"></span><br><span class="line">    官网 sparksql 调优章节 </span><br><span class="line">1.spark.sql.shuffle.partitions 参数 默认 200   这是sparksql里面 设置的shuffle参数 </span><br><span class="line"></span><br><span class="line">2.RDD里的 reduceByKey(，numPartitions）还有印象吗？rdd是在这里设置的 </span><br><span class="line"></span><br><span class="line">sparksql 默认200  生产上绝对是不够的 只要你数据量稍微大一点 200个 一定是扛不住的 </span><br><span class="line"></span><br><span class="line">这个参数 你可以理解为mapreduce里的reduce的数量 ，reduce数量如果大了 </span><br><span class="line">会导致上面问题？程序跑起来是快了 但是 小文件过多 </span><br><span class="line"></span><br><span class="line">那么这个值 该怎么设置呢？</span><br><span class="line">给你个思路 估计你读进来的数据量大小 + 你预估你每个task处理的数据量是多少 </span><br><span class="line">来设计 这个值</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">还有一点就是：</span><br><span class="line">加入这个值是400  </span><br><span class="line">400</span><br><span class="line">    大：小文件多点、</span><br><span class="line">    10exe * 2core = 20task   同一时间点 20个task</span><br><span class="line">               400/20=20轮</span><br><span class="line">               600/20=30轮</span><br></pre></td></tr></table></figure></div>


<p><strong>ETL</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ETL</span><br><span class="line">    input:json</span><br><span class="line">    清洗 ==&gt; ODS  大宽表  HDFS/Hive/SparkSQL</span><br><span class="line">    output: 列式存储  ORC/Parquet   这块一定是要落地的 </span><br><span class="line"></span><br><span class="line">    Stat</span><br><span class="line">        ==&gt;  一个非常简单的SQL搞定</span><br><span class="line">        ==&gt;  复杂：多个SQL 或者 一个复杂SQL搞定</span><br></pre></td></tr></table></figure></div>
<p><a href="https://developer.ibm.com/hadoop/2016/01/14/5-reasons-to-choose-parquet-for-spark-sql/" target="_blank" rel="noopener">Choose Parquet for Spark SQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">行式存储：MySQL</span><br><span class="line">    一条记录有多个列  一行数据是存储在一起的</span><br><span class="line">    优点：</span><br><span class="line">        你每次查询都使用到所有的列</span><br><span class="line">    缺点：</span><br><span class="line">        大宽表有N多列，但是我们仅仅使用其中几列</span><br><span class="line"> 	</span><br><span class="line"> 	因为我使用大宽表(有100列)的时候 假如只用到其中的3个列，</span><br><span class="line"> 	如果我使用 行式存储   加载数据的时候会把 你一行的所有列都加载出来 意味着浪费了97%资源</span><br><span class="line"> </span><br><span class="line"> 列式存储很好的解决这个问题</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">列式：Orc Parquet</span><br><span class="line">    特点：把每一列的数据存放在一起</span><br><span class="line">    优点：减少IO 需要哪几列就直接获取哪几列</span><br><span class="line">    缺点：如果你还是要获取每一行中的所有列，那么性能比行式的差</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102821264517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用行式存储 spark跑程序的时候官网也列举了很多问题 </span><br><span class="line">eg：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028213942626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">那么Most of these failures force Spark to re-try by re-queuing tasks：</span><br><span class="line">spark会重试跑失败的task </span><br><span class="line">注意：</span><br><span class="line">	重试 一般是跑不出来的  如果没有倾斜 和资源够 可能会跑出来</span><br><span class="line">	假设10个task 3个task挂掉了 那么重新起的task 你能确定 </span><br><span class="line">	重启来的task 会在 3个task之前挂掉的executor上面么？</span><br><span class="line">	不能确定 很可能起到别的executor上面 </span><br><span class="line">	（别的executor 可能现在也在跑 其余7个task中的某些task）</span><br><span class="line">	对于这个 executor压力更大 可能会导致你的应用程序被干掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">存储是结合 压缩来用的   eg：orc + lzo</span><br><span class="line">减少disk io</span><br></pre></td></tr></table></figure></div>

<h2 id="beeline-jdbc"><a href="#beeline-jdbc" class="headerlink" title="beeline/jdbc"></a>beeline/jdbc</h2><p>生产上是用的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hiveserver2  beeline/jdbc     Hive里的 </span><br><span class="line">thriftserver beeline/jdbc     spark里的 </span><br><span class="line"></span><br><span class="line">怎么用呢？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ ./start-thriftserver.sh --jars ~/software/mysql-connector-java-5.1.47.jar </span><br><span class="line">starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop101.out</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ tail -200f /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop101.out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Spark Command: /usr/java/java/bin/java -cp /home/double_happy/app/spark/conf/:/home/double_happy/app/spark/jars/*:/home/double_happy/app/hadoop/etc/hadoop/ -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server --jars /home/double_happy/software/mysql-connector-java-5.1.47.jar spark-internal</span><br><span class="line">========================================</span><br><span class="line">19/10/28 22:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/28 22:26:56 INFO HiveThriftServer2: Started daemon with process name: 2297@hadoop101</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for TERM</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for HUP</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for INT</span><br><span class="line">19/10/28 22:26:56 INFO HiveThriftServer2: Starting SparkContext</span><br><span class="line">19/10/28 22:26:56 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/10/28 22:26:56 INFO SparkContext: Submitted application: Thrift JDBC/ODBC Server</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/28 22:26:57 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 35237.</span><br><span class="line">19/10/28 22:26:57 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/10/28 22:26:57 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/10/28 22:26:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/10/28 22:26:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/10/28 22:26:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb4561a2-a2c1-42a6-a313-96e3ff47a7fb</span><br><span class="line">19/10/28 22:26:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/10/28 22:26:58 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/10/28 22:26:58 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/10/28 22:26:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/10/28 22:26:58 INFO SparkContext: Added JAR file:///home/double_happy/software/mysql-connector-java-5.1.47.jar at spark://hadoop101:35237/jars/mysql-connector-java-5.1.47.jar with timestamp 1572272818597</span><br><span class="line">19/10/28 22:26:58 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/10/28 22:26:59 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 33661.</span><br><span class="line">19/10/28 22:26:59 INFO NettyBlockTransferService: Server created on hadoop101:33661</span><br><span class="line">19/10/28 22:26:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:33661 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:27:01 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572272818750</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: loading hive config file: file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/conf/hive-site.xml</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: Setting hive.metastore.warehouse.dir (&apos;null&apos;) to the value of spark.sql.warehouse.dir (&apos;file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse&apos;).</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: Warehouse path is &apos;file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse&apos;.</span><br><span class="line">19/10/28 22:27:01 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</span><br><span class="line">19/10/28 22:27:03 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/28 22:27:03 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:03 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/28 22:27:03 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/28 22:27:05 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/28 22:27:07 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:07 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/28 22:27:08 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/28 22:27:08 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:08 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/28 22:27:08 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:09 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_functions: db=homework pat=*</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=homework pat=*</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_function: homework.add_prefix_new</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.add_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO HiveMetaStore: 0: get_function: homework.remove_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.remove_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created local directory: /tmp/41aaf1c8-5deb-45c7-9c03-ef172a6058a3_resources</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created local directory: /tmp/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3/_tmp_space.db</span><br><span class="line">19/10/28 22:27:10 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse</span><br><span class="line">19/10/28 22:27:10 INFO HiveMetaStore: 0: get_database: default</span><br><span class="line">19/10/28 22:27:10 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_database: default</span><br><span class="line">19/10/28 22:27:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">19/10/28 22:27:10 INFO HiveUtils: Initializing execution hive, version 1.2.1</span><br><span class="line">19/10/28 22:27:11 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/28 22:27:11 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:11 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/28 22:27:11 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/28 22:27:14 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/28 22:27:15 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:15 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</span><br><span class="line">19/10/28 22:27:17 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0</span><br><span class="line">19/10/28 22:27:17 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:18 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created local directory: /tmp/5fe8af31-b32b-4788-a587-4fbf6cab7b1a_resources</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created local directory: /tmp/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a/_tmp_space.db</span><br><span class="line">19/10/28 22:27:18 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: Operation log root directory is created: /tmp/double_happy/operation_logs</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread pool size: 100</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:OperationManager is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:SessionManager is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service: CLIService is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:ThriftBinaryCLIService is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service: HiveServer2 is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:OperationManager is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:SessionManager is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:CLIService is started.</span><br><span class="line">19/10/28 22:27:18 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:18 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/28 22:27:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</span><br><span class="line">19/10/28 22:27:18 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_databases: default</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_databases: default</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: Shutting down the object store...</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Shutting down the object store...</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: Metastore shutdown complete.</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Metastore shutdown complete.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:ThriftBinaryCLIService is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:HiveServer2 is started.</span><br><span class="line">19/10/28 22:27:18 INFO HiveThriftServer2: HiveThriftServer2 started</span><br><span class="line">19/10/28 22:27:18 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">说明 thriftserver 启动起来了 </span><br><span class="line"></span><br><span class="line">sparkui端口 参数</span><br><span class="line">spark.port.maxRetries 16 默认16 也就是同一时间点对一台机器 只能起16个spark-submit </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 software]$ jps</span><br><span class="line">2496 Jps</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">6633 HistoryServer</span><br><span class="line">2297 SparkSubmit</span><br><span class="line">3721 NameNode</span><br><span class="line">4186 ResourceManager</span><br><span class="line">3853 DataNode</span><br><span class="line">[double_happy@hadoop101 software]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">也就是这个 2297 SparkSubmit  最多16个</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028223316966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是 thriftserver 端起来了 说明服务端有了</span><br><span class="line">所以接下来要通过客户端 连接一下 </span><br><span class="line">客户端怎么链接呢？</span><br><span class="line">使用beeline     用法跟Hive里是一毛一样的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/ruozedata_g7 -n double_happy</span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/ruozedata_g7</span><br><span class="line">19/10/28 22:40:56 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:40:56 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:40:56 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/ruozedata_g7</span><br><span class="line">Error: Database &apos;ruozedata_g7&apos; not found; (state=,code=0)</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/ruozedata_g7 (closed)&gt; ^C^C[double_happy@hadoop101 bin]$ </span><br><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/ -n double_happy            </span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/</span><br><span class="line">19/10/28 22:42:15 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:42:15 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:42:16 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/</span><br><span class="line">Connected to: Spark SQL (version 2.4.4)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1.spark2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; show databases;</span><br><span class="line">+---------------+--+</span><br><span class="line">| databaseName  |</span><br><span class="line">+---------------+--+</span><br><span class="line">| default       |</span><br><span class="line">| homework      |</span><br><span class="line">+---------------+--+</span><br><span class="line">2 rows selected (1.206 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; use homework;</span><br><span class="line">+---------+--+</span><br><span class="line">| Result  |</span><br><span class="line">+---------+--+</span><br><span class="line">+---------+--+</span><br><span class="line">No rows selected (0.181 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; show tables;</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| database  |             tableName              | isTemporary  |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| homework  | access_wide                        | false        |</span><br><span class="line">| homework  | dwd_platform_stat_info             | false        |</span><br><span class="line">| homework  | jf_tmp                             | false        |</span><br><span class="line">| homework  | ods_domain_traffic_info            | false        |</span><br><span class="line">| homework  | ods_log_info                       | false        |</span><br><span class="line">| homework  | ods_uid_pid_compression_info       | false        |</span><br><span class="line">| homework  | ods_uid_pid_info                   | false        |</span><br><span class="line">| homework  | ods_uid_pid_info_compression_test  | false        |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">8 rows selected (0.202 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/homework -n double_happy</span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/homework</span><br><span class="line">19/10/28 22:43:14 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:43:14 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:43:14 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/homework</span><br><span class="line">Connected to: Spark SQL (version 2.4.4)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1.spark2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/homework&gt; show tables;</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| database  |             tableName              | isTemporary  |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| homework  | access_wide                        | false        |</span><br><span class="line">| homework  | dwd_platform_stat_info             | false        |</span><br><span class="line">| homework  | jf_tmp                             | false        |</span><br><span class="line">| homework  | ods_domain_traffic_info            | false        |</span><br><span class="line">| homework  | ods_log_info                       | false        |</span><br><span class="line">| homework  | ods_uid_pid_compression_info       | false        |</span><br><span class="line">| homework  | ods_uid_pid_info                   | false        |</span><br><span class="line">| homework  | ods_uid_pid_info_compression_test  | false        |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">8 rows selected (0.352 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/homework&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个东西适用在哪里呢？</span><br><span class="line"></span><br><span class="line">你的数据是通过UI去访问的：eg：HUE/Zeppelin  (他们后台都有一个服务的 )</span><br><span class="line">   </span><br><span class="line">   之后可以写一个 jdbc代码 (跟hive里是一模一样的  把你的sql 发到服务 服务给你返回结果 </span><br><span class="line">    通过你的ui界面 把数据结果渲染出来 )</span><br><span class="line">    </span><br><span class="line">    如果你发的SQL是一个计算/统计SQL：返回肯定是需要时间</span><br><span class="line">    只拿结果，不计算</span><br></pre></td></tr></table></figure></div>
<p>参考官网<a href="http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#distributed-sql-engine" target="_blank" rel="noopener">Distributed SQL Engine</a></p>
<p><strong>Spark On Yarn</strong><br><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Running Spark on YARN</a></p>
<p>There are two deploy modes that can be used to launch Spark applications on YARN. In <strong>cluster mode,</strong> the Spark <strong>driver runs inside an application master process</strong> which is managed by YARN on the cluster, and <strong>the client can go away</strong> after initiating the application. In <strong>client mode</strong>, <strong>the driver runs in the client process</strong>, and the application master is only used for requesting resources from YARN.</p>
<p><img src="https://img-blog.csdnimg.cn/20191029101322751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>client模式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191029103139260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在Spark on YARN中  是没有Worker的概念，是Standalone中的</span><br><span class="line"></span><br><span class="line">Spark on YARN client ：</span><br><span class="line">   1.executor是运行在container中的</span><br><span class="line">   2.driver是跑在本地的</span><br></pre></td></tr></table></figure></div>
<p><strong>cluster模式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191029103558943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">spark on yarn 总结：</span><br><span class="line">Spark：Driver + Executors</span><br><span class="line"></span><br><span class="line">spark on yarn</span><br><span class="line">    cluster</span><br><span class="line">        driver是运行在AM里面的</span><br><span class="line">        AM：AM + Driver   既当爹又当妈 就是既要给executor发task和代码 也要申请资源</span><br><span class="line">        客户端退出   ？作业是没事的 </span><br><span class="line">        日志 是在YARN上的 ***  本地是看不见的 </span><br><span class="line">            yarn logs -applicationId &lt;app ID&gt;</span><br><span class="line"></span><br><span class="line">    client</span><br><span class="line">        driver是运行在本地的      </span><br><span class="line">        客户端退出  作业就退出了</span><br><span class="line">        AM：负责从YARN上去申请资源</span><br><span class="line">        日志是在本地的 ***   方便查看 </span><br><span class="line"></span><br><span class="line">1.</span><br><span class="line">  但是 日志在本地会有一个场景 本地的进程是有一定的限制的  </span><br><span class="line">加入你提交多个作业 都是以yarn client模式 那么 进程可能扎堆出现 机器可能会挂掉 </span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">[double_happy@hadoop101 ~]$ jps</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">17719 CoarseGrainedExecutorBackend</span><br><span class="line">6633 HistoryServer</span><br><span class="line">3721 NameNode</span><br><span class="line">17689 CoarseGrainedExecutorBackend</span><br><span class="line">4186 ResourceManager</span><br><span class="line">17517 SparkSubmit</span><br><span class="line">17645 ExecutorLauncher</span><br><span class="line">3853 DataNode</span><br><span class="line">17966 Jps</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">这是在本地 client 就提交作业 CoarseGrainedExecutorBackend 扎堆出现 多了 机器可能会挂掉</span><br><span class="line"></span><br><span class="line">2.driver 和 executor是有通信的 client模式 下 可能会有一种场景存在</span><br><span class="line">driver可以在任意一台机器上面 但是如果这个机器 不是 集群里的机器 (跟yarn 没有关系哈 这里只讨论机器和集群)</span><br><span class="line">如果这机器是在 集群外 这台机器一定是有集群的 gateway权限的 </span><br><span class="line">driver 和 executor是有通信的 网络会用影响 </span><br><span class="line">工作中在集群外的 很少哈 这里只是说一下这个场景 </span><br><span class="line"></span><br><span class="line">集群内带宽 很高 上面的场景影响不大 </span><br><span class="line"></span><br><span class="line">3.就是client模式就一个弱点 就是 本地进程太多</span><br></pre></td></tr></table></figure></div>

<p><strong>测试：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --help</span><br><span class="line">Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 ~]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line"></span><br><span class="line">spark-shell --master yarn  默认不写  --deploy-mode 是 client模式</span><br></pre></td></tr></table></figure></div>
<p>client模式：测试</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --master yarn</span><br><span class="line">19/10/29 10:41:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;ERROR&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Spark context Web UI available at http://hadoop101:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = yarn, app id = application_1570934113711_0037).</span><br><span class="line">Spark session available as &apos;spark&apos;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4</span><br><span class="line">      /_/</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102910505030.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 写代码跟运行模式是没有关系的 </span><br><span class="line"> 2.  --num-executors   默认是2 个</span><br><span class="line"> 3. id 是application_xxx 开头的必然是 yarn 模式的 去historyserver看见这个开头的 就是yarn模式跑的任务</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029104917905.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-sql --jars ~/software/mysql-connector-java-5.1.47.jar --master yarn</span><br><span class="line">19/10/29 10:54:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/29 10:54:19 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/29 10:54:19 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/29 10:54:20 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/29 10:54:20 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/29 10:54:21 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/29 10:54:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/29 10:54:23 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/29 10:54:24 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_functions: db=homework pat=*</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=homework pat=*</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_function: homework.add_prefix_new</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.add_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO HiveMetaStore: 0: get_function: homework.remove_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.remove_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created local directory: /tmp/eed4bfce-e4a6-4683-81a4-9bda791d7822_resources</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created local directory: /tmp/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822/_tmp_space.db</span><br><span class="line">19/10/29 10:54:25 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/10/29 10:54:25 INFO SparkContext: Submitted application: SparkSQL::172.26.162.56</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/29 10:54:26 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 44153.</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/10/29 10:54:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/10/29 10:54:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/10/29 10:54:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73b7d763-b5a3-476a-a93c-d259c46eac97</span><br><span class="line">19/10/29 10:54:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/10/29 10:54:26 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/10/29 10:54:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/10/29 10:54:27 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">19/10/29 10:54:27 INFO Client: Requesting a new application from cluster with 1 NodeManagers</span><br><span class="line">19/10/29 10:54:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">19/10/29 10:54:27 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead</span><br><span class="line">19/10/29 10:54:27 INFO Client: Setting up container launch context for our AM</span><br><span class="line">19/10/29 10:54:27 INFO Client: Setting up the launch environment for our AM container</span><br><span class="line">19/10/29 10:54:27 INFO Client: Preparing resources for our AM container</span><br><span class="line">19/10/29 10:54:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">19/10/29 10:54:33 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_libs__4819705212614105474.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_libs__4819705212614105474.zip</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/home/double_happy/software/mysql-connector-java-5.1.47.jar -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/mysql-connector-java-5.1.47.jar</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_conf__8084381853282513804.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_conf__.zip</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/29 10:54:37 INFO Client: Submitting application application_1570934113711_0038 to ResourceManager</span><br><span class="line">19/10/29 10:54:37 INFO YarnClientImpl: Submitted application application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:37 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1570934113711_0038 and attemptId None</span><br><span class="line">19/10/29 10:54:38 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:38 INFO Client: </span><br><span class="line">         client token: N/A</span><br><span class="line">         diagnostics: N/A</span><br><span class="line">         ApplicationMaster host: N/A</span><br><span class="line">         ApplicationMaster RPC port: -1</span><br><span class="line">         queue: root.double_happy</span><br><span class="line">         start time: 1572317677273</span><br><span class="line">         final status: UNDEFINED</span><br><span class="line">         tracking URL: http://hadoop101:8088/proxy/application_1570934113711_0038/</span><br><span class="line">         user: double_happy</span><br><span class="line">19/10/29 10:54:39 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:40 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:41 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:42 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:43 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:44 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hadoop101, PROXY_URI_BASES -&gt; http://hadoop101:8088/proxy/application_1570934113711_0038), /proxy/application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:44 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.</span><br><span class="line">19/10/29 10:54:44 INFO Client: Application report for application_1570934113711_0038 (state: RUNNING)</span><br><span class="line">19/10/29 10:54:44 INFO Client: </span><br><span class="line">         client token: N/A</span><br><span class="line">         diagnostics: N/A</span><br><span class="line">         ApplicationMaster host: 172.26.162.56</span><br><span class="line">         ApplicationMaster RPC port: -1</span><br><span class="line">         queue: root.double_happy</span><br><span class="line">         start time: 1572317677273</span><br><span class="line">         final status: UNDEFINED</span><br><span class="line">         tracking URL: http://hadoop101:8088/proxy/application_1570934113711_0038/</span><br><span class="line">         user: double_happy</span><br><span class="line">19/10/29 10:54:44 INFO YarnClientSchedulerBackend: Application application_1570934113711_0038 has started running.</span><br><span class="line">19/10/29 10:54:44 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 38768.</span><br><span class="line">19/10/29 10:54:44 INFO NettyBlockTransferService: Server created on hadoop101:38768</span><br><span class="line">19/10/29 10:54:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:38768 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)</span><br><span class="line">19/10/29 10:54:45 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.</span><br><span class="line">19/10/29 10:54:45 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.162.56:59428) with ID 1</span><br><span class="line">19/10/29 10:54:50 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:36737 with 366.3 MB RAM, BlockManagerId(1, hadoop101, 36737, None)</span><br><span class="line">19/10/29 10:54:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.162.56:34236) with ID 2</span><br><span class="line">19/10/29 10:54:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: loading hive config file: file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/conf/hive-site.xml</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: Setting hive.metastore.warehouse.dir (&apos;null&apos;) to the value of spark.sql.warehouse.dir (&apos;file:/home/double_happy/spark-warehouse&apos;).</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: Warehouse path is &apos;file:/home/double_happy/spark-warehouse&apos;.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.</span><br><span class="line">19/10/29 10:54:52 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:36948 with 366.3 MB RAM, BlockManagerId(2, hadoop101, 36948, None)</span><br><span class="line">19/10/29 10:54:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</span><br><span class="line">19/10/29 10:54:52 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/spark-warehouse</span><br><span class="line">19/10/29 10:54:52 INFO metastore: Mestastore configuration hive.metastore.warehouse.dir changed from /user/hive/warehouse to file:/home/double_happy/spark-warehouse</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Shutting down the object store...</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Shutting down the object store...</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Metastore shutdown complete.</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Metastore shutdown complete.</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: get_database: default</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_database: default</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/29 10:54:52 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/29 10:54:52 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/29 10:54:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/29 10:54:52 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/29 10:54:53 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">Spark master: yarn, Application Id: application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:53 INFO SparkSQLCLIDriver: Spark master: yarn, Application Id: application_1570934113711_0038</span><br><span class="line">spark-sql (default)&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：日志里</span><br><span class="line">19/10/29 10:54:27 WARN Client:</span><br><span class="line"> Neither spark.yarn.jars nor spark.yarn.archive is set, </span><br><span class="line"> falling back to uploading libraries under SPARK_HOME.</span><br><span class="line"></span><br><span class="line">1. spark.yarn.jars nor spark.yarn.archive is set </span><br><span class="line">这个没有设置 会把SPARK_HOME相关的东西 全部传到hdfs上去 </span><br><span class="line">不信看日志 </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">19/10/29 10:54:33 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_libs__4819705212614105474.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_libs__4819705212614105474.zip</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/home/double_happy/software/mysql-connector-java-5.1.47.jar -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/mysql-connector-java-5.1.47.jar</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_conf__8084381853282513804.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_conf__.zip</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">打开这个地址看一眼：</span><br><span class="line"> hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038</span><br><span class="line"></span><br><span class="line">这是我又重启了一个 spark-sql  之前的关掉了 </span><br><span class="line">[double_happy@hadoop101 ~]$ hadoop fs -ls  hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 double_happy supergroup     211902 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/__spark_conf__.zip</span><br><span class="line">-rw-r--r--   1 double_happy supergroup  298846294 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/__spark_libs__2528141214285970680.zip</span><br><span class="line">-rw-r--r--   1 double_happy supergroup    1007502 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/mysql-connector-java-5.1.47.jar</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">__spark_conf__.zip</span><br><span class="line">mysql-connector-java-5.1.47.jar</span><br><span class="line">__spark_libs__2528141214285970680.zip    非常大这个 </span><br><span class="line">作业跑完会把这些自动删掉</span><br><span class="line"></span><br><span class="line">如果上面提到的两个参数没有设置 会把这些传到HDFS  上传是需要花费时间的</span><br><span class="line">这个不解决 你的每个作业 都要这样</span><br><span class="line"></span><br><span class="line">这就是一个调优点：</span><br><span class="line">尽可能的让Spark快速的再yarn上运行起来   该怎么做的呢？</span><br><span class="line"></span><br><span class="line">https://guguoyu.blog.csdn.net/article/details/102644376</span><br></pre></td></tr></table></figure></div>


<p><img src="https://img-blog.csdnimg.cn/20191029105551860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell 和spark-sql 都可以 这不是主要的 主要的是下面的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --master yarn --deploy-mode cluster</span><br><span class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells.</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:853)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:281)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:774)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">为什么Cluster deploy mode is not applicable to Spark shells.？</span><br><span class="line"></span><br><span class="line">因为 spark-shell driver是在本地的 是可以交互代码的  而 yarn-claster  driver是在am里的  明白吗？</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/10/Redis/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            Redis
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/02/02/SparkSQL002/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">SparkSQL002</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
          <li>
            <a href="/2019/11/20/tmp/">tmp</a>
          </li>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>