<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SparkSQL002 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="操作Hive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596object Spar">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL002">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;02&#x2F;SparkSQL002&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="操作Hive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596object Spar">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102813201459.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028140024184.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028140640930.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:08:08.398Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102813201459.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SparkSQL002" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SparkSQL002
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/02/SparkSQL002/" class="article-date">
  <time datetime="2018-02-02T12:07:44.000Z" itemprop="datePublished">2018-02-02</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="操作Hive"><a href="#操作Hive" class="headerlink" title="操作Hive"></a>操作Hive</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;double_happy&quot;)</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;show databases&quot;).show()</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;&quot;).write.saveAsTable(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;&quot;).write.insertInto(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Saves the content of the `DataFrame` as the specified table.</span><br><span class="line">   *</span><br><span class="line">   * In the case the table already exists, behavior of this function depends on the</span><br><span class="line">   * save mode, specified by the `mode` function (default to throwing an exception).</span><br><span class="line">   * When `mode` is `Overwrite`, the schema of the `DataFrame` does not need to be</span><br><span class="line">   * the same as that of the existing table.</span><br><span class="line">   *</span><br><span class="line">   * When `mode` is `Append`, if there is an existing table, we will use the format and options of</span><br><span class="line">   * the existing table. The column order in the schema of the `DataFrame` doesn&apos;t need to be same</span><br><span class="line">   * as that of the existing table. Unlike `insertInto`, `saveAsTable` will use the column names to</span><br><span class="line">   * find the correct column positions. For example:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *    scala&gt; Seq((1, 2)).toDF(&quot;i&quot;, &quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((3, 4)).toDF(&quot;j&quot;, &quot;i&quot;).write.mode(&quot;append&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  i|  j|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  1|  2|</span><br><span class="line">   *    |  4|  3|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * In this method, save mode is used to determine the behavior if the data source table exists in</span><br><span class="line">   * Spark catalog. We will always overwrite the underlying data of data source (e.g. a table in</span><br><span class="line">   * JDBC data source) if the table doesn&apos;t exist in Spark catalog, and will always append to the</span><br><span class="line">   * underlying data of data source if the table already exists.</span><br><span class="line">   *</span><br><span class="line">   * When the DataFrame is created from a non-partitioned `HadoopFsRelation` with a single input</span><br><span class="line">   * path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC</span><br><span class="line">   * and Parquet), the table is persisted in a Hive compatible format, which means other systems</span><br><span class="line">   * like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL</span><br><span class="line">   * specific format.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def saveAsTable(tableName: String): Unit = &#123;</span><br><span class="line">    saveAsTable(df.sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Inserts the content of the `DataFrame` to the specified table. It requires that</span><br><span class="line">   * the schema of the `DataFrame` is the same as the schema of the table.</span><br><span class="line">   *</span><br><span class="line">   * @note Unlike `saveAsTable`, `insertInto` ignores the column names and just uses position-based</span><br><span class="line">   * resolution. For example:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *    scala&gt; Seq((1, 2)).toDF(&quot;i&quot;, &quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((3, 4)).toDF(&quot;j&quot;, &quot;i&quot;).write.insertInto(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((5, 6)).toDF(&quot;a&quot;, &quot;b&quot;).write.insertInto(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  i|  j|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  5|  6|</span><br><span class="line">   *    |  3|  4|</span><br><span class="line">   *    |  1|  2|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Because it inserts data to an existing table, format or options will be ignored.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def insertInto(tableName: String): Unit = &#123;</span><br><span class="line">    insertInto(df.sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建表 也可以直接 spark.sql(&quot;create table xxx&quot;) </span><br><span class="line">但是 不建议这样  因为 表 一般都是提前创建好的  </span><br><span class="line">因为 真正生产上 创建表 是在 一个 web页面 创建表的  是有权限 的</span><br></pre></td></tr></table></figure></div>

<p><a href="http://spark.apache.org/docs/latest/sql-getting-started.html#untyped-dataset-operations-aka-dataframe-operations" target="_blank" rel="noopener">Spark操作Hive 代码</a></p>
<p>大部分人使用spark开发 Hive是使用 spark.sql(“  sql  “)<br>可以的 我不喜欢 我还是喜欢使用api的方式   各有所爱 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">全局排序：这是使用sql的方式写的 </span><br><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line">    </span><br><span class="line">    //需求1 ：统计 每个平台 省市下面 traffic的总和       order by  是全局排序的 </span><br><span class="line">    val sql = &quot;select platform, province, city, sum(traffic) as traffics from log group by platform, province, city order by traffics desc&quot;</span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|platform|province|city|traffics|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|     mac|    香港|    | 2879982|</span><br><span class="line">| windows|    香港|    | 2871537|</span><br><span class="line">| Andriod|    香港|    | 2722363|</span><br><span class="line">|   linux|    香港|    | 2696578|</span><br><span class="line">| Symbain|    香港|    | 2444806|</span><br><span class="line">| Andriod|    山西|忻州|  968255|</span><br><span class="line">|   linux|    台湾|    |  898404|</span><br><span class="line">| windows|    山西|忻州|  894966|</span><br><span class="line">| Andriod|    湖北|武汉|  865758|</span><br><span class="line">|     mac|    湖北|武汉|  848995|</span><br><span class="line">|     mac|    山西|忻州|  837873|</span><br><span class="line">| Symbain|    山西|忻州|  791524|</span><br><span class="line">|   linux|    湖北|武汉|  781347|</span><br><span class="line">| Andriod|    台湾|    |  776642|</span><br><span class="line">|     mac|    台湾|    |  775977|</span><br><span class="line">| Symbain|    台湾|    |  744858|</span><br><span class="line">| windows|    台湾|    |  744558|</span><br><span class="line">| windows|    湖北|武汉|  728412|</span><br><span class="line">| Symbain|    湖北|武汉|  728034|</span><br><span class="line">|   linux|    山西|忻州|  689405|</span><br><span class="line">+--------+--------+----+--------+</span><br></pre></td></tr></table></figure></div>
<p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">Window Functions in Spark SQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">全局排序 ： Api方式   我喜欢的 </span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line">   * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line">   *</span><br><span class="line">   * This is a variant of groupBy that can only group by existing columns using column names</span><br><span class="line">   * (i.e. cannot construct expressions).</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Compute the average for all numeric columns grouped by department.</span><br><span class="line">   *   ds.groupBy(&quot;department&quot;).avg()</span><br><span class="line">   *</span><br><span class="line">   *   // Compute the max age and average salary, grouped by department and gender.</span><br><span class="line">   *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line">   *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line">   *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line">   *   ))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123;</span><br><span class="line">    val colNames: Seq[String] = col1 +: cols</span><br><span class="line">    RelationalGroupedDataset(</span><br><span class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Compute aggregates by specifying a series of aggregate columns. Note that this function by</span><br><span class="line">   * default retains the grouping columns in its output. To not retain grouping columns, set</span><br><span class="line">   * `spark.sql.retainGroupColumns` to false.</span><br><span class="line">   *</span><br><span class="line">   * The available aggregate methods are defined in [[org.apache.spark.sql.functions]].</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Selects the age of the oldest employee and the aggregate expense for each department</span><br><span class="line">   *</span><br><span class="line">   *   // Scala:</span><br><span class="line">   *   import org.apache.spark.sql.functions._</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   // Java:</span><br><span class="line">   *   import static org.apache.spark.sql.functions.*;</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;));</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Note that before Spark 1.4, the default behavior is to NOT retain grouping columns. To change</span><br><span class="line">   * to that behavior, set config variable `spark.sql.retainGroupColumns` to `false`.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Scala, 1.3.x:</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg($&quot;department&quot;, max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   // Java, 1.3.x:</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(col(&quot;department&quot;), max(&quot;age&quot;), sum(&quot;expense&quot;));</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @since 1.3.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def agg(expr: Column, exprs: Column*): DataFrame = &#123;</span><br><span class="line">    toDF((expr +: exprs).map &#123;</span><br><span class="line">      case typed: TypedColumn[_, _] =&gt;</span><br><span class="line">        typed.withInputType(df.exprEnc, df.logicalPlan.output).expr</span><br><span class="line">      case c =&gt; c.expr</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">你要好好看看注释 就会明白下面的代码</span><br><span class="line"></span><br><span class="line">groupBy:Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line">agg : </span><br><span class="line">  * Compute aggregates by specifying a series of aggregate columns. Note that this function by</span><br><span class="line">   * default retains the grouping columns in its output. To not retain grouping columns, set</span><br><span class="line">   * `spark.sql.retainGroupColumns` to false.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Returns a new Dataset sorted by the given expressions. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.sort($&quot;col1&quot;, $&quot;col2&quot;.desc)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sort(sortExprs: Column*): Dataset[T] = &#123;</span><br><span class="line">    sortInternal(global = true, sortExprs)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line">    //需求1 ：统计 每个平台 省市下面 traffic的总和       order by  是全局排序的</span><br><span class="line">        import org.apache.spark.sql.functions._    //spark 内置的函数</span><br><span class="line">        df.groupBy(&quot;platform&quot;, &quot;province&quot;, &quot;city&quot;)</span><br><span class="line">            .agg(sum(&quot;traffic&quot;).as(&quot;traffics&quot;))</span><br><span class="line">            .sort(&apos;traffics.desc)</span><br><span class="line">            .show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Hive的函数 Spark里也是有的 spark自己内置的 </span><br><span class="line">import org.apache.spark.sql.functions._   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果是;</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|platform|province|city|traffics|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|     mac|    香港|    | 2879982|</span><br><span class="line">| windows|    香港|    | 2871537|</span><br><span class="line">| Andriod|    香港|    | 2722363|</span><br><span class="line">|   linux|    香港|    | 2696578|</span><br><span class="line">| Symbain|    香港|    | 2444806|</span><br><span class="line">| Andriod|    山西|忻州|  968255|</span><br><span class="line">|   linux|    台湾|    |  898404|</span><br><span class="line">| windows|    山西|忻州|  894966|</span><br><span class="line">| Andriod|    湖北|武汉|  865758|</span><br><span class="line">|     mac|    湖北|武汉|  848995|</span><br><span class="line">|     mac|    山西|忻州|  837873|</span><br><span class="line">| Symbain|    山西|忻州|  791524|</span><br><span class="line">|   linux|    湖北|武汉|  781347|</span><br><span class="line">| Andriod|    台湾|    |  776642|</span><br><span class="line">|     mac|    台湾|    |  775977|</span><br><span class="line">| Symbain|    台湾|    |  744858|</span><br><span class="line">| windows|    台湾|    |  744558|</span><br><span class="line">| windows|    湖北|武汉|  728412|</span><br><span class="line">| Symbain|    湖北|武汉|  728034|</span><br><span class="line">|   linux|    山西|忻州|  689405|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102813201459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">api方式 开发 你要注意的是：</span><br><span class="line">	Column 和 String  就是你传进去的 列名 要传入 string类型 还是 Column 类型 </span><br><span class="line"></span><br><span class="line">   我个人喜欢 ：</span><br><span class="line">   		 Column  ==》 &apos;列名</span><br><span class="line">   		 String    ==》 “列名”</span><br><span class="line">   毕竟有太多种写法 找一个自己喜欢的  跟找女朋友相反 找女朋友 找一个喜欢自己的  明白吗</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">分组:Top n </span><br><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line"></span><br><span class="line">    // 需求二 ：platform  组内  province 访问次数最多的TopN</span><br><span class="line">    val sql =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |</span><br><span class="line">        |select * from</span><br><span class="line">        |(</span><br><span class="line">        |select t.*, row_number() over(partition by platform order by cnt desc) as r</span><br><span class="line">        |from</span><br><span class="line">        |(select platform,province,count(1) cnt from log group by platform,province) t</span><br><span class="line">        |) a where a.r&lt;=3</span><br><span class="line">        |</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+---+---+</span><br><span class="line">|platform|province|cnt|  r|</span><br><span class="line">+--------+--------+---+---+</span><br><span class="line">|   linux|    香港|606|  1|</span><br><span class="line">|   linux|    广东|211|  2|</span><br><span class="line">|   linux|    台湾|173|  3|</span><br><span class="line">| Symbain|    香港|582|  1|</span><br><span class="line">| Symbain|    广东|222|  2|</span><br><span class="line">| Symbain|    福建|153|  3|</span><br><span class="line">| Andriod|    香港|607|  1|</span><br><span class="line">| Andriod|    广东|223|  2|</span><br><span class="line">| Andriod|    北京|150|  3|</span><br><span class="line">|     mac|    香港|646|  1|</span><br><span class="line">|     mac|    广东|213|  2|</span><br><span class="line">|     mac|    台湾|156|  3|</span><br><span class="line">| windows|    香港|657|  1|</span><br><span class="line">| windows|    广东|186|  2|</span><br><span class="line">| windows|    河北|151|  3|</span><br><span class="line">+--------+--------+---+---+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line">    val sql2 =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |</span><br><span class="line">        |select a.* from</span><br><span class="line">        |(</span><br><span class="line">        |select t.*,</span><br><span class="line">        |row_number() over(partition by platform order by cnt desc) as rn,</span><br><span class="line">        |rank() over(partition by platform order by cnt desc) as r,</span><br><span class="line">        |dense_rank() over(partition by platform order by cnt desc) as dn</span><br><span class="line">        |from</span><br><span class="line">        |(select platform,province,count(1) cnt from log group by platform,province) t</span><br><span class="line">        |) a</span><br><span class="line">        |</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    spark.sql(sql2).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">|platform|province|cnt| rn|  r| dn|</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">|   linux|    香港|606|  1|  1|  1|</span><br><span class="line">|   linux|    广东|211|  2|  2|  2|</span><br><span class="line">|   linux|    台湾|173|  3|  3|  3|</span><br><span class="line">|   linux|    福建|147|  4|  4|  4|</span><br><span class="line">|   linux|    北京|134|  5|  5|  5|</span><br><span class="line">|   linux|    河北|128|  6|  6|  6|</span><br><span class="line">|   linux|    湖北|115|  7|  7|  7|</span><br><span class="line">|   linux|    山西|107|  8|  8|  8|</span><br><span class="line">|   linux|    江西|104|  9|  9|  9|</span><br><span class="line">|   linux|    上海|101| 10| 10| 10|</span><br><span class="line">|   linux|    山东| 26| 11| 11| 11|</span><br><span class="line">| Symbain|    香港|582|  1|  1|  1|</span><br><span class="line">| Symbain|    广东|222|  2|  2|  2|</span><br><span class="line">| Symbain|    福建|153|  3|  3|  3|</span><br><span class="line">| Symbain|    河北|151|  4|  4|  4|</span><br><span class="line">| Symbain|    台湾|146|  5|  5|  5|</span><br><span class="line">| Symbain|    北京|133|  6|  6|  6|</span><br><span class="line">| Symbain|    山西|121|  7|  7|  7|</span><br><span class="line">| Symbain|    湖北|120|  8|  8|  8|</span><br><span class="line">| Symbain|    上海|109|  9|  9|  9|</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">那么： 他们有什么区别？</span><br><span class="line">	row_number   123456  排序的  即使有的值相等 也往下排序</span><br><span class="line">	rank        1233567 排序的  有相同的值  排序号相等 之后会跳过重复的占位 这里就没有4</span><br><span class="line">	dense_rank   12334567 排序的 有相同的值  排序号相等  之后不会跳过重复的占位 这里紧接着4</span><br></pre></td></tr></table></figure></div>

<p><strong>Catalog</strong><br>非常非常重要 spark2.0之后才有的 我开发了一个csv入Hive 就用到了它</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你Hive的元数据存在 MySQl里面的 </span><br><span class="line">如果要代码中使用到元数据 要通过JDBC来取 </span><br><span class="line"></span><br><span class="line">但是2.0版本之后 Spark 提供 Catalog 可以拿到 Hive的元数据</span><br></pre></td></tr></table></figure></div>
<p>开启spark-shell –jars  MySQL驱动</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val catalog = spark.catalog</span><br><span class="line">catalog: org.apache.spark.sql.catalog.Catalog = org.apache.spark.sql.internal.CatalogImpl@672c4e24</span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listDatabases().show</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line">|    name|         description|         locationUri|</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line">| default|Default Hive data...|hdfs://hadoop101:...|</span><br><span class="line">|homework|                    |hdfs://hadoop101:...|</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listDatabases().show(false)</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line">|name    |description          |locationUri                                          |</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line">|default |Default Hive database|hdfs://hadoop101:8020/user/hive/warehouse            |</span><br><span class="line">|homework|                     |hdfs://hadoop101:8020/user/hive/warehouse/homework.db|</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listTables(&quot;homework&quot;).show(false)</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line">|name                             |database|description|tableType|isTemporary|</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line">|access_wide                      |homework|null       |EXTERNAL |false      |</span><br><span class="line">|dwd_platform_stat_info           |homework|null       |MANAGED  |false      |</span><br><span class="line">|jf_tmp                           |homework|null       |MANAGED  |false      |</span><br><span class="line">|ods_domain_traffic_info          |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_log_info                     |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_uid_pid_compression_info     |homework|null       |MANAGED  |false      |</span><br><span class="line">|ods_uid_pid_info                 |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_uid_pid_info_compression_test|homework|null       |EXTERNAL |false      |</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listFunctions().show(5,false)</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">|name|database|description|className                                           |isTemporary|</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">|!   |null    |null       |org.apache.spark.sql.catalyst.expressions.Not       |true       |</span><br><span class="line">|%   |null    |null       |org.apache.spark.sql.catalyst.expressions.Remainder |true       |</span><br><span class="line">|&amp;   |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseAnd|true       |</span><br><span class="line">|*   |null    |null       |org.apache.spark.sql.catalyst.expressions.Multiply  |true       |</span><br><span class="line">|+   |null    |null       |org.apache.spark.sql.catalyst.expressions.Add       |true       |</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listColumns(&quot;homework.dwd_platform_stat_info&quot;).show(false)</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line">|name    |description|dataType|nullable|isPartition|isBucket|</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line">|platform|null       |string  |true    |false      |false   |</span><br><span class="line">|cnt     |null       |int     |true    |false      |false   |</span><br><span class="line">|d       |null       |string  |true    |false      |false   |</span><br><span class="line">|day     |null       |string  |true    |true       |false   |</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">不仅仅这多多哈 catalog 几乎所有的元数据 信息都能搞到 </span><br><span class="line"></span><br><span class="line">但是这些值的返回值 都是 DataSet 接下来 讲讲DataSet</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028140024184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>给你一个使用上面catalog的场景<br>做一个页面：</p>
<p><img src="https://img-blog.csdnimg.cn/20191028140640930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>DataSet</strong><br>这个东西很简单的<br>Untyped Dataset = Row </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">DataSet就是你可以把它当作rdd来操作 </span><br><span class="line"></span><br><span class="line">object DSApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;DSApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;inferSchema&quot;,&quot;true&quot;).csv(&quot;file:///C:/IdeaProjects/spark/data/sale.csv&quot;)</span><br><span class="line">    val ds = df.as[Sales]</span><br><span class="line"></span><br><span class="line">    ds.printSchema()</span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    // ROW  DF弱类型</span><br><span class="line">    //    df.select(&quot;transactionId&quot;).show(false)</span><br><span class="line"></span><br><span class="line">    ds.map(columns =&gt;&#123;</span><br><span class="line">        columns.transactionId match &#123;</span><br><span class="line">        case 111 =&gt;Sales(columns.transactionId,columns.customerId,columns.itemId,columns.amountPaid+200)</span><br><span class="line">        case _ =&gt; Sales(columns.transactionId,columns.customerId,columns.itemId,columns.amountPaid)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Sales(transactionId:Int,customerId:Int,itemId:Int,amountPaid:Double)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- transactionId: integer (nullable = true)</span><br><span class="line"> |-- customerId: integer (nullable = true)</span><br><span class="line"> |-- itemId: integer (nullable = true)</span><br><span class="line"> |-- amountPaid: double (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|transactionId|customerId|itemId|amountPaid|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|          111|         1|     1|     100.0|</span><br><span class="line">|          112|         2|     2|     500.0|</span><br><span class="line">|          113|         3|     3|     400.0|</span><br><span class="line">|          114|         1|     4|     300.0|</span><br><span class="line">|          115|         1|     1|     200.0|</span><br><span class="line">|          116|         1|     2|     700.0|</span><br><span class="line">|          117|         4|     3|     800.0|</span><br><span class="line">|          118|         5|     1|     200.0|</span><br><span class="line">|          119|         3|     4|     200.0|</span><br><span class="line">|          120|         1|     1|     300.0|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|transactionId|customerId|itemId|amountPaid|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|111          |1         |1     |300.0     |</span><br><span class="line">|112          |2         |2     |500.0     |</span><br><span class="line">|113          |3         |3     |400.0     |</span><br><span class="line">|114          |1         |4     |300.0     |</span><br><span class="line">|115          |1         |1     |200.0     |</span><br><span class="line">|116          |1         |2     |700.0     |</span><br><span class="line">|117          |4         |3     |800.0     |</span><br><span class="line">|118          |5         |1     |200.0     |</span><br><span class="line">|119          |3         |4     |200.0     |</span><br><span class="line">|120          |1         |1     |300.0     |</span><br><span class="line">+-------------+----------+------+----------+</span><br></pre></td></tr></table></figure></div>


<p><strong>Interoperating with RDDs</strong><br><a href="http://spark.apache.org/docs/latest/sql-getting-started.html#interoperating-with-rdds" target="_blank" rel="noopener">Interoperating with RDDs</a><br>和RDD的交互操作</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DS  --》 DF   通过 DS.toDF(&quot;列名。。&quot;)</span><br><span class="line">DF--》DS   通过 样例类    df.as[样例类]</span><br><span class="line"></span><br><span class="line">RDD ---》 DF   两种 </span><br><span class="line">  Spark SQL supports two different methods for converting existing RDDs into Datasets.</span><br><span class="line">	1.反射   就是使用case  class  你的case class 定义的就是 table的信息</span><br><span class="line">	2.The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD.</span><br><span class="line">	编程的方式 ：</span><br><span class="line">		就是你的哪个字段什么类型指定好就可以了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1.反射的方式   RDD -》 DF</span><br><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // RDD ==&gt; DF/DS</span><br><span class="line">        val peopleDF = spark.sparkContext</span><br><span class="line">          .textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">          .map(_.split(&quot;,&quot;))</span><br><span class="line">          .map(x =&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">          .toDF()</span><br><span class="line"></span><br><span class="line">        peopleDF.show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+------------+---+</span><br><span class="line">|name        |age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy|25 |</span><br><span class="line">|Kairis      |25 |</span><br><span class="line">|Kite        |32 |</span><br><span class="line">+------------+---+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">2.编程的方式</span><br><span class="line">When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.</span><br><span class="line">  1.Create an RDD of Rows from the original RDD;</span><br><span class="line">  2.Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step </span><br><span class="line">  3.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.</span><br><span class="line"></span><br><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">    val fields: Array[StructField] = schemaString.split(&quot; &quot;).map(fieldName =&gt; &#123;</span><br><span class="line">      StructField(fieldName, StringType)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    val rowRDD: RDD[Row] = peopleRDD.map(_.split(&quot;,&quot;)).map(x=&gt;Row(x(0),x(1).trim))</span><br><span class="line"></span><br><span class="line">    val peopleDF: DataFrame = spark.createDataFrame(rowRDD,schema)</span><br><span class="line"></span><br><span class="line">    //TODO... 业务逻辑</span><br><span class="line">    peopleDF.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+------------+---+</span><br><span class="line">|        name|age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy| 25|</span><br><span class="line">|      Kairis| 25|</span><br><span class="line">|        Kite| 32|</span><br><span class="line">+------------+---+</span><br><span class="line"></span><br><span class="line">官网给的例子 不是很好  难道你们生产上全是 String 类型的么？ 我只能说还真是 我上一家公司就是 </span><br><span class="line">建议哈 统计字段 还是采用标准的int 或者 double类型  </span><br><span class="line">我之前统计的时候 全是String 的 就会出现 指标不准的问题  </span><br><span class="line">我遇到过 同事说用String 很爽 我只能说 是的是的 </span><br><span class="line">在他们眼里 spark不就是写sql吗？ </span><br><span class="line">emmm 统计指标可以的 但是 如果让你做 基础架构开发 呢？ </span><br><span class="line">不要仅仅局限于指标需求哈 那么你这大数据工程师 就是 sql怪</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"> </span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schema = StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;,StringType),</span><br><span class="line">      StructField(&quot;age&quot;,IntegerType)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1).trim.toInt))</span><br><span class="line">      </span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"> case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	Row里面的数据类型 一定要和 schema里的数据类型匹配上</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+------------+---+</span><br><span class="line">|        name|age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy| 25|</span><br><span class="line">|      Kairis| 25|</span><br><span class="line">|        Kite| 32|</span><br><span class="line">+------------+---+</span><br></pre></td></tr></table></figure></div>
<p><strong>UDF</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object UDFApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    /**</span><br><span class="line">      * step1： 定义 注册</span><br><span class="line">      * step2： 使用</span><br><span class="line">      */</span><br><span class="line">    spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/udf.txt&quot;)</span><br><span class="line">      .map(_.split(&quot; &quot;))</span><br><span class="line">      .map(x =&gt; FootballTeam(x(0), x(1)))</span><br><span class="line">      .toDF().createOrReplaceTempView(&quot;teams&quot;)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(&quot;teams_length&quot;,(input:String)=&gt;&#123;</span><br><span class="line">      input.split(&quot;，&quot;).length</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    //统计一个人喜欢的球队的个数</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;select name,teams,teams_length(teams) from teams&quot;).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  case class FootballTeam(name:String, teams:String)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line">|  name|             teams|UDF:teams_length(teams)|</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line">|苍老师 |      喵喵喵，红魔  |                      2|</span><br><span class="line">|    pk|小破车，国足，宅团   |                      3|</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	  spark.udf.register(&quot;teams_length&quot;,(input:String)=&gt;&#123;</span><br><span class="line">      input.split(&quot;，&quot;).length</span><br><span class="line">    &#125;)</span><br><span class="line">def register[RT: TypeTag, A1: TypeTag](name: String, func: Function1[A1, RT]): UserDefinedFunction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Function 就是传进去一个 函数 </span><br><span class="line"></span><br><span class="line">还有一种UDF函数的使用就是 api的方式 </span><br><span class="line"></span><br><span class="line">functions里面 有个 udf 方法 传进去一个函数    再 结合 withColumns 方法 使用 也是一样的</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/05/SparkSQL03/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            SparkSQL03
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/02/01/SparkSQL01/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">SparkSQL01</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Kudu-Impala%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B01-double-happy/">Kudu+Impala故障案例01--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink04-double-happy/">Flink04--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink03-double-happy/">Flink03--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink02-double-happy/">Flink02--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>