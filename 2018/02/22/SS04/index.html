<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SS04 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="1234之前的ss程序都是运行在idea那么如何提交到服务器上运行呢？  演示：  一步一步来  先不管理offset 把代码提交到yarn上 把wc统计出来  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051数据从Kafka过来然后 ss消费到 把wc统计出来">
<meta property="og:type" content="article">
<meta property="og:title" content="SS04">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;22&#x2F;SS04&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="1234之前的ss程序都是运行在idea那么如何提交到服务器上运行呢？  演示：  一步一步来  先不管理offset 把代码提交到yarn上 把wc统计出来  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051数据从Kafka过来然后 ss消费到 把wc统计出来">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103144003315.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103144048102.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103152748544.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103154351394.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103162228877.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103164705730.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103175129217.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103175538708.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103175830927.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103180027441.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019110318053852.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103182702262.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103183912458.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103184011115.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103184757992.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103191627790.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103192244253.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:13:16.486Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191103144003315.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SS04" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SS04
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/22/SS04/" class="article-date">
  <time datetime="2018-02-22T12:12:50.000Z" itemprop="datePublished">2018-02-22</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">之前的ss程序都是运行在idea</span><br><span class="line">那么如何提交到服务器上运行呢？</span><br><span class="line">  演示：</span><br><span class="line">  一步一步来  先不管理offset 把代码提交到yarn上 把wc统计出来</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">数据从Kafka过来然后 ss消费到 把wc统计出来：</span><br><span class="line"></span><br><span class="line">object StreamingKakfaDirectYarnApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    //参数从外面传进 来    topics groupId brokers</span><br><span class="line">    if(args.size != 3)&#123;</span><br><span class="line">      System.err.println(&quot;Usage:StreamingKakfaDirectYarnApp &lt;brokers&gt; &lt;topic&gt; &lt;groupId&gt;&quot;)</span><br><span class="line">      System.exit(-1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val Array(brokers,topic,groupId) = args</span><br><span class="line">    </span><br><span class="line">    val sparkConf: SparkConf = new SparkConf()</span><br><span class="line">    val ssc =new StreamingContext(sparkConf,Seconds(10))</span><br><span class="line">   // val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; brokers, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">idea测试结果：</span><br><span class="line">Usage:StreamingKakfaDirectYarnApp &lt;brokers&gt; &lt;topic&gt; &lt;groupId&gt;</span><br><span class="line"></span><br><span class="line">注意：idea里怎么把参数传进去呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103144003315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191103144048102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">运行结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572763410000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,19)</span><br><span class="line">(b,18)</span><br><span class="line">(f,21)</span><br><span class="line">(e,17)</span><br><span class="line">(a,24)</span><br><span class="line">(c,21)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572763420000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572763430000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">说明本地改造完成 那么我们打包上传到服务器上运行</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">提交命令：</span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; --name StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">19/11/03 15:08:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/11/03 15:08:42 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/11/03 15:08:43 INFO SparkContext: Submitted application: StreamingKakfaDirectYarnApp</span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/11/03 15:08:43 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 40978.</span><br><span class="line">19/11/03 15:08:43 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/11/03 15:08:43 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/11/03 15:08:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d01d319f-1fe4-4025-bcf4-418a06809ccc</span><br><span class="line">19/11/03 15:08:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/11/03 15:08:43 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/11/03 15:08:43 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/11/03 15:08:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/11/03 15:08:43 INFO SparkContext: Added JAR file:/home/double_happy/lib/spark-core-1.0.jar at spark://hadoop101:40978/jars/spark-core-1.0.jar with timestamp 1572764923743</span><br><span class="line">19/11/03 15:08:43 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/11/03 15:08:43 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 33748.</span><br><span class="line">19/11/03 15:08:43 INFO NettyBlockTransferService: Server created on hadoop101:33748</span><br><span class="line">19/11/03 15:08:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:33748 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:45 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572764923782</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp$.main(StreamingKakfaDirectYarnApp.scala:36)</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp.main(StreamingKakfaDirectYarnApp.scala)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 14 more</span><br><span class="line">19/11/03 15:08:45 INFO SparkContext: Invoking stop() from shutdown hook</span><br><span class="line">19/11/03 15:08:45 INFO SparkUI: Stopped Spark web UI at http://hadoop101:4040</span><br><span class="line">19/11/03 15:08:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">19/11/03 15:08:45 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">19/11/03 15:08:45 INFO BlockManager: BlockManager stopped</span><br><span class="line">19/11/03 15:08:46 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">19/11/03 15:08:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">19/11/03 15:08:46 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">19/11/03 15:08:46 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">19/11/03 15:08:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-6ffb645c-d7fd-44e8-b0e5-256cae7b11ea</span><br><span class="line">19/11/03 15:08:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a31f335-3d75-4c59-b7a5-c5bf023d1265</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line">为什么呢？ 在idea里都可以的 </span><br><span class="line">StringDeserializer 类是在</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">这个包里面的</span><br><span class="line"></span><br><span class="line">而这个包Spark本身是没有的 是我们额外加进来的喽 </span><br><span class="line">那么这个包没有在服务器上 为什么该怎么办呢？ 看看官网怎么说的</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#deploying" target="_blank" rel="noopener">Deploying</a>：部署<br>As with any Spark applications, spark-submit is used to launch your application.</p>
<p>For Scala and Java applications, if you are using SBT or Maven for project management, <strong>then package spark-streaming-kafka-0-10_2.12 and its dependencies into the application JAR.</strong> <strong>Make sure spark-core_2.12 and spark-streaming_2.12 are marked as provided dependencies as those are already present in a Spark installation.</strong> Then use spark-submit to launch your application (see Deploying section in the main programming guide).<br>这种方式不好 换一个</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">因为需要把这个spark-streaming-kafka-0-10_2.11包 传到服务器上</span><br><span class="line">./spark-submit --help  查查 可以加maven 的依赖  怎么加呢？</span><br><span class="line"></span><br><span class="line"> --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line"></span><br><span class="line">这个参数 可以指向 maven的一些jar包 **** </span><br><span class="line">修改提交命令：</span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line"> --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.4 \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line"></span><br><span class="line">但是这个东西 需要联网 不能联网是不行的 一会看日志就清除了 它需要联网去下载 maven依赖</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --master local[2] --name StreamingKakfaDirectYarnApp  --packageg.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.4 --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp /home/double_happy/lib/spark-core-1.0.jar hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">Ivy Default Cache set to: /home/double_happy/.ivy2/cache</span><br><span class="line">The jars for the packages stored in: /home/double_happy/.ivy2/jars</span><br><span class="line">:: loading settings :: url = jar:file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml</span><br><span class="line">org.apache.spark#spark-streaming-kafka-0-10_2.11 added as a dependency</span><br><span class="line">:: resolving dependencies :: org.apache.spark#spark-submit-parent-c0781f34-3101-4762-8694-9aa38b463184;1.0</span><br><span class="line">        confs: [default]</span><br><span class="line">        found org.apache.spark#spark-streaming-kafka-0-10_2.11;2.4.4 in central</span><br><span class="line">        found org.apache.kafka#kafka-clients;2.0.0 in central</span><br><span class="line">        found org.lz4#lz4-java;1.4.0 in central</span><br><span class="line">        found org.xerial.snappy#snappy-java;1.1.7.3 in central</span><br><span class="line">        found org.slf4j#slf4j-api;1.7.16 in central</span><br><span class="line">        found org.spark-project.spark#unused;1.0.0 in central</span><br><span class="line">:: resolution report :: resolve 496ms :: artifacts dl 9ms</span><br><span class="line">        :: modules in use:</span><br><span class="line">        org.apache.kafka#kafka-clients;2.0.0 from central in [default]</span><br><span class="line">        org.apache.spark#spark-streaming-kafka-0-10_2.11;2.4.4 from central in [default]</span><br><span class="line">        org.lz4#lz4-java;1.4.0 from central in [default]</span><br><span class="line">        org.slf4j#slf4j-api;1.7.16 from central in [default]</span><br><span class="line">        org.spark-project.spark#unused;1.0.0 from central in [default]</span><br><span class="line">        org.xerial.snappy#snappy-java;1.1.7.3 from central in [default]</span><br><span class="line">        ---------------------------------------------------------------------</span><br><span class="line">        |                  |            modules            ||   artifacts   |</span><br><span class="line">        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|</span><br><span class="line">        ---------------------------------------------------------------------</span><br><span class="line">        |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |</span><br><span class="line">        ---------------------------------------------------------------------</span><br><span class="line">:: retrieving :: org.apache.spark#spark-submit-parent-c0781f34-3101-4762-8694-9aa38b463184</span><br><span class="line">        confs: [default]</span><br><span class="line">        0 artifacts copied, 6 already retrieved (0kB/10ms)</span><br><span class="line"></span><br><span class="line">我截取了一小部分日志 你看 第一次需要下载maven依赖的  </span><br><span class="line">所以这个 参数也有弊端的  (毕竟公司的服务器是不可能连接外网的  )</span><br><span class="line"></span><br><span class="line">还有其他的方式可以解决 一会介绍</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103152748544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="结果"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">那么刚刚packages  有小问题  那么怎么办呢 ？</span><br><span class="line"></span><br><span class="line">1.先把spark-streaming-kafka-0-10_2.11依赖包 上传到服务器上 </span><br><span class="line">2.通过--jars 来指定</span><br><span class="line"></span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line">--jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; --name StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; --jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar \</span><br><span class="line">&gt; --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">19/11/03 15:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Submitted application: StreamingKakfaDirectYarnApp</span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/11/03 15:38:39 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 45422.</span><br><span class="line">19/11/03 15:38:39 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/11/03 15:38:39 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/11/03 15:38:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/11/03 15:38:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/11/03 15:38:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ed285e0b-aab6-4fa6-a09d-6776f02d7a71</span><br><span class="line">19/11/03 15:38:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/11/03 15:38:39 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/11/03 15:38:39 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/11/03 15:38:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Added JAR file:///home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar at spark://hadoop101:45422/jars/spark-streaming-kafka-0-10_2.11-2.4.4.jar with timestamp 1572766719888</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Added JAR file:/home/double_happy/lib/spark-core-1.0.jar at spark://hadoop101:45422/jars/spark-core-1.0.jar with timestamp 1572766719889</span><br><span class="line">19/11/03 15:38:39 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/11/03 15:38:40 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 40256.</span><br><span class="line">19/11/03 15:38:40 INFO NettyBlockTransferService: Server created on hadoop101:40256</span><br><span class="line">19/11/03 15:38:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/11/03 15:38:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:40256 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572766719940</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp$.main(StreamingKakfaDirectYarnApp.scala:36)</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp.main(StreamingKakfaDirectYarnApp.scala)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 14 more</span><br><span class="line">19/11/03 15:38:41 INFO SparkContext: Invoking stop() from shutdown hook</span><br><span class="line">19/11/03 15:38:41 INFO SparkUI: Stopped Spark web UI at http://hadoop101:4040</span><br><span class="line">19/11/03 15:38:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">19/11/03 15:38:41 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">19/11/03 15:38:41 INFO BlockManager: BlockManager stopped</span><br><span class="line">19/11/03 15:38:41 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">19/11/03 15:38:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">19/11/03 15:38:41 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">19/11/03 15:38:41 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">19/11/03 15:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-7780e105-fa8c-4592-ac12-7d27fc631ccd</span><br><span class="line">19/11/03 15:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-58bc8e97-e4ea-4708-9819-b98f98cb2212</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line"></span><br><span class="line">这个东西和上面一样 因为这个东西是在哪？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103154351394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">因为sparkStreaming-kafka包里面包含kafka-client </span><br><span class="line">你idea里pom 配置一个ss-kafka是可以的 但是到服务器上 是需要kafka-client这个jar包的</span><br><span class="line">所以把它 也上传到服务器上</span><br><span class="line"></span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line">--jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar,/home/double_happy/lib/kafka-clients-2.0.0.jar \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --master local[2] --name StreamingKakfaDirectYarnApp --jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar,/home/double_happy/lib/kafka-clients-2.0.0.jar --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp /home/double_happy/lib/spark-core-1.0.jar hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">19/11/03 15:51:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/11/03 15:51:12 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/11/03 15:51:12 INFO SparkContext: Submitted application: StreamingKakfaDirectYarnApp</span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/11/03 15:51:13 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 44185.</span><br><span class="line">19/11/03 15:51:13 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/11/03 15:51:13 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/11/03 15:51:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d86d96f2-228c-4046-b62b-bbc683c696e8</span><br><span class="line">19/11/03 15:51:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/11/03 15:51:13 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/11/03 15:51:13 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/11/03 15:51:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/11/03 15:51:13 INFO SparkContext: Added JAR file:///home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar at spark://hadoop101:44185/jars/spark-streaming-kafka-0-10_2.11-2.4.4.jar with timestamp 1572767473466</span><br><span class="line">19/11/03 15:51:13 INFO SparkContext: Added JAR file:///home/double_happy/lib/kafka-clients-2.0.0.jar at spark://hadoop101:44185/jars/kafka-clients-2.0.0.jar with timestamp 1572767473467</span><br><span class="line">19/11/03 15:51:13 INFO SparkContext: Added JAR file:/home/double_happy/lib/spark-core-1.0.jar at spark://hadoop101:44185/jars/spark-core-1.0.jar with timestamp 1572767473467</span><br><span class="line">19/11/03 15:51:13 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/11/03 15:51:13 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 34854.</span><br><span class="line">19/11/03 15:51:13 INFO NettyBlockTransferService: Server created on hadoop101:34854</span><br><span class="line">19/11/03 15:51:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:34854 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:14 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572767473528</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding enable.auto.commit to false for executor</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding auto.offset.reset to none for executor</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding executor group.id to spark-executor-double_happy_group3</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5427abd</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7fbff13b</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@57b2814e</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4b876f31</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@5d06636e</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5db077dc</span><br><span class="line">19/11/03 15:51:15 INFO ConsumerConfig: ConsumerConfig values: </span><br><span class="line">        auto.commit.interval.ms = 5000</span><br><span class="line">        auto.offset.reset = earliest</span><br><span class="line">        bootstrap.servers = [hadoop101:9092, hadoop101:9093, hadoop101:9094]</span><br><span class="line">        check.crcs = true</span><br><span class="line">        client.id = </span><br><span class="line">        connections.max.idle.ms = 540000</span><br><span class="line">        default.api.timeout.ms = 60000</span><br><span class="line">        enable.auto.commit = false</span><br><span class="line">        exclude.internal.topics = true</span><br><span class="line">        fetch.max.bytes = 52428800</span><br><span class="line">        fetch.max.wait.ms = 500</span><br><span class="line">        fetch.min.bytes = 1</span><br><span class="line">        group.id = double_happy_group3</span><br><span class="line">        heartbeat.interval.ms = 3000</span><br><span class="line">        interceptor.classes = []</span><br><span class="line">        internal.leave.group.on.close = true</span><br><span class="line">        isolation.level = read_uncommitted</span><br><span class="line">        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">        max.partition.fetch.bytes = 1048576</span><br><span class="line">        max.poll.interval.ms = 300000</span><br><span class="line">        max.poll.records = 500</span><br><span class="line">        metadata.max.age.ms = 300000</span><br><span class="line">        metric.reporters = []</span><br><span class="line">        metrics.num.samples = 2</span><br><span class="line">        metrics.recording.level = INFO</span><br><span class="line">        metrics.sample.window.ms = 30000</span><br><span class="line">        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]</span><br><span class="line">        receive.buffer.bytes = 65536</span><br><span class="line">        reconnect.backoff.max.ms = 1000</span><br><span class="line">        reconnect.backoff.ms = 50</span><br><span class="line">        request.timeout.ms = 30000</span><br><span class="line">        retry.backoff.ms = 100</span><br><span class="line">        sasl.client.callback.handler.class = null</span><br><span class="line">        sasl.jaas.config = null</span><br><span class="line">        sasl.kerberos.kinit.cmd = /usr/bin/kinit</span><br><span class="line">        sasl.kerberos.min.time.before.relogin = 60000</span><br><span class="line">        sasl.kerberos.service.name = null</span><br><span class="line">        sasl.kerberos.ticket.renew.jitter = 0.05</span><br><span class="line">        sasl.kerberos.ticket.renew.window.factor = 0.8</span><br><span class="line">        sasl.login.callback.handler.class = null</span><br><span class="line">        sasl.login.class = null</span><br><span class="line">        sasl.login.refresh.buffer.seconds = 300</span><br><span class="line">        sasl.login.refresh.min.period.seconds = 60</span><br><span class="line">        sasl.login.refresh.window.factor = 0.8</span><br><span class="line">        sasl.login.refresh.window.jitter = 0.05</span><br><span class="line">        sasl.mechanism = GSSAPI</span><br><span class="line">        security.protocol = PLAINTEXT</span><br><span class="line">        send.buffer.bytes = 131072</span><br><span class="line">        session.timeout.ms = 10000</span><br><span class="line">        ssl.cipher.suites = null</span><br><span class="line">        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]</span><br><span class="line">        ssl.endpoint.identification.algorithm = https</span><br><span class="line">        ssl.key.password = null</span><br><span class="line">        ssl.keymanager.algorithm = SunX509</span><br><span class="line">        ssl.keystore.location = null</span><br><span class="line">        ssl.keystore.password = null</span><br><span class="line">        ssl.keystore.type = JKS</span><br><span class="line">        ssl.protocol = TLS</span><br><span class="line">        ssl.provider = null</span><br><span class="line">        ssl.secure.random.implementation = null</span><br><span class="line">        ssl.trustmanager.algorithm = PKIX</span><br><span class="line">        ssl.truststore.location = null</span><br><span class="line">        ssl.truststore.password = null</span><br><span class="line">        ssl.truststore.type = JKS</span><br><span class="line">        value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"></span><br><span class="line">19/11/03 15:51:15 INFO AppInfoParser: Kafka version : 2.0.0</span><br><span class="line">19/11/03 15:51:15 INFO AppInfoParser: Kafka commitId : 3402a8361b734732</span><br><span class="line">19/11/03 15:51:15 INFO Metadata: Cluster ID: QW2v3GZOQYCYmgUBgDaicA</span><br><span class="line">19/11/03 15:51:15 INFO AbstractCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Discovered group coordinator hadoop101:9092 (id: 2147483647 rack: null)</span><br><span class="line">19/11/03 15:51:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Revoking previously assigned partitions []</span><br><span class="line">19/11/03 15:51:15 INFO AbstractCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] (Re-)joining group</span><br><span class="line">19/11/03 15:51:15 INFO AbstractCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Successfully joined group with generation 7</span><br><span class="line">19/11/03 15:51:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Setting newly assigned partitions [double_happy_offset-0, double_happy_offset-1, double_happy_offset-2]</span><br><span class="line">19/11/03 15:51:15 INFO Fetcher: [Consumer clientId=consumer-1, groupId=double_happy_group3] Resetting offset for partition double_happy_offset-1 to offset 0.</span><br><span class="line">19/11/03 15:51:15 INFO Fetcher: [Consumer clientId=consumer-1, groupId=double_happy_group3] Resetting offset for partition double_happy_offset-2 to offset 0.</span><br><span class="line">19/11/03 15:51:15 INFO Fetcher: [Consumer clientId=consumer-1, groupId=double_happy_group3] Resetting offset for partition double_happy_offset-0 to offset 0.</span><br><span class="line">19/11/03 15:51:15 INFO RecurringTimer: Started timer for JobGenerator at time 1572767480000</span><br><span class="line">19/11/03 15:51:15 INFO JobGenerator: Started JobGenerator at 1572767480000 ms</span><br><span class="line">19/11/03 15:51:15 INFO JobScheduler: Started JobScheduler</span><br><span class="line">19/11/03 15:51:15 INFO StreamingContext: StreamingContext started</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">StreamingContext started    ok没有问题</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这样做 只需要把你需要的依赖包拿过来就可以了 </span><br><span class="line"></span><br><span class="line">如果你需要额外的依赖包很多怎么办？</span><br><span class="line"></span><br><span class="line">虽然 --packages  不能去中央仓库去下载 但是你公司应该有一个 maven私服 那么你直接用私服里的就可以  </span><br><span class="line"></span><br><span class="line"> 这样做的好处 就是你spark代码包很小的</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 lib]$ ll -lh</span><br><span class="line">total 2.4M</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 1.9M Nov  3 14:16 kafka-clients-2.0.0.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy  48K Oct 24 23:24 local-1571929727692</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 1.1K Sep 25 19:45 site.log</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 225K Nov  3 15:05 spark-core-1.0.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 212K Nov  3 14:16 spark-streaming-kafka-0-10_2.11-2.4.4.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy  37K Sep 23 18:32 udf.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy  36K Sep 23 11:21 wc.jar</span><br><span class="line">[double_happy@hadoop101 lib]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其实还有一种方式 ：</span><br><span class="line">我们开发的的都是 瘦包 ：仅仅只包含你自己开发的代码 不包括其他的依赖</span><br><span class="line"> </span><br><span class="line"> 瘦包 ：仅仅只包含你自己开发的代码 不包括其他的依赖</span><br><span class="line"> 		包小</span><br><span class="line"> 		需要的依赖的包自己来挑选</span><br><span class="line">胖包：不仅仅会把你自己开发的打包 还会把你的指定的依赖包一起打进去 </span><br><span class="line">		包大</span><br><span class="line">		所有的东西(Hadoop/Spark 除外 )都在里面 运行起来方便</span><br><span class="line">那么胖包怎么使用呢？就是我上面不推荐的链接    因为我之前就用这个方式 修改代码的时候 还得把 那个选项打开 我不喜欢</span><br><span class="line"></span><br><span class="line">瘦包还有一个好处就是 ： 方便升级   胖包真的不好</span><br></pre></td></tr></table></figure></div>


<p><strong>transformation</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">之前写的算子 都是按照每一个批次来处理的 或者是可以累计的等</span><br><span class="line"></span><br><span class="line">新需求：</span><br><span class="line">每隔5秒钟统计前10s钟的数据 </span><br><span class="line">每隔1分钟统计前5分钟的数据</span><br><span class="line"></span><br><span class="line">就是每隔多久统计前多久的数据  那么</span><br><span class="line">这类需求 就是 Window</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations" target="_blank" rel="noopener">Window Operations</a><br>As shown in the figure, every time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream. In this specific case, the operation is applied over the last 3 time units of data, and slides by 2 time units. This shows that any window operation needs to specify two parameters.</p>
<p>window length - The duration of the window (3 in the figure).<br>sliding interval - The interval at which the window operation is performed (2 in the figure).<br>These two parameters must be multiples of the batch interval of the source DStream (1 in the figure).<br><img src="https://img-blog.csdnimg.cn/20191103162228877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>案列</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Return a new DStream by applying `reduceByKey` over a sliding window. This is similar to</span><br><span class="line">   * `DStream.reduceByKey()` but applies it over a sliding window. Hash partitioning is used to</span><br><span class="line">   * generate the RDDs with Spark&apos;s default number of partitions.</span><br><span class="line">   * @param reduceFunc associative and commutative reduce function</span><br><span class="line">   * @param windowDuration width of the window; must be a multiple of this DStream&apos;s</span><br><span class="line">   *                       batching interval</span><br><span class="line">   * @param slideDuration  sliding interval of the window (i.e., the interval after which</span><br><span class="line">   *                       the new DStream will generate RDDs); must be a multiple of this</span><br><span class="line">   *                       DStream&apos;s batching interval</span><br><span class="line">   */</span><br><span class="line">  def reduceByKeyAndWindow(</span><br><span class="line">      reduceFunc: (V, V) =&gt; V,</span><br><span class="line">      windowDuration: Duration,</span><br><span class="line">      slideDuration: Duration</span><br><span class="line">    ): DStream[(K, V)] = ssc.withScope &#123;</span><br><span class="line">    reduceByKeyAndWindow(reduceFunc, windowDuration, slideDuration, defaultPartitioner())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">只要你见Window  参数里一定带 窗口大小 和 滑动大小的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">5秒的批次 每隔5秒统计前10秒</span><br><span class="line"></span><br><span class="line">object StreamingKakfaWindowApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 5)</span><br><span class="line">    </span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1))</span><br><span class="line">      .reduceByKeyAndWindow((a:Int,b:Int) =&gt;</span><br><span class="line">      (a + b),  //窗口内统计两辆相加    业务</span><br><span class="line">      Seconds(10),  //窗口大小</span><br><span class="line">      Seconds(5)) //滑动大小</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,2)</span><br><span class="line">(b,1)</span><br><span class="line">(f,3)</span><br><span class="line">(e,2)</span><br><span class="line">(a,1)</span><br><span class="line">(c,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770065000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,2)</span><br><span class="line">(b,1)</span><br><span class="line">(f,3)</span><br><span class="line">(e,2)</span><br><span class="line">(a,1)</span><br><span class="line">(c,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770075000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">业务理解即可  这是最基本的统计</span><br><span class="line"></span><br><span class="line">问题：下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103164705730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>DataFrame and SQL Operations</strong><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#dataframe-and-sql-operations" target="_blank" rel="noopener">DataFrame and SQL Operations</a></p>
<p>这是批流一体带来的非常大的好处</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">object StreamingSqlApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 5)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    ).map(_.value())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line"></span><br><span class="line">    stream.foreachRDD(rdd =&gt; &#123;   //注意 stream 前面把 value取出来</span><br><span class="line"></span><br><span class="line">      // Get the singleton instance of SparkSession</span><br><span class="line">      val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()</span><br><span class="line">      import spark.implicits._</span><br><span class="line"></span><br><span class="line">      // Convert RDD[String] to DataFrame</span><br><span class="line">      val wordsDataFrame = rdd.toDF(&quot;word&quot;)</span><br><span class="line"></span><br><span class="line">      wordsDataFrame.groupBy(&quot;word&quot;).count().show(false)</span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/11/03 16:55:35 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|f   |25   |</span><br><span class="line">|e   |20   |</span><br><span class="line">|d   |25   |</span><br><span class="line">|c   |23   |</span><br><span class="line">|b   |21   |</span><br><span class="line">|a   |26   |</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line">19/11/03 16:55:40 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line">19/11/03 16:55:45 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">那么这个地方我们使用DF的方式 也可以按sql写 </span><br><span class="line"></span><br><span class="line">官网也有些累加器广播变量在ss里面的使用 和RDD都是一样的  看官网学习</span><br></pre></td></tr></table></figure></div>

<p><strong>消费语义****</strong><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#definitions" target="_blank" rel="noopener">Definitions</a><br>The semantics of streaming systems are often captured in terms of <strong>how many times each record can be processed by the system</strong>. There are three types of guarantees that a system can provide under all possible operating conditions (despite failures, etc.)</p>
<p>1.<strong>At most once</strong>: Each record will be <strong>either processed once or not processed at all.</strong><br>2.<strong>At least once</strong>: Each record will be processed one or more times. This is stronger than at-most once as it ensure that <strong>no data will be lost</strong>.                      <strong>But there may be duplicates</strong>.<br>3.<strong>Exactly once</strong>: Each record will be processed exactly once - <strong>no data will be lost and no data will be processed multiple times</strong>. This is obviously the strongest guarantee of the three.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1.流系统中 你的数据被处理了多少次  根据处理多少次 分为三大类</span><br><span class="line">	At most once  </span><br><span class="line">		 最多一次</span><br><span class="line">		 数据可能有丢失</span><br><span class="line">   At least once    </span><br><span class="line">   		至少一次</span><br><span class="line">   		数据不会丢失 但是数据可能会重复</span><br><span class="line">   Exactly once</span><br><span class="line">   		仅一次</span><br><span class="line">   		数据不丢失 数据不会重复 数据也不会被处理多次</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At most once  ：</span><br><span class="line">	如果ss 消费kafka的数据 先保存offset 再处理结果 (我之前演示的代码 都是最后提交offset) </span><br><span class="line">	但是结果处理挂了 由于offset已经保存了 再处理结果 数据就丢失了 </span><br><span class="line">	所以 一定要先处理结果再保存offset</span><br><span class="line">	</span><br><span class="line"> At least once ：按着上面的方式提交offset</span><br><span class="line"> 	就是结果处理挂了 offset没有提交 再处理结果 数据就重复了 </span><br><span class="line"></span><br><span class="line"> Exactly once：</span><br><span class="line"> 	这个是最完美的 但是***</span><br><span class="line"> 	你要保证它还是有难度的   看官网</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations" target="_blank" rel="noopener">Semantics of output operations</a><br><strong>Output operations (like foreachRDD) have at-least once semantics</strong>, that is, the <strong>transformed data may get written to an external entity more than once in the event of a worker failure.</strong> While this is acceptable for saving to file systems using the saveAs<strong><em>Files operations (as the file will simply get overwritten with the same data), *</em>additional effort may be necessary to achieve exactly-once semantics.</strong> There are two approaches.<br>1.<strong>Idempotent updates</strong>: Multiple attempts always write the same data. For example, <strong>saveAs*</strong>Files** always writes the same data to the generated files.<br>2.<strong>Transactional updates</strong>: All updates are made transactionally so that updates are made exactly once atomically. One way to do this would be the following.</p>
<p>Use the batch time (available in foreachRDD) and the partition index of the RDD to create an identifier. This identifier uniquely identifies a blob data in the streaming application.<br>Update external system with this blob transactionally (that is, exactly once, atomically) using the identifier. That is, if the identifier is not already committed, commit the partition data and the identifier atomically. Else, if this was already committed, skip the update.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">	dstream.foreachRDD &#123; (rdd, time) =&gt;   //time就是你当前批次的时间</span><br><span class="line">  rdd.foreachPartition &#123; partitionIterator =&gt;</span><br><span class="line">    val partitionId = TaskContext.get.partitionId()    //task id </span><br><span class="line">    val uniqueId = generateUniqueId(time.milliseconds, partitionId)   //根据你 的批次的时间 和 task ID 来组成  唯一的一个key (这个key 你每次的操作基于这个key)</span><br><span class="line">    // use this uniqueId to transactionally commit the data in partitionIterator</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Output operations (like foreachRDD) have at-least once semantics</span><br><span class="line"></span><br><span class="line">foreachRDD是保证 at-least onc 这个级别的奥   并不是保证 仅一次的语义</span><br><span class="line"></span><br><span class="line"> two approaches:</span><br><span class="line"> 	1.Idempotent updates  幂等    幂等可以通过主键来控制  主键设计不好等于0</span><br><span class="line">	2.Transactional updates</span><br><span class="line">   3.自己实现  把我们数据和offset绑定  </span><br><span class="line"></span><br><span class="line">也就是说 spark 默认是达到 At least once  </span><br><span class="line"></span><br><span class="line">需要借助At least once 去自己实现 Exactly once</span><br><span class="line"></span><br><span class="line">Exactly once：其实挺简单的 </span><br><span class="line">1.一个是offset提交</span><br><span class="line">2.第二个是 业务数据写出去 </span><br><span class="line">这个两个东西只要有offset能够关联的上  是没有问题的</span><br></pre></td></tr></table></figure></div>

<h2 id="调优"><a href="#调优" class="headerlink" title="调优 ****"></a>调优 ****</h2><p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#performance-tuning" target="_blank" rel="noopener">Performance Tuning</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.减少每隔批次处理的时间</span><br><span class="line">2.设置合理的批次大小  也就是说  你多久跑一个批次</span><br><span class="line"></span><br><span class="line">那么通过案例结合UI讲解</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">object StreamingTuningApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 5)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">查看UI：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103175129217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽——————————————————————————————————</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ok 我们往kafka写10条数据</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103175538708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽——————————————————————————————————<br><img src="https://img-blog.csdnimg.cn/20191103175830927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽——————————————————————————————————<br><img src="https://img-blog.csdnimg.cn/20191103180027441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input Rate：数据输入的速率</span><br><span class="line">Scheduling Delay：每个批次启动任务等待了多少时间被调度  叫 调度的延迟</span><br><span class="line">Processing Time：每个批次处理花费了多少时间</span><br><span class="line">Total Delay：调度延迟 + 处理时间</span><br><span class="line"></span><br><span class="line">这些在ui最下面都能看到</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019110318053852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">最佳实践：</span><br><span class="line">	在下一个批次启动任务之前，一定要运行完前一个批次的数据处理     </span><br><span class="line"></span><br><span class="line">如果你当前批次数据都没有处理完 下一个批次数据进来 也就意味着 你的数据逐渐逐渐堆积的 </span><br><span class="line">你的数据在堆积 也就意味着 后面的作业肯定 对于Scheduling Delay 要花一些时间的 </span><br><span class="line">整个作业运行时间也就越来越长的</span><br><span class="line"></span><br><span class="line">这个就符合官网的两点 ：</span><br><span class="line">	1.合适的batch size  也就是你的这一个批次 尽快的 处理完 不然你这个一个批次 接受数据以后 都不能很快的处理完</span><br><span class="line">		后面的作业逐渐的堆积的 越堆积越多 那么越到后面你的应用程序会完蛋</span><br><span class="line">   那么 batch time 设置多少合适？是根据需求来定的 </span><br><span class="line"></span><br><span class="line">影响任务运行时长的要素有哪些？</span><br><span class="line">	1.数据规模       </span><br><span class="line">			数据量大 一定要多放core （多放core 不一定有用 为什么？ 因为你topic的partition 和RDD的partition是一一对应的）</span><br><span class="line">			可以调整topic的分区数  分区数越多 也就意味着RDD的分区越多   RDD的分区越多task也就越多  task多 并行度就上去了</span><br><span class="line">	2.batch time  </span><br><span class="line">			time越长表示 一个批次的数据越多  数据越多你相同的资源下面 处理数据的时长肯定要多一点</span><br><span class="line">	3.业务复杂度</span><br><span class="line">			如果你的算子用的不好 也就意味着整个 带着大量的shuffle 你的性能会差很多很多  </span><br><span class="line">所以这些东西一定要先测</span><br><span class="line">	batch time 设置 需求来定是一方面  另一个一定到环境上测试 测试得到满意的结果 不是像sb产品经理拍脑袋那样 设置的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103182702262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个地方kafka是有一个限速的</span><br><span class="line"></span><br><span class="line">为了ss程序7*24小时高性能稳定的跑 所以尽可能的 你的批次处理时间和调度间隔 有一个什么关系呢？ 你的批次处理处理时间 要比调度间隔小</span><br><span class="line"></span><br><span class="line">Kafka限速：</span><br><span class="line">  	配置一个参数 </span><br><span class="line">  		 spark.streaming.kafka.maxRatePerPartition  ： </span><br><span class="line">  		 	Maximum rate (number of records per second) read from kafka</span><br><span class="line">  		 	 when using the new Kafka direct stream API</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">修改代码：</span><br><span class="line">  def getStreamingContext(appname:String,batch:Int,defalut:String = &quot;local[2]&quot;) =&#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line">    </span><br><span class="line">    //</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;10&quot;)</span><br><span class="line"></span><br><span class="line">    new StreamingContext(sparkConf,Seconds(batch))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">先测试没有修改前的：</span><br><span class="line"> 同时我写入kafka一些数据   查看结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103183912458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试修改后的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103184011115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">说明这个参数没有生效 emm </span><br><span class="line">我的问题 因为 我们每次往kafka写的数据才10条 我调大一下在测试   改为1000条  我写了两次往kafka里 查看结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103184757992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">看 说面限速成功了  这样第一次处理就很好的限制你能处理的范围内</span><br><span class="line"></span><br><span class="line">但是 300 怎么来的？</span><br><span class="line">	而 这个参数 sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;10&quot;)</span><br><span class="line">	我们设置的是 10 </span><br><span class="line">	为什么ui上面看到的是300呢？</span><br><span class="line"></span><br><span class="line">有个计算公式的 </span><br><span class="line">	10s一个批次 </span><br><span class="line">	topic 3 个分区   ==》数据量 = 10 *3*10 =300 	</span><br><span class="line">	topic 1个分区   ===》 数据量 = 10 *1 *10 =100</span><br><span class="line"></span><br><span class="line">maxRatePerPartition 指的是每一个分区10条	  那么一个topic就是30条  10s就是 300条</span><br><span class="line"></span><br><span class="line">这个参数只适合 direct api  </span><br><span class="line"></span><br><span class="line">限速的地方：</span><br><span class="line">	1.当你topic里有大量没有处理的数据的时候 并且  &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot; 选择earliest （就是从最早消费）</span><br><span class="line">		为了防止第一个批次数据量过大 要设置限速</span><br><span class="line">    2.你的业务高峰期和低峰期的时候数据量是不一样的       高峰期是低峰期数据量的很多倍的</span><br><span class="line">    你不限速 很多作业都会处在等待状态 因为你前面批次的那一点时间已经处理不过来这一批次的数据了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">但是有一个问题哟？</span><br><span class="line"> 我们把消费进来的最大数据量是控制住了 但是这个值是个静态的值  </span><br><span class="line"> </span><br><span class="line"> 假设你的集群吞吐量可以 你的这个值设置小了 怎么办？</span><br><span class="line"> 	随着业务的数据量增长，那么这个东西在生产环境上运行一段时间以后 kafka 消费进来的数据最大的量 应该也</span><br><span class="line"> 	要随着 业务变化而变化就好了 引出一个东西   背压机制</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">背压机制  ： backpressure  1.5版本引进来的</span><br><span class="line">什么是背压呢？</span><br><span class="line">	可以在运行时根据前一个批次数据的运行情况，动态调整后续批次读入的数据量</span><br><span class="line">	这样可以很长从容的面对数据量 突增 和波动的情况 </span><br><span class="line"></span><br><span class="line">这个东西就是一个参数控制一下就ok了 </span><br><span class="line"></span><br><span class="line">spark.streaming.backpressure.enabled</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">背压：它是根据当前批次决定后一个批次 </span><br><span class="line">	</span><br><span class="line">	如果offset 从头开始消费 而且数据量很多的时候   我们启动的时候是从第一个批次启动的</span><br><span class="line">	但是第一个批次 依据谁呢？  没有的</span><br><span class="line">	所以你第一次处理 没有很好的办法评估读取的量  所以还有一个参数 初始化的一个东西</span><br><span class="line">spark.streaming.backpressure.initialRate  用来控制背压初始化读取的数据量</span><br><span class="line">	但是：看下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103191627790.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果按照官方这个描述 数据是从receiver过来的  </span><br><span class="line">而我们是没有receiver 这个东西的  direct是没有receiver的</span><br><span class="line"></span><br><span class="line">这个参数能起作用么？测试一下</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">我设置为150</span><br><span class="line"> def getStreamingContext(appname:String,batch:Int,defalut:String = &quot;local[2]&quot;) =&#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">    //</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;10&quot;)</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;,&quot;true&quot;)</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.backpressure.initialRate&quot;,&quot;150&quot;)</span><br><span class="line"></span><br><span class="line">    new StreamingContext(sparkConf,Seconds(batch))</span><br><span class="line">  &#125;</span><br><span class="line">  查看ui：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103192244253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>不能使用</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那么该怎么办呢？  自己找找答案</span><br></pre></td></tr></table></figure></div>

<h2 id="优雅的关闭JVM"><a href="#优雅的关闭JVM" class="headerlink" title="优雅的关闭JVM"></a>优雅的关闭JVM</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.stopGracefullyOnShutdown  </span><br><span class="line">	If true, Spark shuts down the StreamingContext gracefully on JVM shutdown rather than immediately.</span><br><span class="line">	会缓慢的关闭 而不是直接关闭</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def getStreamingContext(appname: String, batch: Int, defalut: String = &quot;local[2]&quot;) = &#123;</span><br><span class="line"></span><br><span class="line">  val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">  //</span><br><span class="line">  sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;, &quot;10&quot;)</span><br><span class="line">  sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;, &quot;true&quot;)</span><br><span class="line">  sparkConf.set(&quot;spark.streaming.stopGracefullyOnShutdown &quot;, &quot;true&quot;)</span><br><span class="line">  new StreamingContext(sparkConf, Seconds(batch))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            雅恩资源调优---double_happy
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/02/21/SS03/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">SS03</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
          <li>
            <a href="/2019/11/20/tmp/">tmp</a>
          </li>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>