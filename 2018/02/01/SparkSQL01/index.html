<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SparkSQL01 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="工作当中几乎全用SparkSQL ，RDD用的很少(面试多)SparkSQL误区 Spark SQL is Apache Spark’s module for working with structured data.不要把SparkSQL认为就是处理SQl的 或者认为就是写SQLSparkSQL 12345678910111213141516171819202122误区：    1）Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL01">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;01&#x2F;SparkSQL01&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="工作当中几乎全用SparkSQL ，RDD用的很少(面试多)SparkSQL误区 Spark SQL is Apache Spark’s module for working with structured data.不要把SparkSQL认为就是处理SQl的 或者认为就是写SQLSparkSQL 12345678910111213141516171819202122误区：    1）Spark">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191027131424394.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191027131724269.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191027173652956.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191027180607177.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028083903382.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028084355225.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028084118930.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028084634122.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028090741224.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028091025124.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102809233182.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028092452531.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028102115717.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028103234302.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028103316281.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028103611462.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028104324951.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028104719825.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028110000707.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028111043452.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191028113030955.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:07:23.880Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191027131424394.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SparkSQL01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SparkSQL01
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/01/SparkSQL01/" class="article-date">
  <time datetime="2018-02-01T12:06:54.000Z" itemprop="datePublished">2018-02-01</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p>工作当中几乎全用SparkSQL ，RDD用的很少(面试多)<br><strong>SparkSQL误区</strong></p>
<p>Spark SQL is Apache Spark’s module for working with structured data.<br>不要把SparkSQL认为就是处理SQl的 或者认为就是写SQL<br><a href="http://spark.apache.org/sql/" target="_blank" rel="noopener">SparkSQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">误区：</span><br><span class="line">    1）Spark SQL是处理结构化数据</span><br><span class="line">        并不是仅仅能够处理SQL</span><br><span class="line">        SQL仅仅是Spark SQL这个模块的一小部分应用</span><br><span class="line">        API/ExtDS</span><br><span class="line">    2）Uniform Data Access  外部数据源(*****)</span><br><span class="line">        Spark SQL是能够处理多种不同的数据源的数据</span><br><span class="line">            text、json、parquet、orc、hive、jdbc    数据的格式 </span><br><span class="line">            HDFS/S3(a/n)/OSS/COS                数据的存储系统</span><br><span class="line">        不同的数据格式压缩的不压缩的 sparksql都是兼容的 </span><br><span class="line">        你访问不同的数据源SparkSQl都是用统一的访问方式  这就是外部数据源</span><br><span class="line"></span><br><span class="line">SparkSQL能面试的东西 就是两个 ：</span><br><span class="line">	DataFrame 、 外部数据源、catelist </span><br><span class="line"></span><br><span class="line">2.能集成Hive</span><br><span class="line">你的数仓以前是基于Hive来做的 都是Hive的脚本 </span><br><span class="line"> 现在 如果想使用SparkSQL访问Hive的数据 SparkSQL能连接到MetaStore才可以</span><br><span class="line"> (把Hive-site.xml  拷贝到Sparkconf目录下就可以了)</span><br><span class="line"> 因为MetaStore 是 on Hadoop的核心所在 </span><br><span class="line"></span><br><span class="line">所以你要把Hive迁移到Spark上来 成本是很低的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027131424394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3.Standard Connectivity</span><br><span class="line">Hive能通过HiveServer2提供一个服务 大家去查，那么 spark里面有个thriftServer </span><br><span class="line">他们底层都是用thrift协议的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027131724269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">误区3：</span><br><span class="line">MR==&gt;Hive==&gt;  Hive底层当时是MR 慢 所以出来Spark </span><br><span class="line">     Spark==&gt; AMPLab Shark(为了将Hive SQL跑在Spark上)  1.x  配套一个打了补丁的Hive</span><br><span class="line">        Spark1.0  Shark不维护</span><br><span class="line">            ==&gt; Spark SQL 是在Spark里面的</span><br><span class="line">            ==&gt; Hive on Spark 是在Hive里面的      是Hive的引擎是Spark</span><br><span class="line"></span><br><span class="line">误区3）</span><br><span class="line">    Hive on Spark不是Spark SQL</span><br><span class="line">        Hive刚开始时底层执行引擎只有一个：MR</span><br><span class="line">        后期：Tez Spark</span><br><span class="line">        set hive.execution.engine=spark;    就可以 Hive on Spark</span><br><span class="line"></span><br><span class="line">    SparkSQL on Hive  X</span><br></pre></td></tr></table></figure></div>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started" target="_blank" rel="noopener">Hive On Spark</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Time taken: 6.86 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; set hive.execution.engine;</span><br><span class="line">hive.execution.engine=mr</span><br><span class="line">hive (default)&gt; set hive.execution.engine=spark;</span><br><span class="line">hive (default)&gt; set hive.execution.engine;</span><br><span class="line">hive.execution.engine=spark</span><br><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">default</span><br><span class="line">homework</span><br><span class="line">Time taken: 0.008 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个东西了解即可 Hive On Spark 真正生产上用的很少的 </span><br><span class="line">这个东西不是很成熟的</span><br></pre></td></tr></table></figure></div>

<h2 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">出来的时间：</span><br><span class="line"></span><br><span class="line">Spark SQL</span><br><span class="line">    1.0     </span><br><span class="line">    SchemaRDD  ==&gt; Table     RDD(存数据) + schema = Table</span><br><span class="line">    ==&gt; DataFrame  1.2/3     由SchemaRDD  变为DataFrame 原因是 更加 OO</span><br><span class="line">    ==&gt; Dataset    1.6    由DataFrame  变为Dataset 因为 compile-time type safety</span><br></pre></td></tr></table></figure></div>
<p><strong>DataFrame</strong><br>A Dataset is a distributed collection of data.<br>A DataFrame is a Dataset organized into named columns.<br>DataFrame = Dataset[Row]<br>In Scala and Java, a DataFrame is represented by a Dataset of Rows. </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataFrame ：</span><br><span class="line">	1.named columns    就是一个表  包含 列的名字 + 列的类型 </span><br><span class="line">	</span><br><span class="line">Row ： 可以理解为  一行数据 没有scheme的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SparkSession是Spark编程的入口点</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027173652956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Api：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SparkSession：</span><br><span class="line">  /**</span><br><span class="line">   * Executes a SQL query using Spark, returning the result as a `DataFrame`.</span><br><span class="line">   * The dialect that is used for SQL parsing can be configured with &apos;spark.sql.dialect&apos;.</span><br><span class="line">   *</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def sql(sqlText: String): DataFrame = &#123;</span><br><span class="line">    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">	1. returning the result as a `DataFrame`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Dataset：</span><br><span class="line">  /**</span><br><span class="line">   * Displays the top 20 rows of Dataset in a tabular form.</span><br><span class="line">   *</span><br><span class="line">   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</span><br><span class="line">   *                 be truncated and all cells will be aligned right</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def show(truncate: Boolean): Unit = show(20, truncate)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;show tables&quot;).show</span><br><span class="line"></span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| default|  student|      false|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	启动spark-shell的时候  指定MySQL驱动  </span><br><span class="line">	个人建议使用 --jars 指定MySQL驱动 </span><br><span class="line">	不建议把MySQL驱动 直接丢在Spark jar路径里</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">查看Hive里元数据：</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from DBS;</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">| DB_ID | DESC                  | DB_LOCATION_URI                                       | NAME     | OWNER_NAME   | OWNER_TYPE |</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">|     1 | Default Hive database | hdfs://hadoop101:8020/user/hive/warehouse             | default  | public       | ROLE       |</span><br><span class="line">|     6 | NULL                  | hdfs://hadoop101:8020/user/hive/warehouse/homework.db | homework | double_happy | USER       |</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from TBLS;</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER        | RETENTION | SD_ID | TBL_NAME                          | TBL_TYPE       | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT |</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">|      1 |  1568615059 |     1 |                0 | double_happy |         0 |     1 | student                           | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|      8 |  1568616039 |     6 |                0 | double_happy |         0 |     8 | ods_domain_traffic_info           | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|      9 |  1568620410 |     6 |                0 | double_happy |         0 |     9 | ods_uid_pid_info                  | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     17 |  1568860945 |     6 |                0 | double_happy |         0 |    17 | jf_tmp                            | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     21 |  1569056727 |     6 |                0 | double_happy |         0 |    21 | access_wide                       | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     26 |  1569209493 |     6 |                0 | double_happy |         0 |    31 | ods_uid_pid_info_compression_test | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     27 |  1569209946 |     6 |                0 | double_happy |         0 |    32 | ods_uid_pid_compression_info      | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     31 |  1569224142 |     6 |                0 | double_happy |         0 |    36 | dwd_platform_stat_info            | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     53 |  1570957119 |     6 |                0 | double_happy |         0 |    63 | ods_log_info                      | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">9 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell查询Hive里的表：</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select * from homework.dwd_platform_stat_info&quot;).show</span><br><span class="line">+--------+---+--------+--------+                                                </span><br><span class="line">|platform|cnt|       d|     day|</span><br><span class="line">+--------+---+--------+--------+</span><br><span class="line">| Andriod|658|20190921|20190921|</span><br><span class="line">| Symbain|683|20190921|20190921|</span><br><span class="line">|   linux|639|20190921|20190921|</span><br><span class="line">|     mac|652|20190921|20190921|</span><br><span class="line">| windows|640|20190921|20190921|</span><br><span class="line">+--------+---+--------+--------+</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用sparksql 在spark-shell交互 还得写 spark.sql</span><br><span class="line">在spark里 有个 spark-sql  用法和 spark-shell 是一样的</span><br></pre></td></tr></table></figure></div>

<h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a><strong>编程</strong></h2><p><a href="http://spark.apache.org/docs/latest/sql-getting-started.html" target="_blank" rel="noopener">sparksql编程</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1.SparkSession构建</span><br><span class="line"></span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">当然 你spark一些参数如何传进去呢？</span><br><span class="line">提供config传进去</span><br><span class="line">eg ： 你要设置多少个分区呀 等</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027180607177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Data Sources</strong></p>
<p><strong>1.读文本数据</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">1.读文本数据</span><br><span class="line"></span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+---------------+</span><br><span class="line">|          value|</span><br><span class="line">+---------------+</span><br><span class="line">|double_happy,25|</span><br><span class="line">|      Kairis,25|</span><br><span class="line">|        Kite,32|</span><br><span class="line">+---------------+</span><br><span class="line"></span><br><span class="line">1. 但是有一个问题 读取进来的数据   把所有内容</span><br><span class="line">都放到 value这个列 里面去了 </span><br><span class="line">该怎么办？</span><br><span class="line"></span><br><span class="line">2. 上面那种写法读进来的是DF</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def text(spark: SparkSession) = &#123;</span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">        ds.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">读进来的是DS</span><br><span class="line">结果是一样的：</span><br><span class="line">+---------------+</span><br><span class="line">|          value|</span><br><span class="line">+---------------+</span><br><span class="line">|double_happy,25|</span><br><span class="line">|      Kairis,25|</span><br><span class="line">|        Kite,32|</span><br><span class="line">+---------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Loads text files and returns a [[Dataset]] of String. See the documentation on the</span><br><span class="line">   * other overloaded `textFile()` method for more details.</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def textFile(path: String): Dataset[String] = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    textFile(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">可以传入多个路径的    textFile(Seq(path): _*)</span><br></pre></td></tr></table></figure></div>

<p><strong>取出第一列输出出去  注意df 和ds的区别</strong> </p>
<p>df：<br><img src="https://img-blog.csdnimg.cn/20191028083903382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028084355225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028084118930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. df.map  里面是row     ds.map  里面是String</span><br><span class="line"> 2. ds 可以map 里面 x.split </span><br><span class="line">   df 就不可以 </span><br><span class="line"> 那我要取出第一列使用df 该这么办？</span><br><span class="line">这就是 df 和 ds 编程的 最本质的区别   df = ds[Row]</span><br><span class="line"></span><br><span class="line">所以 df 得使用  df.rdd.map  </span><br><span class="line">而且他的返回值是 rdd</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028084634122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val result: RDD[(String, String)] = df.rdd.map(x =&gt; &#123;</span><br><span class="line">      val tmp: String = x.getString(0)</span><br><span class="line">      val splits: Array[String] = tmp.split(&quot;,&quot;)</span><br><span class="line">      (splits(0), splits(1))</span><br><span class="line">    &#125;)</span><br><span class="line">    result.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(double_happy,25)</span><br><span class="line">(Kairis,25)</span><br><span class="line">(Kite,32)</span><br><span class="line"></span><br><span class="line">这个结果不是我们想要的 ，我要的是 把结果写出去 </span><br><span class="line">上面这种是 df的 那么 ds该怎么操作呢？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS: Dataset[(String, String)] = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      (splits(0), splits(1))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028090741224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">那么我们只输出一列 ：</span><br><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      splits(0)</span><br><span class="line">    &#125;)</span><br><span class="line">    resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果 ：ok</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028091025124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">但是 有个问题的 文本格式是非常常用的格式  你只支持  一列输出 有个鬼用</span><br><span class="line"></span><br><span class="line">这个问题该这么解决呢？</span><br><span class="line">  这个问题很重要 前面的 不同类型日志输出  一定是多列的  </span><br><span class="line">  下面讲到压缩  给你一个场景 </span><br><span class="line">  andriod 的 bzip的  ios gzip 的   windos bz2  你该这么办？  这都是常见的需求</span><br></pre></td></tr></table></figure></div>
<p>上面的问题之后再解决 </p>
<p>那么 这个输出的数据也是可以用压缩的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      (splits(0))</span><br><span class="line">    &#125;)</span><br><span class="line">   resultDS.write.option(&quot;compression&quot;,&quot;gzip&quot;).mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102809233182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>注意：<br><img src="https://img-blog.csdnimg.cn/20191028092452531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说这个压缩 codec 是有限制的 </span><br><span class="line">问题：让是输出使用lzo 压缩该怎么办呢？</span><br></pre></td></tr></table></figure></div>
<p><strong>2.读json数据</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191028102115717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    json(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- _corrupt_record: string (nullable = true)</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- branch: string (nullable = true)</span><br><span class="line"> |-- camera_id: string (nullable = true)</span><br><span class="line"> |-- camera_ip: string (nullable = true)</span><br><span class="line"> |-- client_time: struct (nullable = true)</span><br><span class="line"> |    |-- enter_time: long (nullable = true)</span><br><span class="line"> |    |-- exit_time: long (nullable = true)</span><br><span class="line"> |    |-- first_time: long (nullable = true)</span><br><span class="line"> |    |-- last_time: long (nullable = true)</span><br><span class="line"> |-- events: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- host_time: long (nullable = true)</span><br><span class="line"> |    |    |-- name: string (nullable = true)</span><br><span class="line"> |    |    |-- osd_time: long (nullable = true)</span><br><span class="line"> |-- face_id: string (nullable = true)</span><br><span class="line"> |-- gender: long (nullable = true)</span><br><span class="line"> |-- is_new_user: boolean (nullable = true)</span><br><span class="line"> |-- mall_id: string (nullable = true)</span><br><span class="line"> |-- match_photo_index: long (nullable = true)</span><br><span class="line"> |-- match_score: long (nullable = true)</span><br><span class="line"> |-- package_index: long (nullable = true)</span><br><span class="line"> |-- photos: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- frame_time: long (nullable = true)</span><br><span class="line"> |    |    |-- quality: double (nullable = true)</span><br><span class="line"> |    |    |-- url: string (nullable = true)</span><br><span class="line"> |-- process_context: struct (nullable = true)</span><br><span class="line"> |    |-- history_res: string (nullable = true)</span><br><span class="line"> |    |-- temp_res: string (nullable = true)</span><br><span class="line"> |-- process_end_time: long (nullable = true)</span><br><span class="line"> |-- process_start_time: long (nullable = true)</span><br><span class="line"> |-- product_id: string (nullable = true)</span><br><span class="line"> |-- project_id: string (nullable = true)</span><br><span class="line"> |-- race: long (nullable = true)</span><br><span class="line"> |-- request_id: string (nullable = true)</span><br><span class="line"> |-- request_time: long (nullable = true)</span><br><span class="line"> |-- site_id: string (nullable = true)</span><br><span class="line"> |-- status: long (nullable = true)</span><br><span class="line"> |-- temp_id: string (nullable = true)</span><br><span class="line"> |-- tracks: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- box: struct (nullable = true)</span><br><span class="line"> |    |    |    |-- angle: long (nullable = true)</span><br><span class="line"> |    |    |    |-- height: long (nullable = true)</span><br><span class="line"> |    |    |    |-- left: long (nullable = true)</span><br><span class="line"> |    |    |    |-- top: long (nullable = true)</span><br><span class="line"> |    |    |    |-- width: long (nullable = true)</span><br><span class="line"> |    |    |-- host_time: long (nullable = true)</span><br><span class="line"> |    |    |-- index: long (nullable = true)</span><br><span class="line"> |    |    |-- video_time: long (nullable = true)</span><br><span class="line"> |-- user_id: string (nullable = true)</span><br><span class="line"></span><br><span class="line">19/10/28 10:17:49 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &apos;spark.debug.maxToStringFields&apos; in SparkEnv.conf.</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">|_corrupt_record|age|              branch|           camera_id|  camera_ip|         client_time|              events|             face_id|gender|is_new_user|           mall_id|match_photo_index|match_score|package_index|              photos|process_context|process_end_time|process_start_time| product_id|          project_id|race|          request_id| request_time|           site_id|status|temp_id|              tracks|user_id|</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">|           null|  0|low_quality_faceI...|afu-hanghai-yxhqg...|172.16.10.2|[1555054289000, 1...|[[1555073266644, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                0|          0|            9|[[1555054284000, ...|   [null, null]|   1555073288125|     1555073288099|trafficfull|AFU_shanghai_yxhq...|   0|f0cbcac5-60aa-498...|1555073288097|AFU_shanghai_yxhqg|    -1|       |[[[-21, 62, 884, ...|       |</span><br><span class="line">|           null| 23|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055539...|[[1555073289722, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           -1|[[1555055540000, ...|   [null, null]|   1555073292530|     1555073292487|trafficfull|AFU_beijing_xhm_t...|   0|b264f039-431e-4c9...|1555073292487|   AFU_beijing_xhm|     4|       |[[[0, 74, 1656, 1...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            3|[[1555054311000, ...|   [null, null]|   1555073297234|     1555073297137|trafficfull|AFU_shanghai_yxhq...|   1|9e9b0963-96d9-44b...|1555073297136|AFU_shanghai_yxhqg|    -1|       |[[[12, 88, 674, 4...|       |</span><br><span class="line">|           null| 22|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055539...|[[1555073289893, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|            1|[[1555055543000, ...|   [null, null]|   1555073298078|     1555073298034|trafficfull|AFU_beijing_xhm_t...|   0|8703d95e-8ca2-4a8...|1555073298034|   AFU_beijing_xhm|    -1|       |[[[7, 72, 171, 56...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            5|[[1555054311000, ...|   [null, null]|   1555073300572|     1555073300471|trafficfull|AFU_shanghai_yxhq...|   1|163a8256-d832-427...|1555073300471|AFU_shanghai_yxhqg|    -1|       |[[[19, 96, 625, 3...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-cytj-...|172.16.10.2|[0, 0, 1555068460...|[[1555073300056, ...|PROJAFU_beijing_c...|     0|      false|  AFU_beijing_cytj|                0|          0|           -1|                null|           [, ]|   1555073300572|     1555073300572|trafficfull|AFU_beijing_cytj_...|   0|5499d569-4067-42d...|1555073300494|  AFU_beijing_cytj|     4|       |[[[26, 55, 1341, ...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           14|[[1555055520000, ...|   [null, null]|   1555073301353|     1555073301300|trafficfull|AFU_beijing_xhm_t...|   1|80f35edb-e3c9-48f...|1555073301299|   AFU_beijing_xhm|    -1|       |[[[25, 110, 1269,...|       |</span><br><span class="line">|           null| 29|  low_quality_faceId|afu-beijing-cytj-...|172.16.10.2|[0, 0, 1555068461...|[[1555073300728, ...|PROJAFU_beijing_c...|     0|       true|  AFU_beijing_cytj|                0|          0|           -1|[[1555068461000, ...|   [null, null]|   1555073302108|     1555073302059|trafficfull|AFU_beijing_cytj_...|   1|ddbacad2-cd35-4a1...|1555073302058|  AFU_beijing_cytj|     4|       |[[[1, 108, 825, 1...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            6|[[1555054311000, ...|   [null, null]|   1555073302221|     1555073302126|trafficfull|AFU_shanghai_yxhq...|   1|3b0353b3-d5ec-492...|1555073302125|AFU_shanghai_yxhqg|    -1|       |[[[4, 85, 647, 39...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055554...|[[1555073303646, ...|PROJAFU_beijing_x...|     1|       true|   AFU_beijing_xhm|                0|          0|           -1|[[1555055554000, ...|   [null, null]|   1555073305191|     1555073305148|trafficfull|AFU_beijing_xhm_t...|   0|3bd7e125-ac80-4ff...|1555073305148|   AFU_beijing_xhm|     4|       |[[[11, 63, 925, 1...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           17|[[1555055520000, ...|   [null, null]|   1555073306338|     1555073306297|trafficfull|AFU_beijing_xhm_t...|   1|26383dcd-47a4-410...|1555073306297|   AFU_beijing_xhm|    -1|       |[[[11, 101, 1254,...|       |</span><br><span class="line">|           null| 26|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055563...|[[1555073312893, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                1|          0|           -1|[[1555055564000, ...|   [null, null]|   1555073314733|     1555073314663|trafficfull|AFU_beijing_xhm_t...|   0|e8a517a4-bf72-46f...|1555073314663|   AFU_beijing_xhm|     4|       |[[[0, 102, 554, 2...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055566...|[[1555073315646, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073315801|     1555073315801|trafficfull|AFU_beijing_xhm_t...|   0|1256cd65-3100-448...|1555073315797|   AFU_beijing_xhm|     4|       |[[[-8, 79, 1638, ...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           23|[[1555055520000, ...|   [null, null]|   1555073316495|     1555073316453|trafficfull|AFU_beijing_xhm_t...|   1|292ea3d7-cc3b-452...|1555073316453|   AFU_beijing_xhm|    -1|       |[[[27, 99, 1243, ...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055567...|[[1555073316558, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073316856|     1555073316856|trafficfull|AFU_beijing_xhm_t...|   0|caf82eb1-8f49-485...|1555073316856|   AFU_beijing_xhm|     4|       |[[[0, 68, 1695, 3...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055567...|[[1555073316313, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073317218|     1555073317218|trafficfull|AFU_beijing_xhm_t...|   0|868a5c14-903e-461...|1555073317129|   AFU_beijing_xhm|     4|       |[[[9, 129, 993, 3...|       |</span><br><span class="line">|           null| 31|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055564000, 1...|[[1555073302556, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                2|          0|            5|[[1555055558000, ...|   [null, null]|   1555073317592|     1555073317503|trafficfull|AFU_beijing_xhm_t...|   0|bb3b4831-f1db-4f9...|1555073317503|   AFU_beijing_xhm|    -1|       |[[[6, 91, 172, 56...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055569...|[[1555073318311, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073318529|     1555073318529|trafficfull|AFU_beijing_xhm_t...|   0|312c43a4-a247-464...|1555073318529|   AFU_beijing_xhm|     4|       |[[[2, 73, 1024, 1...|       |</span><br><span class="line">|           null| 31|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055564000, 1...|[[1555073302556, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                2|          0|            7|[[1555055558000, ...|   [null, null]|   1555073320823|     1555073320721|trafficfull|AFU_beijing_xhm_t...|   0|da438ef2-daf6-472...|1555073320721|   AFU_beijing_xhm|    -1|       |[[[18, 105, 186, ...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|          -18|[[1555054311000, ...|   [null, null]|   1555073321796|     1555073321700|trafficfull|AFU_shanghai_yxhq...|   1|d16a278f-3ae9-44a...|1555073321700|AFU_shanghai_yxhqg|     4|       |[[[10, 58, 598, 3...|       |</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&quot;is_new_user = true &quot;).show(10)</span><br><span class="line">    </span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).where(&quot;is_new_user = true&quot;).show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">结果是一样的哈 </span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Filters rows using the given SQL expression.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   peopleDs.where(&quot;age &gt; 15&quot;)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def where(conditionExpr: String): Dataset[T] = &#123;</span><br><span class="line">    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">where 底层调用的是 filter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter 和where 里面 有好多中写法 ：</span><br><span class="line">个人喜欢使用 &apos;列名 +判断条件</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028103234302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是报错：  加一个隐式转换</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028103316281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&apos;is_new_user === &quot;true&quot;).show(10)</span><br><span class="line">  &#125;</span><br><span class="line">结果是：</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191028103611462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">写法很多 ：</span><br><span class="line"> def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(df.col(&quot;is_new_user&quot;) === &quot;true&quot;).show(10)</span><br><span class="line">  &#125;</span><br><span class="line">结果是一样的</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">我个人是喜欢 </span><br><span class="line">import spark.implicits._</span><br><span class="line">select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;)  +  .filter(&apos;is_new_user === &quot;true&quot;)    </span><br><span class="line"></span><br><span class="line">这样写代码量少一些</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDF: Dataset[Row] = df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&apos;is_new_user === &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">    resultDF.write</span><br><span class="line">      .mode(SaveMode.Overwrite)</span><br><span class="line">      .format(&quot;json&quot;)</span><br><span class="line">      .save(&quot;file:///C:/IdeaProjects/spark/out-sparksql-json&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    json(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028104324951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">解析json 嵌套 + Sturct类型的 你会么？  给个思路 就是  exploded +打点</span><br></pre></td></tr></table></figure></div>
<p><strong>3.读csv数据</strong><br>csv文件打开是execel能看见的<br><img src="https://img-blog.csdnimg.cn/20191028104719825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- _c0: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line">|                 _c0|</span><br><span class="line">+--------------------+</span><br><span class="line">|pid	pid_type	stor...|</span><br><span class="line">|2637034	GLOBAL	30...|</span><br><span class="line">|127599	GLOBAL	303...|</span><br><span class="line">|2626026	GLOBAL	30...|</span><br><span class="line">|2643291	GLOBAL	30...|</span><br><span class="line">|182310	GLOBAL	303...|</span><br><span class="line">|182310	GLOBAL	303...|</span><br><span class="line">|856248	GLOBAL	303...|</span><br><span class="line">|29052	GLOBAL	3039...|</span><br><span class="line">|29052	GLOBAL	3039...|</span><br><span class="line">+--------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">所以这种处理结果不是我们想要的 </span><br><span class="line">所以处理 csv 文件的时候 需要一些 option 需要我们添加的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- pid	pid_type	store_id	store_name	floor	start_time	end_time	event_type	label_version	channel: string (nullable = true)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">|pid	pid_type	store_id	store_name	floor	start_time	end_time	event_type	label_version	channel|</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">|                                                                       2637034	GLOBAL	30...|</span><br><span class="line">|                                                                       127599	GLOBAL	303...|</span><br><span class="line">|                                                                       2626026	GLOBAL	30...|</span><br><span class="line">|                                                                       2643291	GLOBAL	30...|</span><br><span class="line">|                                                                       182310	GLOBAL	303...|</span><br><span class="line">|                                                                       182310	GLOBAL	303...|</span><br><span class="line">|                                                                       856248	GLOBAL	303...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">表 头出来了  但是不是我们想要的</span><br><span class="line">这个头 就一列  没有分开  所以 还得加option  把头拆开</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;\t&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- pid: string (nullable = true)</span><br><span class="line"> |-- pid_type: string (nullable = true)</span><br><span class="line"> |-- store_id: string (nullable = true)</span><br><span class="line"> |-- store_name: string (nullable = true)</span><br><span class="line"> |-- floor: string (nullable = true)</span><br><span class="line"> |-- start_time: string (nullable = true)</span><br><span class="line"> |-- end_time: string (nullable = true)</span><br><span class="line"> |-- event_type: string (nullable = true)</span><br><span class="line"> |-- label_version: string (nullable = true)</span><br><span class="line"> |-- channel: string (nullable = true)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">|    pid|pid_type|store_id|    store_name|floor|start_time|end_time|event_type|       label_version|channel|</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">|2637034|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  17:38:44|17:39:32|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 127599|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  20:09:26|20:18:03|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|2626026|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  11:38:21|11:38:50|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|2643291|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  21:07:31|21:09:01|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 182310|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  10:41:34|10:41:55|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 182310|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  10:42:02|10:57:19|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 856248|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:49:23|14:56:18|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  13:12:00|13:13:57|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:14:28|14:14:55|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:30:38|14:30:52|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;\t&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    df.select(&quot;pid&quot;,&quot;store_name&quot;).filter($&quot;store_id&quot; === &quot;3039A&quot;)</span><br><span class="line">      .write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;)</span><br><span class="line">      .save(&quot;file:///C:/IdeaProjects/spark/out-sparksql-csv&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028110000707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些 option 参数 我是怎么知道的 ？去源码里找 </span><br><span class="line">CSVOptions 类下面</span><br></pre></td></tr></table></figure></div>
<p><strong>4.读jdbc数据</strong><br>MySQL中的数据是这样的<br><img src="https://img-blog.csdnimg.cn/20191028111043452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def jdbc(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val jdbcDF = spark.read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">        jdbcDF.show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是:</span><br><span class="line">+---------------+-----+---+</span><br><span class="line">|         domain|  url|cnt|</span><br><span class="line">+---------------+-----+---+</span><br><span class="line">|  www.baidu.com| url5|  5|</span><br><span class="line">|  www.baidu.com| url2|  2|</span><br><span class="line">|  www.baidu.com| url4|  4|</span><br><span class="line">|  www.baidu.com| url1|  1|</span><br><span class="line">|  www.baidu.com| url3|  3|</span><br><span class="line">|www.twitter.com| url6|  1|</span><br><span class="line">|www.twitter.com|url10| 11|</span><br><span class="line">|www.twitter.com| url9|  6|</span><br><span class="line">| www.google.com| url2|  2|</span><br><span class="line">| www.google.com| url6|  7|</span><br><span class="line">| www.google.com| url1|  1|</span><br><span class="line">| www.google.com| url8|  7|</span><br><span class="line">+---------------+-----+---+</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank" rel="noopener">JDBC To Other Databases</a><br>官网有好多写法 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line">  def jdbc(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val jdbcDF = spark.read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">        jdbcDF.show()</span><br><span class="line"></span><br><span class="line">    jdbcDF.filter(&apos;domain === &quot;www.google.com&quot;)</span><br><span class="line">      .write.format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn_2&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .save()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：写回MySQL</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show tables;</span><br><span class="line">+--------------------+</span><br><span class="line">| Tables_in_hive_dwd |</span><br><span class="line">+--------------------+</span><br><span class="line">| stat               |</span><br><span class="line">| topn               |</span><br><span class="line">| topn_2             |</span><br><span class="line">+--------------------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from topn_2;</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| domain         | url  | cnt  |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| www.google.com | url2 |    2 |</span><br><span class="line">| www.google.com | url6 |    7 |</span><br><span class="line">| www.google.com | url1 |    1 |</span><br><span class="line">| www.google.com | url8 |    7 |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">但是按照上面写 是不是太恶心了 参数 全都写死的 </span><br><span class="line">通过读取配置文件的方式  ：有很多种写法  这里列出一个</span><br><span class="line"></span><br><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">    def jdbc2(spark: SparkSession) = &#123;</span><br><span class="line">      import spark.implicits._</span><br><span class="line"></span><br><span class="line">      val config = ConfigFactory.load()</span><br><span class="line">      val url = config.getString(&quot;db.default.url&quot;)</span><br><span class="line">      val user = config.getString(&quot;db.default.user&quot;)</span><br><span class="line">      val password = config.getString(&quot;db.default.password&quot;)</span><br><span class="line">      val srcTable = config.getString(&quot;db.default.srctable&quot;)</span><br><span class="line">      val targetTable = config.getString(&quot;db.default.targettable&quot;)</span><br><span class="line"></span><br><span class="line">      val jdbcDF = spark.read</span><br><span class="line">        .format(&quot;jdbc&quot;)</span><br><span class="line">        .option(&quot;url&quot;, url)</span><br><span class="line">        .option(&quot;dbtable&quot;, srcTable)</span><br><span class="line">        .option(&quot;user&quot;, user)</span><br><span class="line">        .option(&quot;password&quot;, password)</span><br><span class="line">        .load()</span><br><span class="line"></span><br><span class="line">          jdbcDF.show()</span><br><span class="line"></span><br><span class="line">      jdbcDF.filter(&apos;domain === &quot;www.google.com&quot;)</span><br><span class="line">        .write.format(&quot;jdbc&quot;)</span><br><span class="line">        .option(&quot;url&quot;, url)</span><br><span class="line">        .option(&quot;dbtable&quot;, targetTable)</span><br><span class="line">        .option(&quot;user&quot;, user)</span><br><span class="line">        .option(&quot;password&quot;, password)</span><br><span class="line">        .save()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc2(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028113030955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from topn_3;</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| domain         | url  | cnt  |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| www.google.com | url2 |    2 |</span><br><span class="line">| www.google.com | url6 |    7 |</span><br><span class="line">| www.google.com | url1 |    1 |</span><br><span class="line">| www.google.com | url8 |    7 |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/02/SparkSQL002/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            SparkSQL002
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/28/Spark11-shuffle/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark11-shuffle</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Kudu-Impala%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B01-double-happy/">Kudu+Impala故障案例01--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink04-double-happy/">Flink04--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink03-double-happy/">Flink03--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink02-double-happy/">Flink02--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>