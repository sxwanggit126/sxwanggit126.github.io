<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>SS03 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="12345678批流一体：未来的发展趋势    Spark    Flink   他们可以做到 MR&#x2F;Spark&#x2F;Flink on YARN：是现在是主流方式 但是 k8s是未来的主流  等学到容器 用这个  1234Spark Streaming provides two categories of built-in streaming sources.Basic sources: Source">
<meta property="og:type" content="article">
<meta property="og:title" content="SS03">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;02&#x2F;21&#x2F;SS03&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="12345678批流一体：未来的发展趋势    Spark    Flink   他们可以做到 MR&#x2F;Spark&#x2F;Flink on YARN：是现在是主流方式 但是 k8s是未来的主流  等学到容器 用这个  1234Spark Streaming provides two categories of built-in streaming sources.Basic sources: Source">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101105050468.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101115349607.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019110111454712.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019110114001642.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101140557306.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101142414924.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101143014894.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101143701991.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101145733172.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019110114582759.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101150455299.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101162513463.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101162924889.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:12:32.556Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191101105050468.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-SS03" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      SS03
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/21/SS03/" class="article-date">
  <time datetime="2018-02-21T12:12:09.000Z" itemprop="datePublished">2018-02-21</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">批流一体：未来的发展趋势</span><br><span class="line">    Spark</span><br><span class="line">    Flink</span><br><span class="line">   他们可以做到 </span><br><span class="line"></span><br><span class="line">MR/Spark/Flink on YARN：是现在是主流方式 </span><br><span class="line"></span><br><span class="line">但是 k8s是未来的主流  等学到容器 用这个</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming provides two categories of built-in streaming sources.</span><br><span class="line"></span><br><span class="line">Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.</span><br><span class="line">Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Some of these advanced sources are as follows.</span><br><span class="line"></span><br><span class="line">Kafka: Spark Streaming 2.4.4 is compatible with Kafka broker versions 0.8.2.1 or higher. See the Kafka Integration Guide for more details.</span><br><span class="line"></span><br><span class="line">Flume: Spark Streaming 2.4.4 is compatible with Flume 1.6.0. See the Flume Integration Guide for more details.</span><br><span class="line"></span><br><span class="line">Kinesis: Spark Streaming 2.4.4 is compatible with Kinesis Client Library 1.2.1. See the Kinesis Integration Guide for more details.</span><br></pre></td></tr></table></figure></div>
<p><strong>Kafka整合</strong></p>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html#spark-streaming-kafka-integration-guide" target="_blank" rel="noopener">Spark Streaming + Kafka Integration Guide</a></p>
<p>The Kafka project introduced <strong>a new consumer API between versions 0.8 and 0.10,</strong> so there are 2 separate corresponding Spark Streaming packages available. Please choose the correct package for your brokers and desired features; note that the 0.8 integration is compatible with later 0.9 and 0.10 brokers, but the 0.10 integration is not compatible with earlier brokers.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.高阶Api</span><br><span class="line">2.低阶Api  </span><br><span class="line">	就是offset需要我们自己维护</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101105050468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一定要用这个：</span><br><span class="line">		spark-streaming-kafka-0-10   和0.8的差别 </span><br><span class="line">		主要在Receiver DStream</span><br></pre></td></tr></table></figure></div>

<p><strong>区别</strong><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html#spark-streaming-kafka-integration-guide-kafka-broker-version-082" target="_blank" rel="noopener">spark-streaming-kafka-0-8</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-streaming-kafka-0-8：</span><br><span class="line"> 第一种方式：Receiver-based Approach</span><br><span class="line"> 	1.uses a Receiver to receive the data</span><br><span class="line"> 		那么Receiver是跑在哪里的？executor里面的 </span><br><span class="line"> 	2.using the Kafka high-level consumer API.</span><br><span class="line"> 	3. received from Kafka through a Receiver is stored in Spark executors</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101115349607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线———————————————————————————————————————-</p>
<p>However, under <strong>default configuration</strong>, this approach can <strong>lose data</strong> under failures (see receiver reliability. <strong>To ensure zero-data loss</strong>, you have to additionally <strong>enable Write-Ahead Logs</strong> in Spark Streaming (introduced in Spark 1.2). This synchronously saves all the <strong>received Kafka data into write-ahead logs on a distributed file system</strong> (e.g HDFS), so that all the data can be recovered on failure. See Deploying section in the streaming programming guide for more details on Write-Ahead Logs.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WAL机制：先把日志记录下来 这里就是数据</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">注意：Kafka的整合只有一个工具类 叫KafkaUtils</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create an input stream that pulls messages from Kafka Brokers.</span><br><span class="line">   * @param ssc       StreamingContext object</span><br><span class="line">   * @param zkQuorum  Zookeeper quorum (hostname:port,hostname:port,..)</span><br><span class="line">   * @param groupId   The group id for this consumer</span><br><span class="line">   * @param topics    Map of (topic_name to numPartitions) to consume. Each partition is consumed</span><br><span class="line">   *                  in its own thread</span><br><span class="line">   * @param storageLevel  Storage level to use for storing the received objects</span><br><span class="line">   *                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)</span><br><span class="line">   * @return DStream of (Kafka message key, Kafka message value)</span><br><span class="line">   */</span><br><span class="line">  def createStream(</span><br><span class="line">      ssc: StreamingContext,</span><br><span class="line">      zkQuorum: String,</span><br><span class="line">      groupId: String,</span><br><span class="line">      topics: Map[String, Int],</span><br><span class="line">      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2</span><br><span class="line">    ): ReceiverInputDStream[(String, String)] = &#123;</span><br><span class="line">    val kafkaParams = Map[String, String](</span><br><span class="line">      &quot;zookeeper.connect&quot; -&gt; zkQuorum, &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;zookeeper.connection.timeout.ms&quot; -&gt; &quot;10000&quot;)</span><br><span class="line">    createStream[String, String, StringDecoder, StringDecoder](</span><br><span class="line">      ssc, kafkaParams, topics, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">Receiver这个方式：</span><br><span class="line">注意：</span><br><span class="line">    storageLevel  ==MEMORY_AND_DISK_SER_2</span><br><span class="line">    1）数据丢失</span><br><span class="line">    2）WAL ==&gt; 数据延迟</span><br><span class="line">    3）offset我们不care</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为什么MEMORY_AND_DISK_SER_2设计为 2 ？</span><br><span class="line">	2的原因就是防止数据丢失 但是 问题是 即使是2 也不能保证数据 不丢 </span><br><span class="line"></span><br><span class="line">如果数据是通过WAL机制 写到HDFS上去  那么这个storageLevel 还有必要是 2 么？</span><br><span class="line">	一定是没有必要的    官网有写  你想想哈 如果是2  再加上hdfs本身的副本数  数据量是不是太大了 </span><br><span class="line"></span><br><span class="line">虽然WAL解决数据丢失问题 但是带来了一个问题？</span><br><span class="line">     就是数据写到HDFS上之后 更新zk 里的offset  </span><br><span class="line">     那么整体的时效性一定是下降的    你的实时跟HDFS挂钩了  实时性降低 数据延迟</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019110111454712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Points to remember:</p>
<p><strong>Topic partitions in Kafka do not correlate to partitions of RDDs generated in Spark Streaming</strong>. So increasing the number of topic-specific partitions in the KafkaUtils.createStream() only increases the number of threads using which topics that <strong>are consumed within a single receiver</strong>. <strong>It does not increase the parallelism of Spark in processing the data</strong>. Refer to the main document for more information on that.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Topic是有partition的 </span><br><span class="line">假设 一个Topic 对应3个partition</span><br><span class="line">   Kafka 里的partition 和 SS产生的RDD里面的partition 不是一个概念 </span><br><span class="line">   即：</span><br><span class="line">   	1 Topic ==》 3 parititions    RDD的并行度 并不是3  </span><br><span class="line">2.所以你增加Topic的分区 仅仅增加 使用的线程数 去处理topic的 还是a single receiver</span><br><span class="line"> 根本不会增加spark处理数据的并行度</span><br></pre></td></tr></table></figure></div>

<p>Multiple Kafka input DStreams can be created with different groups and topics for parallel receiving of data using multiple receivers.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">	也就是 spark-streaming-kafka-0-8 问题很多 别用了  要了解原理</span><br></pre></td></tr></table></figure></div>

<p><strong>spark-streaming-kafka-0-10  重点</strong><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">    spark-streaming-kafka-0-10</a></p>
<p>The Spark Streaming integration for Kafka 0.10 is similar in design to the 0.8 Direct Stream approach. It provides simple parallelism, <strong>1:1 correspondence between Kafka partitions and Spark partitions</strong>, and <strong>access to offsets and metadata</strong>. However, <strong>because the newer integration uses the new Kafka consumer API instead of the simple API, there are notable differences in usage.</strong> This version of the integration is marked as experimental, so the API is potentially subject to change.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Offset管理的时候不同</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#creating-a-direct-stream" target="_blank" rel="noopener">Creating a Direct Stream</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Direct</span><br><span class="line">    不需要Receiver</span><br><span class="line">    Topic的partition和RDD的partition是1:1</span><br><span class="line">    自己手工维护offset   (那么默认offset存在哪？知道么）</span><br></pre></td></tr></table></figure></div>
<p><strong>案例</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line">Producer:  console  （测试的时候 ） 就是使用KafkaApi代码实现</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * A Kafka client that publishes records to the Kafka cluster.</span><br><span class="line"> * &lt;P&gt;</span><br><span class="line"> * The producer is &lt;i&gt;thread safe&lt;/i&gt; and sharing a single producer instance across threads will generally be faster than</span><br><span class="line"> * having multiple instances.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * Here is a simple example of using the producer to send records with strings containing sequential numbers as the key/value</span><br><span class="line"> * pairs.</span><br><span class="line"> * &lt;pre&gt;</span><br><span class="line"> * &#123;@code</span><br><span class="line"> * Properties props = new Properties();</span><br><span class="line"> * props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line"> * props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line"> * props.put(&quot;retries&quot;, 0);</span><br><span class="line"> * props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line"> * props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line"> * props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line"> * props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"> * props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"> *</span><br><span class="line"> * Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"> * for (int i = 0; i &lt; 100; i++)</span><br><span class="line"> *     producer.send(new ProducerRecord&lt;String, String&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"> *</span><br><span class="line"> * producer.close();</span><br><span class="line"> * &#125;&lt;/pre&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The producer consists of a pool of buffer space that holds records that haven&apos;t yet been transmitted to the server</span><br><span class="line"> * as well as a background I/O thread that is responsible for turning these records into requests and transmitting them</span><br><span class="line"> * to the cluster. Failure to close the producer after use will leak these resources.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &#123;@link #send(ProducerRecord) send()&#125; method is asynchronous. When called it adds the record to a buffer of pending record sends</span><br><span class="line"> * and immediately returns. This allows the producer to batch together individual records for efficiency.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &lt;code&gt;acks&lt;/code&gt; config controls the criteria under which requests are considered complete. The &quot;all&quot; setting</span><br><span class="line"> * we have specified will result in blocking on the full commit of the record, the slowest but most durable setting.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * If the request fails, the producer can automatically retry, though since we have specified &lt;code&gt;retries&lt;/code&gt;</span><br><span class="line"> * as 0 it won&apos;t. Enabling retries also opens up the possibility of duplicates (see the documentation on</span><br><span class="line"> * &lt;a href=&quot;http://kafka.apache.org/documentation.html#semantics&quot;&gt;message delivery semantics&lt;/a&gt; for details).</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The producer maintains buffers of unsent records for each partition. These buffers are of a size specified by</span><br><span class="line"> * the &lt;code&gt;batch.size&lt;/code&gt; config. Making this larger can result in more batching, but requires more memory (since we will</span><br><span class="line"> * generally have one of these buffers for each active partition).</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * By default a buffer is available to send immediately even if there is additional unused space in the buffer. However if you</span><br><span class="line"> * want to reduce the number of requests you can set &lt;code&gt;linger.ms&lt;/code&gt; to something greater than 0. This will</span><br><span class="line"> * instruct the producer to wait up to that number of milliseconds before sending a request in hope that more records will</span><br><span class="line"> * arrive to fill up the same batch. This is analogous to Nagle&apos;s algorithm in TCP. For example, in the code snippet above,</span><br><span class="line"> * likely all 100 records would be sent in a single request since we set our linger time to 1 millisecond. However this setting</span><br><span class="line"> * would add 1 millisecond of latency to our request waiting for more records to arrive if we didn&apos;t fill up the buffer. Note that</span><br><span class="line"> * records that arrive close together in time will generally batch together even with &lt;code&gt;linger.ms=0&lt;/code&gt; so under heavy load</span><br><span class="line"> * batching will occur regardless of the linger configuration; however setting this to something larger than 0 can lead to fewer, more</span><br><span class="line"> * efficient requests when not under maximal load at the cost of a small amount of latency.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &lt;code&gt;buffer.memory&lt;/code&gt; controls the total amount of memory available to the producer for buffering. If records</span><br><span class="line"> * are sent faster than they can be transmitted to the server then this buffer space will be exhausted. When the buffer space is</span><br><span class="line"> * exhausted additional send calls will block. The threshold for time to block is determined by &lt;code&gt;max.block.ms&lt;/code&gt; after which it throws</span><br><span class="line"> * a TimeoutException.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &lt;code&gt;key.serializer&lt;/code&gt; and &lt;code&gt;value.serializer&lt;/code&gt; instruct how to turn the key and value objects the user provides with</span><br><span class="line"> * their &lt;code&gt;ProducerRecord&lt;/code&gt; into bytes. You can use the included &#123;@link org.apache.kafka.common.serialization.ByteArraySerializer&#125; or</span><br><span class="line"> * &#123;@link org.apache.kafka.common.serialization.StringSerializer&#125; for simple string or byte types.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * From Kafka 0.11, the KafkaProducer supports two additional modes: the idempotent producer and the transactional producer.</span><br><span class="line"> * The idempotent producer strengthens Kafka&apos;s delivery semantics from at least once to exactly once delivery. In particular</span><br><span class="line"> * producer retries will no longer introduce duplicates. The transactional producer allows an application to send messages</span><br><span class="line"> * to multiple partitions (and topics!) atomically.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * To enable idempotence, the &lt;code&gt;enable.idempotence&lt;/code&gt; configuration must be set to true. If set, the</span><br><span class="line"> * &lt;code&gt;retries&lt;/code&gt; config will default to &lt;code&gt;Integer.MAX_VALUE&lt;/code&gt; and the &lt;code&gt;acks&lt;/code&gt; config will</span><br><span class="line"> * default to &lt;code&gt;all&lt;/code&gt;. There are no API changes for the idempotent producer, so existing applications will</span><br><span class="line"> * not need to be modified to take advantage of this feature.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * To take advantage of the idempotent producer, it is imperative to avoid application level re-sends since these cannot</span><br><span class="line"> * be de-duplicated. As such, if an application enables idempotence, it is recommended to leave the &lt;code&gt;retries&lt;/code&gt;</span><br><span class="line"> * config unset, as it will be defaulted to &lt;code&gt;Integer.MAX_VALUE&lt;/code&gt;. Additionally, if a &#123;@link #send(ProducerRecord)&#125;</span><br><span class="line"> * returns an error even with infinite retries (for instance if the message expires in the buffer before being sent),</span><br><span class="line"> * then it is recommended to shut down the producer and check the contents of the last produced message to ensure that</span><br><span class="line"> * it is not duplicated. Finally, the producer can only guarantee idempotence for messages sent within a single session.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;To use the transactional producer and the attendant APIs, you must set the &lt;code&gt;transactional.id&lt;/code&gt;</span><br><span class="line"> * configuration property. If the &lt;code&gt;transactional.id&lt;/code&gt; is set, idempotence is automatically enabled along with</span><br><span class="line"> * the producer configs which idempotence depends on. Further, topics which are included in transactions should be configured</span><br><span class="line"> * for durability. In particular, the &lt;code&gt;replication.factor&lt;/code&gt; should be at least &lt;code&gt;3&lt;/code&gt;, and the</span><br><span class="line"> * &lt;code&gt;min.insync.replicas&lt;/code&gt; for these topics should be set to 2. Finally, in order for transactional guarantees</span><br><span class="line"> * to be realized from end-to-end, the consumers must be configured to read only committed messages as well.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The purpose of the &lt;code&gt;transactional.id&lt;/code&gt; is to enable transaction recovery across multiple sessions of a</span><br><span class="line"> * single producer instance. It would typically be derived from the shard identifier in a partitioned, stateful, application.</span><br><span class="line"> * As such, it should be unique to each producer instance running within a partitioned application.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;All the new transactional APIs are blocking and will throw exceptions on failure. The example</span><br><span class="line"> * below illustrates how the new APIs are meant to be used. It is similar to the example above, except that all</span><br><span class="line"> * 100 messages are part of a single transaction.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * &lt;pre&gt;</span><br><span class="line"> * &#123;@code</span><br><span class="line"> * Properties props = new Properties();</span><br><span class="line"> * props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line"> * props.put(&quot;transactional.id&quot;, &quot;my-transactional-id&quot;);</span><br><span class="line"> * Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props, new StringSerializer(), new StringSerializer());</span><br><span class="line"> *</span><br><span class="line"> * producer.initTransactions();</span><br><span class="line"> *</span><br><span class="line"> * try &#123;</span><br><span class="line"> *     producer.beginTransaction();</span><br><span class="line"> *     for (int i = 0; i &lt; 100; i++)</span><br><span class="line"> *         producer.send(new ProducerRecord&lt;&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"> *     producer.commitTransaction();</span><br><span class="line"> * &#125; catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123;</span><br><span class="line"> *     // We can&apos;t recover from these exceptions, so our only option is to close the producer and exit.</span><br><span class="line"> *     producer.close();</span><br><span class="line"> * &#125; catch (KafkaException e) &#123;</span><br><span class="line"> *     // For all other exceptions, just abort the transaction and try again.</span><br><span class="line"> *     producer.abortTransaction();</span><br><span class="line"> * &#125;</span><br><span class="line"> * producer.close();</span><br><span class="line"> * &#125; &lt;/pre&gt;</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * As is hinted at in the example, there can be only one open transaction per producer. All messages sent between the</span><br><span class="line"> * &#123;@link #beginTransaction()&#125; and &#123;@link #commitTransaction()&#125; calls will be part of a single transaction. When the</span><br><span class="line"> * &lt;code&gt;transactional.id&lt;/code&gt; is specified, all messages sent by the producer must be part of a transaction.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The transactional producer uses exceptions to communicate error states. In particular, it is not required</span><br><span class="line"> * to specify callbacks for &lt;code&gt;producer.send()&lt;/code&gt; or to call &lt;code&gt;.get()&lt;/code&gt; on the returned Future: a</span><br><span class="line"> * &lt;code&gt;KafkaException&lt;/code&gt; would be thrown if any of the</span><br><span class="line"> * &lt;code&gt;producer.send()&lt;/code&gt; or transactional calls hit an irrecoverable error during a transaction. See the &#123;@link #send(ProducerRecord)&#125;</span><br><span class="line"> * documentation for more details about detecting errors from a transactional send.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;/p&gt;By calling</span><br><span class="line"> * &lt;code&gt;producer.abortTransaction()&lt;/code&gt; upon receiving a &lt;code&gt;KafkaException&lt;/code&gt; we can ensure that any</span><br><span class="line"> * successful writes are marked as aborted, hence keeping the transactional guarantees.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * This client can communicate with brokers that are version 0.10.0 or newer. Older or newer brokers may not support</span><br><span class="line"> * certain client features.  For instance, the transactional APIs need broker versions 0.11.0 or later. You will receive an</span><br><span class="line"> * &lt;code&gt;UnsupportedVersionException&lt;/code&gt; when invoking an API that is not available in the running broker version.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> */</span><br><span class="line">public class KafkaProducer&lt;K, V&gt; implements Producer&lt;K, V&gt; &#123;&#125;</span><br><span class="line"></span><br><span class="line">KafkaProducer源码里都介绍了怎么使用  之后讲解略过</span><br><span class="line"></span><br><span class="line">Consumer:console  （测试的时候 ）</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">KafkaProducer：</span><br><span class="line"></span><br><span class="line">先测试 ：发送abcdef </span><br><span class="line">object DataGenerator &#123;</span><br><span class="line">  private val logger: Logger = LoggerFactory.getLogger(DataGenerator.getClass)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val props = new Properties()</span><br><span class="line">    props.put(&quot;bootstrap.servers&quot;, &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;)</span><br><span class="line">    props.put(&quot;acks&quot;, &quot;all&quot;)</span><br><span class="line">    props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">    props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line"></span><br><span class="line">    val producer = new KafkaProducer[String, String](props)</span><br><span class="line"></span><br><span class="line">    for (i &lt;- 1 to 10) &#123;</span><br><span class="line">      Thread.sleep(100)</span><br><span class="line">      //拿一个abcdef</span><br><span class="line">      val word: String = String.valueOf((new Random().nextInt(6) + &apos;a&apos;).toChar)</span><br><span class="line">      val part = i % 3   //发到哪个分区 因为是三个分区</span><br><span class="line"></span><br><span class="line">      logger.error(&quot;word : &#123;&#125;&quot;,word)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果： 注意这代码 全是源码注释里有 你不需要记住 </span><br><span class="line">你只需要记住：</span><br><span class="line">	1.KafkaProducer 创建   </span><br><span class="line">	2.发送数据 要序列化</span><br><span class="line">	3.怎么发   </span><br><span class="line">	这三个东西 源码注释里全都是写好的 </span><br><span class="line"></span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : d</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : d</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : e</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : a</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : a</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">怎么发送呢？</span><br><span class="line"> /**</span><br><span class="line">     * Creates a record to be sent to a specified topic and partition</span><br><span class="line">     *</span><br><span class="line">     * @param topic The topic the record will be appended to</span><br><span class="line">     * @param partition The partition to which the record should be sent</span><br><span class="line">     * @param key The key that will be included in the record</span><br><span class="line">     * @param value The record contents</span><br><span class="line">     */</span><br><span class="line">    public ProducerRecord(String topic, Integer partition, K key, V value) &#123;</span><br><span class="line">        this(topic, partition, null, key, value, null);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object DataGenerator &#123;</span><br><span class="line"></span><br><span class="line">  private val logger: Logger = LoggerFactory.getLogger(DataGenerator.getClass)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val props = new Properties()</span><br><span class="line">    props.put(&quot;bootstrap.servers&quot;, &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;)</span><br><span class="line">    props.put(&quot;acks&quot;, &quot;all&quot;)</span><br><span class="line">    props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">    props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">    val producer = new KafkaProducer[String, String](props)</span><br><span class="line"></span><br><span class="line">    for (i &lt;- 1 to 10) &#123;</span><br><span class="line">      Thread.sleep(100)</span><br><span class="line"></span><br><span class="line">      //拿一个abcdef</span><br><span class="line">      val word: String = String.valueOf((new Random().nextInt(6) + &apos;a&apos;).toChar)</span><br><span class="line">      val part = i % 3 //发到哪个分区 因为是三个分区</span><br><span class="line"></span><br><span class="line">      logger.error(&quot;word : &#123;&#125;&quot;, word)</span><br><span class="line"></span><br><span class="line">      val record = producer.send(new ProducerRecord[String, String](&quot;double_happy_offset&quot;, part, &quot;&quot;,word))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    producer.close()</span><br><span class="line">    println(&quot;double_happy 数据产生完毕..........&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ok进行测试</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line">&gt; --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic double_happy_offset \</span><br><span class="line">&gt; --from-beginning</span><br><span class="line"></span><br><span class="line">运行idea代码结果：</span><br><span class="line">19/11/01 13:33:56 ERROR DataGenerator$: word : a</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : e</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : a</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : e</span><br><span class="line">19/11/01 13:33:58 ERROR DataGenerator$: word : a</span><br><span class="line">double_happy 数据产生完毕..........</span><br><span class="line"></span><br><span class="line">kafka控制台结果：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line">&gt; --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic double_happy_offset \</span><br><span class="line">&gt; --from-beginning</span><br><span class="line">a</span><br><span class="line">e</span><br><span class="line">f</span><br><span class="line">a</span><br><span class="line">f</span><br><span class="line">c</span><br><span class="line">c</span><br><span class="line">f</span><br><span class="line">e</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">对接kafka + ss前期准备工作ok</span><br></pre></td></tr></table></figure></div>
<p><strong>对接</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Scala constructor for a DStream where</span><br><span class="line">   * each given Kafka topic/partition corresponds to an RDD partition.</span><br><span class="line">   * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number</span><br><span class="line">   *  of messages</span><br><span class="line">   * per second that each &apos;&apos;&apos;partition&apos;&apos;&apos; will accept.</span><br><span class="line">   * @param locationStrategy In most cases, pass in [[LocationStrategies.PreferConsistent]],</span><br><span class="line">   *   see [[LocationStrategies]] for more details.</span><br><span class="line">   * @param consumerStrategy In most cases, pass in [[ConsumerStrategies.Subscribe]],</span><br><span class="line">   *   see [[ConsumerStrategies]] for more details</span><br><span class="line">   * @tparam K type of Kafka message key</span><br><span class="line">   * @tparam V type of Kafka message value</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  def createDirectStream[K, V](</span><br><span class="line">      ssc: StreamingContext,</span><br><span class="line">      locationStrategy: LocationStrategy,</span><br><span class="line">      consumerStrategy: ConsumerStrategy[K, V]</span><br><span class="line">    ): InputDStream[ConsumerRecord[K, V]] = &#123;</span><br><span class="line">    val ppc = new DefaultPerPartitionConfig(ssc.sparkContext.getConf)</span><br><span class="line">    createDirectStream[K, V](ssc, locationStrategy, consumerStrategy, ppc)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">locationStrategy 策略什么意思呢？官网有  一会代码里我写了解释</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    stream.map(record =&gt; (record.key, record.value))  </span><br><span class="line">      .map(_._2).print()   //因为我们key就没有设置  只取value</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587100000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">a</span><br><span class="line">a</span><br><span class="line">c</span><br><span class="line">a</span><br><span class="line">f</span><br><span class="line">c</span><br><span class="line">e</span><br><span class="line">e</span><br><span class="line">f</span><br><span class="line">f</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587110000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587120000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587130000 ms</span><br><span class="line">-------------------------------------------     这块 我们kafka又发了一次数据 ssc接收到了</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587140000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">c</span><br><span class="line">a</span><br><span class="line">e</span><br><span class="line">c</span><br><span class="line">d</span><br><span class="line">d</span><br><span class="line">a</span><br><span class="line">b</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">   //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    //结果入库 写到redis里</span><br><span class="line">    result.foreachRDD(rdd =&gt;&#123;</span><br><span class="line">      rdd.foreachPartition(paritition =&gt;&#123;</span><br><span class="line">        val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line"></span><br><span class="line">        paritition.foreach(pair =&gt;&#123;</span><br><span class="line">          jedis.hincrBy(&quot;kafka_ss_redis_wc&quot;,pair._1,pair._2)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        jedis.close()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;kafka_ss_redis_wc&quot;</span><br><span class="line">3) &quot;doublehappy_redis_wc&quot;</span><br><span class="line">hadoop101:6379&gt; HGETALL kafka_ss_redis_wc</span><br><span class="line"> 1) &quot;e&quot;</span><br><span class="line"> 2) &quot;3&quot;</span><br><span class="line"> 3) &quot;d&quot;</span><br><span class="line"> 4) &quot;2&quot;</span><br><span class="line"> 5) &quot;a&quot;</span><br><span class="line"> 6) &quot;6&quot;</span><br><span class="line"> 7) &quot;b&quot;</span><br><span class="line"> 8) &quot;2&quot;</span><br><span class="line"> 9) &quot;c&quot;</span><br><span class="line">10) &quot;4&quot;</span><br><span class="line">11) &quot;f&quot;</span><br><span class="line">12) &quot;3&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019110114001642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个时候 把实时代码关掉 ：</span><br><span class="line"></span><br><span class="line">redis里的数据翻倍了 因为又写了一次嘛  </span><br><span class="line">但是这样是不行的  </span><br><span class="line">	因为代码里 这个控制的 </span><br><span class="line">		&quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">  </span><br><span class="line">那么接下来 看看offset 怎么获取呢？怎么提交offset呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101140557306.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#obtaining-offsets" target="_blank" rel="noopener">Obtaining Offsets</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">trait HasOffsetRanges &#123;</span><br><span class="line">  def offsetRanges: Array[OffsetRange]    //拿到offset的范围  返回值是数组 </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Represents a range of offsets from a single Kafka TopicPartition. Instances of this class</span><br><span class="line"> * can be created with `OffsetRange.create()`.</span><br><span class="line"> * @param topic Kafka topic name</span><br><span class="line"> * @param partition Kafka partition id</span><br><span class="line"> * @param fromOffset Inclusive starting offset</span><br><span class="line"> * @param untilOffset Exclusive ending offset</span><br><span class="line"> */</span><br><span class="line">final class OffsetRange private(</span><br><span class="line">    val topic: String,    </span><br><span class="line">    val partition: Int,</span><br><span class="line">    val fromOffset: Long,</span><br><span class="line">    val untilOffset: Long) extends Serializable &#123;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">获取offset：</span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">   //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    result.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  </span><br><span class="line"></span><br><span class="line">      //获取分区数</span><br><span class="line">      println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">      //获取当前批次的offset数据</span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">        println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding enable.auto.commit to false for executor</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding auto.offset.reset to none for executor</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135</span><br><span class="line">19/11/01 14:18:20 ERROR JobScheduler: Error running job streaming job 1572589100000 ms.0</span><br><span class="line">---------2</span><br><span class="line">java.lang.ClassCastException: org.apache.spark.rdd.ShuffledRDD cannot be cast to org.apache.spark.streaming.kafka010.HasOffsetRanges</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:48)</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:42)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.ClassCastException: org.apache.spark.rdd.ShuffledRDD cannot be cast to org.apache.spark.streaming.kafka010.HasOffsetRanges</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:48)</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:42)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101142414924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">为什么报错呢？  *****  而且分区数还是2  说明不对</span><br><span class="line">java.lang.ClassCastException: org.apache.spark.rdd.ShuffledRDD cannot be cast to org.apache.spark.streaming.kafka010.HasOffsetRanges</span><br><span class="line"></span><br><span class="line">ShuffledRDD 不能转成 HasOffsetRanges 上面图片解释了 </span><br><span class="line"></span><br><span class="line">为什么是ShuffledRDD 呢？ </span><br><span class="line">	因为reduceBykey之后的流里面 的rdd   就是ShuffledRDD 类型的   主要是经过了reduceBykey 明白吗</span><br><span class="line"></span><br><span class="line">所以上面的代码 不对 </span><br><span class="line">业务逻辑的代码是要放在里面写  而不是在前面做   ***</span><br><span class="line">所以直接用stream + foreachRDD   (业务逻辑在foreachRDD 里面写)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101143014894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">获取offset：</span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  </span><br><span class="line"></span><br><span class="line">      //获取分区数</span><br><span class="line">      println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">      //获取当前批次的offset数据</span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">        println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：他是一直在跑的哈</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 0 8</span><br><span class="line">double_happy_offset 2 0 6</span><br><span class="line">double_happy_offset 0 0 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line"></span><br><span class="line">看sparkUI   解释 结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101143701991.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 0 8</span><br><span class="line">double_happy_offset 2 0 6</span><br><span class="line">double_happy_offset 0 0 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8         从8 开始   因为没有数据进来了 </span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">20哪里来的？  8 +6+6 = 20</span><br><span class="line"></span><br><span class="line">由于数据过来 第一个批次全部处理完了  </span><br><span class="line">所以第二个批次 结果 是从8开始  </span><br><span class="line"></span><br><span class="line">那么我再生产10条数据 看结果</span><br><span class="line"></span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 6 9          也就是说 这个拿了3条数据</span><br><span class="line">double_happy_offset 1 8 12        拿了4条数据</span><br><span class="line">double_happy_offset 0 6 9          拿了3条数据</span><br><span class="line">---------3</span><br><span class="line"></span><br><span class="line">那么 我把程序关掉 结果一定是这样的：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 0 9</span><br><span class="line">double_happy_offset 2 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line"></span><br><span class="line">你获得到了偏移量  由于你没有提交 没有保存偏移量 </span><br><span class="line">所以重启之后都是从头开始跑的  就意味着 第一批数据 会很多 </span><br><span class="line"></span><br><span class="line">接下来就是提交offset</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets" target="_blank" rel="noopener">Storing Offsets</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">很多方式 ：</span><br><span class="line">1.Checkpoints   别用了 不好用 </span><br><span class="line">2.Kafka itself    </span><br><span class="line">	Kafka has an offset commit API that stores offsets in a special Kafka topic.</span><br><span class="line">3.Your own data store</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">2.Kafka itself    ：</span><br><span class="line"></span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  因为</span><br><span class="line"></span><br><span class="line">      //获取分区数</span><br><span class="line">      println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">      //获取当前批次的offset数据</span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">        println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      //kafka自身的方式  提交 更新的offset</span><br><span class="line">      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 0 9</span><br><span class="line">double_happy_offset 1 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 9 9</span><br><span class="line">double_happy_offset 1 12 12</span><br><span class="line">double_happy_offset 0 9 9</span><br><span class="line">---------3</span><br><span class="line"></span><br><span class="line">关闭重启 之后结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 9 9</span><br><span class="line">double_happy_offset 1 12 12</span><br><span class="line">double_happy_offset 0 9 9</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 9 9</span><br><span class="line">double_happy_offset 1 12 12</span><br><span class="line">double_happy_offset 0 9 9</span><br><span class="line"></span><br><span class="line">说明offset 提交ok了 </span><br><span class="line"></span><br><span class="line">而且两次操作sparkUi也证实了</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101145733172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="第一次"><br>第二次 重启之后<br><img src="https://img-blog.csdnimg.cn/2019110114582759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">数据是0 对吧  因为数据已经提交过了 </span><br><span class="line">通过 kafka 命令可以查到offset </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$ ./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoop101:9092,hadoop101:9093,hadoop101:9094 --topic double_happy_offset</span><br><span class="line">double_happy_offset:0:9</span><br><span class="line">double_happy_offset:1:12</span><br><span class="line">double_happy_offset:2:9</span><br><span class="line">[double_happy@hadoop101 bin]$ </span><br><span class="line"></span><br><span class="line">那么kafka自身维护的offset存在哪里呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101150455299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>However, you can <strong>commit offsets to Kafka after you know your output has been stored,</strong> using the commitAsync API. The benefit as compared to checkpoints is that Kafka is a durable store regardless of changes to your application code. However, <strong>Kafka is not transactional, so your outputs must still be idempotent.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1.你的业务逻辑完成之后再提交offset</span><br><span class="line">2.kafka并不是事务性的 所以你的输出 必须保证幂等性</span><br><span class="line"></span><br><span class="line">假设 </span><br><span class="line">double_happy_offset:0:9</span><br><span class="line">double_happy_offset:1:12</span><br><span class="line">double_happy_offset:2:9</span><br><span class="line"></span><br><span class="line">你第一次处理完了 结果也写到redis里了  </span><br><span class="line">因为种种原因我们可以手工指定 kafka的偏移量的  </span><br><span class="line"></span><br><span class="line">假设 第一个批次 5 接下来是 5-9   这里 5-9 已经消费了对吧 </span><br><span class="line"></span><br><span class="line">那么 我们手工指定 5-9 这个批次再消费一次 也就是说 </span><br><span class="line">redis 里面的结果又是错的  因为结果又重复了呀  这就是上面刚开始的 演示  </span><br><span class="line"></span><br><span class="line">这就是说你输出的代码 也要有幂等性  不管你输出跑多少次  即使重复消费 也要保证结果是具有幂等性的</span><br></pre></td></tr></table></figure></div>

<p><strong>3.Your own data store</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">这里我使用redis  你选择MySQL也是可以的    为了测试换一个groupid  让他重新消费</span><br><span class="line">注意：这种方式 提交offset</span><br><span class="line">手动提交offset的时候  要与groupid 对应 </span><br><span class="line"></span><br><span class="line">key： Topic + groupid </span><br><span class="line"></span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  因为</span><br><span class="line"></span><br><span class="line">      if(!rdd.isEmpty())&#123;</span><br><span class="line"></span><br><span class="line">        //获取分区数</span><br><span class="line">        println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">        //获取当前批次的offset数据</span><br><span class="line">        val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //TODO ... 处理业务逻辑 wc</span><br><span class="line"></span><br><span class="line">        //ToDO ... 提交Offset到Redis  使用第三种方式</span><br><span class="line">        val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line"></span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          val topicGroupId = x.topic + &quot;_&quot;+ groupId    //key = topic + groupId    </span><br><span class="line">          jedis.hset(topicGroupId,x.partition+&quot;&quot;,x.untilOffset+&quot;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line">        jedis.close()</span><br><span class="line"></span><br><span class="line">      &#125;else&#123;</span><br><span class="line">        println(&quot;当前批次没有数据.....&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 0 9</span><br><span class="line">double_happy_offset 1 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;kafka_ss_redis_wc&quot;</span><br><span class="line">3) &quot;doublehappy_redis_wc&quot;</span><br><span class="line">4) &quot;double_happy_offset_double_happy_group&quot;</span><br><span class="line">hadoop101:6379&gt; HGETALL double_happy_offset_double_happy_group</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) &quot;9&quot;</span><br><span class="line">3) &quot;1&quot;</span><br><span class="line">4) &quot;12&quot;</span><br><span class="line">5) &quot;0&quot;</span><br><span class="line">6) &quot;9&quot;</span><br><span class="line">hadoop101:6379&gt; </span><br><span class="line">结果是没有问题 的 </span><br><span class="line">key = topic + groupId      </span><br><span class="line"> jedis.hset(topicGroupId,x.partition+&quot;&quot;,x.untilOffset+&quot;&quot;)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101162513463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">再生产一批数据 查看结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 0 9</span><br><span class="line">double_happy_offset 1 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 12 16</span><br><span class="line">double_happy_offset 2 9 12</span><br><span class="line">double_happy_offset 0 9 12</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; HGETALL double_happy_offset_double_happy_group</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) &quot;9&quot;</span><br><span class="line">3) &quot;1&quot;</span><br><span class="line">4) &quot;12&quot;</span><br><span class="line">5) &quot;0&quot;</span><br><span class="line">6) &quot;9&quot;</span><br><span class="line">hadoop101:6379&gt; HGETALL double_happy_offset_double_happy_group</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) &quot;12&quot;</span><br><span class="line">3) &quot;1&quot;</span><br><span class="line">4) &quot;16&quot;</span><br><span class="line">5) &quot;0&quot;</span><br><span class="line">6) &quot;12&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101162924889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">我把程序关掉 先生成两个批次的数据  再把程序打开  查看结果</span><br><span class="line"></span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 0 0 18</span><br><span class="line">double_happy_offset 2 0 18</span><br><span class="line">double_happy_offset 1 0 24</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">为什么从0开始消费？？</span><br><span class="line">因为 </span><br><span class="line">&quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;   控制的 </span><br><span class="line">应该是 你offset保存在哪里  下次启动的时候去哪里取</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">测试：取出 offset 从redis</span><br><span class="line"></span><br><span class="line">object RedisOffsetApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line"></span><br><span class="line">    //TODO...从保存offset的地方 eg：redis 去获取已经提交的offset的记录信息</span><br><span class="line"></span><br><span class="line">    val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line">    val offsets: util.Map[String, String] = jedis.hgetAll(topics(0)+&quot;_&quot;+groupId)</span><br><span class="line"></span><br><span class="line">    import  scala.collection.JavaConversions._   //offsets 想使用  scala 里集合的map方法 要进行隐式转换 java-&gt; scala</span><br><span class="line"></span><br><span class="line">    offsets.map(x=&gt; &#123;</span><br><span class="line"></span><br><span class="line">      //  offsets map后要获取一种什么样的数据结构呢？ </span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">//  offsets map后要获取一种什么样的数据结构呢？ 之前KafkaUtils.createDirectStream 里面的消费策略</span><br><span class="line">Subscribe 传入 topics 和 kafkaParams  查看源码发现 </span><br><span class="line"></span><br><span class="line">  def Subscribe[K, V](</span><br><span class="line">      topics: Iterable[jl.String],</span><br><span class="line">      kafkaParams: collection.Map[String, Object],</span><br><span class="line">      offsets: collection.Map[TopicPartition, Long]): ConsumerStrategy[K, V] = &#123;</span><br><span class="line">    new Subscribe[K, V](</span><br><span class="line">      new ju.ArrayList(topics.asJavaCollection),</span><br><span class="line">      new ju.HashMap[String, Object](kafkaParams.asJava),</span><br><span class="line">      new ju.HashMap[TopicPartition, jl.Long](offsets.mapValues(l =&gt; new jl.Long(l)).asJava))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> offsets: collection.Map[TopicPartition, Long])    即  TopicPartition 和 偏移量  组成的这样的格式  Map[TopicPartition, Long]</span><br><span class="line"></span><br><span class="line">    public TopicPartition(String topic, int partition) &#123;</span><br><span class="line">        this.partition = partition;</span><br><span class="line">        this.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">所以取出 offset 从redis  取出的数据结构设计要符合 </span><br><span class="line">createDirectStream  里的 Subscribe 参数的数据结构</span><br><span class="line"></span><br><span class="line">(因为 取出 offset 从redis  是在  createDirectStream之前执行的 )</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">object RedisOffsetApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line">    val topics = Array(topic)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //TODO...从保存offset的地方 eg：redis 去获取已经提交的offset的记录信息</span><br><span class="line"></span><br><span class="line">    val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line">    val offsets: util.Map[String, String] = jedis.hgetAll(topics(0) + &quot;_&quot; + groupId)</span><br><span class="line"></span><br><span class="line">    var fromOffsets: Map[TopicPartition, Long] = Map[TopicPartition, Long]()</span><br><span class="line"></span><br><span class="line">    import scala.collection.JavaConversions._ //offsets 想使用  scala 里集合的map方法 要进行隐式转换 java-&gt; scala</span><br><span class="line">    offsets.map(x =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      //  offsets map后要获取一种什么样的数据结构呢？ offsets  Map[TopicPartition, Long]()</span><br><span class="line">      fromOffsets += new TopicPartition(topics(0), x._1.toInt) -&gt; x._2.toLong</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    fromOffsets.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">(double_happy_offset-0,18)</span><br><span class="line">(double_happy_offset-1,24)</span><br><span class="line">(double_happy_offset-2,18)</span><br><span class="line"></span><br><span class="line">取出 offset 从redis</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">所以我们再测试实时的代码  ：</span><br><span class="line">	我们先不提交 看看控制台 </span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    var fromOffsets: Map[TopicPartition, Long] = Map[TopicPartition, Long]()</span><br><span class="line"></span><br><span class="line">    //TODO...从保存offset的地方 eg：redis 去获取已经提交的offset的记录信息</span><br><span class="line">    val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line">    val offsets: util.Map[String, String] = jedis.hgetAll(topics(0)+&quot;_&quot;+groupId)</span><br><span class="line"></span><br><span class="line">     //offsets 想使用  scala 里集合的map方法 要进行隐式转换 java-&gt; scala</span><br><span class="line">    import scala.collection.JavaConversions._</span><br><span class="line">    offsets.map(x =&gt; &#123;</span><br><span class="line">      //offsets map后要获取一种什么样的数据结构呢？ offsets  Map[TopicPartition, Long]()</span><br><span class="line">      fromOffsets += new TopicPartition(topics(0), x._1.toInt) -&gt; x._2.toLong</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams,fromOffsets)  //从已有的offset里读取数据 开始消费</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  因为</span><br><span class="line"></span><br><span class="line">      if(!rdd.isEmpty())&#123;</span><br><span class="line"></span><br><span class="line">        //获取分区数</span><br><span class="line">        println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">        //获取当前批次的offset数据</span><br><span class="line">        val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        //TODO ... 处理业务逻辑 wc</span><br><span class="line"></span><br><span class="line">/*        //ToDO ... 提交Offset到Redis  使用第三种方式</span><br><span class="line">        val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line"></span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          val topicGroupId = x.topic + &quot;_&quot;+ groupId</span><br><span class="line">          jedis.hset(topicGroupId,x.partition+&quot;&quot;,x.untilOffset+&quot;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line">        jedis.close()*/</span><br><span class="line"></span><br><span class="line">      &#125;else&#123;</span><br><span class="line"></span><br><span class="line">        println(&quot;当前批次没有数据.....&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/11/01 17:14:51 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">说明ok  没有重新消费 </span><br><span class="line"></span><br><span class="line">那么我们再写一批数据 查看结果：</span><br><span class="line"></span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 0 18 21</span><br><span class="line">double_happy_offset 1 24 28</span><br><span class="line">double_happy_offset 2 18 21</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">说明程序ok</span><br><span class="line"></span><br><span class="line">那么我们把实时程序停掉 再产生两次数据 再重启实时程序  对比最初的那次测试：</span><br><span class="line"></span><br><span class="line">还记得么？最初那次 是从0 开始消费的  那么这次测试结果呢？</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 24 36</span><br><span class="line">double_happy_offset 0 18 27</span><br><span class="line">double_happy_offset 2 18 27</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">终于ok了 (这是打印在控制台)</span><br><span class="line"></span><br><span class="line">那么写入redis offset 测试也是ok的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">虽然上面的东西一大坨 实际上思路很清晰 很简单 主要是几行破代码 </span><br><span class="line"></span><br><span class="line">而且大部分演示的都是不能用的 但是目的是让你知道 这些坑 而不是直接拿代码直接用 (了解原理之后可以 要不然之后出错了你都不知道怎么维护)</span><br></pre></td></tr></table></figure></div>
<p><strong>总结</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.  &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;  </span><br><span class="line">  最终成品 这个参数设置选别的还是 这个 都无所谓的 </span><br><span class="line">  因为你的offsets 是走的 fromOffsets 你自己定义的那个 </span><br><span class="line">  	(就是把你们保存的offset 拿出来丢到 fromOffsets  这里(格式是重点)   创建流的时候 把fromOffsets 丢到消费策略那个参数里)</span><br><span class="line"></span><br><span class="line">2.业务处理前的第一件事是把偏移量拿到 (就是foreachRDD里面第一件事)</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/22/SS04/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            SS04
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/02/20/SS02/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">SS02</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Kudu-Impala%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B01-double-happy/">Kudu+Impala故障案例01--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink04-double-happy/">Flink04--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink03-double-happy/">Flink03--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink02-double-happy/">Flink02--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>