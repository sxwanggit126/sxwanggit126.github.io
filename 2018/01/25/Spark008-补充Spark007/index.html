<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark008--补充Spark007 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="上次的结果输出的文本是这样的，客户说我需要压缩的格式呢？生产上出来的数据非常非常大 肯定是需要压缩的eg：5分钟数据量达到10多个G 12345678910111213141516171819&#x2F;**   * Output the RDD to any Hadoop-supported file system, using a Hadoop &#96;OutputFormat&#96; class   * supp">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark008--补充Spark007">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;25&#x2F;Spark008-%E8%A1%A5%E5%85%85Spark007&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="上次的结果输出的文本是这样的，客户说我需要压缩的格式呢？生产上出来的数据非常非常大 肯定是需要压缩的eg：5分钟数据量达到10多个G 12345678910111213141516171819&#x2F;**   * Output the RDD to any Hadoop-supported file system, using a Hadoop &#96;OutputFormat&#96; class   * supp">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024155059801.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024155642185.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024192531620.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024200131809.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024203040986.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024203854192.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024204201773.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024204355319.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024204640805.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024224445859.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024230341733.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024231709105.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025111143777.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025111221278.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102423185850.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024232531503.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024233636507.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102510374444.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102510514440.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025114937388.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:04:22.912Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191024155059801.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-Spark008-补充Spark007" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      Spark008--补充Spark007
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/25/Spark008-%E8%A1%A5%E5%85%85Spark007/" class="article-date">
  <time datetime="2018-01-25T12:03:56.000Z" itemprop="datePublished">2018-01-25</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><img src="https://img-blog.csdnimg.cn/20191024155059801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上次的结果输出的文本是这样的，客户说我需要压缩的格式呢？<br>生产上出来的数据非常非常大 肯定是需要压缩的<br>eg：5分钟数据量达到10多个G</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class</span><br><span class="line">   * supporting the key and value types K and V in this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note We should make sure our tasks are idempotent when speculation is enabled, i.e. do</span><br><span class="line">   * not use output committer that writes data directly.</span><br><span class="line">   * There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad</span><br><span class="line">   * result of using direct output committer with speculation enabled.</span><br><span class="line">   */</span><br><span class="line">  def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      conf: JobConf = new JobConf(self.context.hadoopConfiguration),</span><br><span class="line">      codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">codec 就是指定压缩的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    /**</span><br><span class="line">      * Android</span><br><span class="line">      *   xxxx.log</span><br><span class="line">      *</span><br><span class="line">      * iOS</span><br><span class="line">      *   xxx.log</span><br><span class="line">      */</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],</span><br><span class="line">        classOf[RuozedataMultipleTextOutputFormat],</span><br><span class="line">        classOf[GzipCodec])      //这块加上压缩的格式</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024155642185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>学学底层的实现：</strong> 前面的文章好像有写 忘记了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">1.saveAsHadoopFile</span><br><span class="line"> def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      codec: Class[_ &lt;: CompressionCodec]): Unit = self.withScope &#123;</span><br><span class="line">    saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">      new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2.点进去</span><br><span class="line"> saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">      new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line"></span><br><span class="line">3.</span><br><span class="line">  def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      conf: JobConf = new JobConf(self.context.hadoopConfiguration),</span><br><span class="line">      codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit = self.withScope &#123;</span><br><span class="line">    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).</span><br><span class="line">    val hadoopConf = conf</span><br><span class="line">    hadoopConf.setOutputKeyClass(keyClass)</span><br><span class="line">    hadoopConf.setOutputValueClass(valueClass)</span><br><span class="line">    conf.setOutputFormat(outputFormatClass)</span><br><span class="line">    for (c &lt;- codec) &#123;</span><br><span class="line">      hadoopConf.setCompressMapOutput(true)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,</span><br><span class="line">        CompressionType.BLOCK.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Use configured output committer if already set</span><br><span class="line">    if (conf.getOutputCommitter == null) &#123;</span><br><span class="line">      hadoopConf.setOutputCommitter(classOf[FileOutputCommitter])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // When speculation is on and output committer class name contains &quot;Direct&quot;, we should warn</span><br><span class="line">    // users that they may loss data if they are using a direct output committer.</span><br><span class="line">    val speculationEnabled = self.conf.getBoolean(&quot;spark.speculation&quot;, false)</span><br><span class="line">    val outputCommitterClass = hadoopConf.get(&quot;mapred.output.committer.class&quot;, &quot;&quot;)</span><br><span class="line">    if (speculationEnabled &amp;&amp; outputCommitterClass.contains(&quot;Direct&quot;)) &#123;</span><br><span class="line">      val warningMessage =</span><br><span class="line">        s&quot;$outputCommitterClass may be an output committer that writes data directly to &quot; +</span><br><span class="line">          &quot;the final location. Because speculation is enabled, this output committer may &quot; +</span><br><span class="line">          &quot;cause data loss (see the case in SPARK-10063). If possible, please use an output &quot; +</span><br><span class="line">          &quot;committer that does not have this behavior (e.g. FileOutputCommitter).&quot;</span><br><span class="line">      logWarning(warningMessage)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FileOutputFormat.setOutputPath(hadoopConf,</span><br><span class="line">      SparkHadoopWriterUtils.createPathFromString(path, hadoopConf))</span><br><span class="line">    saveAsHadoopDataset(hadoopConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4. 3里面关键</span><br><span class="line">   for (c &lt;- codec) &#123;</span><br><span class="line">      hadoopConf.setCompressMapOutput(true)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,</span><br><span class="line">        CompressionType.BLOCK.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"> hadoopConf.setCompressMapOutput(true) 点进去看看</span><br><span class="line"></span><br><span class="line"> public void setCompressMapOutput(boolean compress) &#123;</span><br><span class="line">    setBoolean(JobContext.MAP_OUTPUT_COMPRESS, compress);</span><br><span class="line">  &#125;</span><br><span class="line">再点进去</span><br><span class="line"></span><br><span class="line">public static final String MAP_OUTPUT_COMPRESS = &quot;mapreduce.map.output.compress&quot;;</span><br><span class="line"></span><br><span class="line">mapreduce.map.output.compress 这个参数不就是设置map端的输出压缩</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec 这个参数 </span><br><span class="line">设置最终输出的压缩 前面的文章写过</span><br><span class="line"></span><br><span class="line">所以这底层的代码就是Mapreduce代码 **** 就是Spark给封装好的</span><br></pre></td></tr></table></figure></div>

<h2 id="补充知识点"><a href="#补充知识点" class="headerlink" title="补充知识点"></a>补充知识点</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      *  map vs mapPartitions(优先选择)  transformation</span><br><span class="line">      *</span><br><span class="line">      *  RDD 1w个元素  需要你把这个RDD的元素写入到MySQL （前提没有使用数据库连接池那种）</span><br><span class="line">      * 1w个元素 我要写1w次MySQL</span><br><span class="line">      * </span><br><span class="line">      *  RDD 1w个元素  10分区   10次</span><br><span class="line">      *</span><br><span class="line">map vs mapPartitions ：</span><br><span class="line">这两个写入db哪个好？   优先级角度选 mapPartitions 是没有问题的 能解决80%生产上的操作</span><br><span class="line">分情况的 应该说都不好  你应该想说 mapPartitions好 但是</span><br><span class="line">当你数据量很大 分区数很少 那么一个分区里的数据量很大 写的时候 可能oom </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      *</span><br><span class="line">      *  foreach vs foreachPartition   action</span><br><span class="line">      *</span><br><span class="line">      * rdd==&gt; transformations ==&gt; action</span><br><span class="line">      *  真正生产上把RDD的数据写入到DB，是使用foreachPartition</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03ToMySQL &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_)</span><br><span class="line">        .sortBy(-_._2)</span><br><span class="line">        .foreachPartition(partition =&gt;&#123;</span><br><span class="line">          var connection : Connection = null</span><br><span class="line">          var pstmt:PreparedStatement = null</span><br><span class="line">          try&#123;</span><br><span class="line">            connection = MySQLUtils.getConnection()</span><br><span class="line">            val sql = &quot;insert into topn(domain,url,cnt) values (?,?,?)&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            //真正的数据是分区里的元素</span><br><span class="line">            partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.execute()  </span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;catch &#123;</span><br><span class="line">            case  e:Exception =&gt; e.printStackTrace()</span><br><span class="line">          &#125;finally &#123;</span><br><span class="line">            MySQLUtils.closeResource(pstmt,connection)  //这块只关闭connection可以么? 可以的 java知识</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">object MySQLUtils &#123;</span><br><span class="line">  def getConnection():Connection = &#123;</span><br><span class="line">    Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    getConnection(&quot;hadoop101&quot;, &quot;3306&quot;, &quot;hive_dwd&quot;, &quot;root&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  def getConnection(host: String, port: String, database: String, user: String, password: String):Connection = &#123;</span><br><span class="line">    Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    DriverManager.getConnection(s&quot;jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/$&#123;database&#125;&quot;, user, password)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 关闭资源</span><br><span class="line">    *</span><br><span class="line">    * @param resources 可变数组</span><br><span class="line">    */</span><br><span class="line">  def closeResource(resources: AutoCloseable*): Unit = &#123;</span><br><span class="line">    for (resource &lt;- resources) &#123;</span><br><span class="line">      resource.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from topn;</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| domain          | url   | cnt  |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| www.baidu.com   | url2  |    2 |</span><br><span class="line">| www.baidu.com   | url5  |    5 |</span><br><span class="line">| www.baidu.com   | url1  |    1 |</span><br><span class="line">| www.baidu.com   | url4  |    4 |</span><br><span class="line">| www.baidu.com   | url3  |    3 |</span><br><span class="line">| www.twitter.com | url10 |   11 |</span><br><span class="line">| www.twitter.com | url6  |    1 |</span><br><span class="line">| www.twitter.com | url9  |    6 |</span><br><span class="line">| www.google.com  | url2  |    2 |</span><br><span class="line">| www.google.com  | url6  |    7 |</span><br><span class="line">| www.google.com  | url1  |    1 |</span><br><span class="line">| www.google.com  | url8  |    7 |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">能使用scalikejdbc 把数据写入MySQL更好哈   前面scala篇有讲</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">问题指出：</span><br><span class="line">  partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.execute()  </span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">如果你一个partition 一个元素执行一次  里有1w个元素呢？   性能不好</span><br><span class="line"></span><br><span class="line">肯定是要批处理的   一个批次给它搞一个事务 把自动提交给关掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03ToMySQL &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_)</span><br><span class="line">        .sortBy(-_._2)</span><br><span class="line">        .foreachPartition(partition =&gt;&#123;</span><br><span class="line">          var connection : Connection = null</span><br><span class="line">          var pstmt:PreparedStatement = null</span><br><span class="line">          try&#123;</span><br><span class="line">            connection = MySQLUtils.getConnection()</span><br><span class="line">            connection.setAutoCommit(false)</span><br><span class="line"></span><br><span class="line">     /*       //先把之前的数据删掉   </span><br><span class="line">            val sqlDelete = s&quot;delete from topn where access_time = $&#123;args(0)&#125;&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sqlDelete)</span><br><span class="line">            pstmt.execute()*/</span><br><span class="line"></span><br><span class="line">            //再插入数据</span><br><span class="line">            val sql = &quot;insert into topn(domain,url,cnt) values (?,?,?)&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            //真正的数据是分区里的元素</span><br><span class="line">            partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.addBatch()</span><br><span class="line">            &#125;)</span><br><span class="line">            pstmt.executeBatch()   //执行批次</span><br><span class="line">            connection.commit()  //提交事务</span><br><span class="line">          &#125;catch &#123;</span><br><span class="line">            case  e:Exception =&gt; e.printStackTrace()</span><br><span class="line">          &#125;finally &#123;</span><br><span class="line">            MySQLUtils.closeResource(pstmt,connection)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; truncate table topn;       //代码里加上删除之前的数据 可以解决 最好别truncate 只是学习时方便</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from topn; </span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| domain          | url   | cnt  |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| www.baidu.com   | url5  |    5 |</span><br><span class="line">| www.baidu.com   | url2  |    2 |</span><br><span class="line">| www.baidu.com   | url4  |    4 |</span><br><span class="line">| www.baidu.com   | url1  |    1 |</span><br><span class="line">| www.baidu.com   | url3  |    3 |</span><br><span class="line">| www.twitter.com | url6  |    1 |</span><br><span class="line">| www.twitter.com | url10 |   11 |</span><br><span class="line">| www.twitter.com | url9  |    6 |</span><br><span class="line">| www.google.com  | url2  |    2 |</span><br><span class="line">| www.google.com  | url6  |    7 |</span><br><span class="line">| www.google.com  | url1  |    1 |</span><br><span class="line">| www.google.com  | url8  |    7 |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><p>工作当中是再idea里开发的 在生产上是在Submitting Applications</p>
<p><img src="https://img-blog.csdnimg.cn/20191024192531620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.Spark-shell 底层调用的是Spark-submit</span><br><span class="line"></span><br><span class="line">Spark-submit怎么使用呢？</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications" target="_blank" rel="noopener">Submitting Applications</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">idea里面：</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">        val conf = new SparkConf()       //提交到集群上的时候 setAppName 和setMater全都去掉</span><br><span class="line">        val sc = new SparkContext(conf)</span><br><span class="line">        val input = sc.textFile(args(0))</span><br><span class="line">        val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot;,&quot;)</span><br><span class="line">          val site = splits(0)</span><br><span class="line">          val url = splits(1)</span><br><span class="line">          ((site, url), 1)</span><br><span class="line">        &#125;).reduceByKey(_+_).saveAsTextFile(args(1))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">打包提交到集群上去</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --help</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 bin]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[]  可选  &lt;&gt; 必选</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  \</span><br><span class="line">--name InterviewApp03 \</span><br><span class="line">--class com.ruozedata.spark.spark05.InterviewApp03 \</span><br><span class="line">--master local[2] \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">/data_spark/input/  /data_spark/output </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">抛出一个问题 我的路径前面没有添加 hdfs:xxx:8020/路径  为什么我的不用加 ？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024200131809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 lib]$ hadoop fs -text /data_spark/output/par*</span><br><span class="line">19/10/24 20:05:35 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">19/10/24 20:05:35 INFO compress.CodecPool: Got brand-new decompressor [.bz2]</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line">((www.baidu.com,url1),1)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line">((www.google.com,url1),1)</span><br><span class="line">((www.baidu.com,url3),3)</span><br><span class="line">((www.google.com,url2),2)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url6),1)</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url2),2)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">[double_happy@hadoop101 lib]$</span><br></pre></td></tr></table></figure></div>
<p>If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a Spark cluster. <strong>To do this, create an assembly jar (or “uber” jar) containing your code and its dependencies.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">你的代码 依赖hadoop spark 但是这些集群本身 就有 打包的时候 要的是瘦包 如果需要第三方的包</span><br><span class="line">可以使用  --jars  或者 package到application 里 </span><br><span class="line"></span><br><span class="line">个人不建议使用assembly 打包</span><br><span class="line"></span><br><span class="line">到ss 打包再讨论</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line"></span><br><span class="line">第三方包 加入这个命令</span><br></pre></td></tr></table></figure></div>
<h2 id="Loading-Configuration-from-a-File"><a href="#Loading-Configuration-from-a-File" class="headerlink" title="Loading Configuration from a File"></a>Loading Configuration from a File</h2><p>The <strong>spark-submit script</strong> can load default Spark configuration values from a properties file and pass them on to your application. By default, it will <strong>read options from conf/spark-defaults.conf in the Spark directory.</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">spark-submit 默认会加载 conf/spark-defaults.conf   in the Spark directory </span><br><span class="line"></span><br><span class="line">当然也可以人为指定 ：</span><br><span class="line"> --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"># spark.master                     spark://master:7077</span><br><span class="line"># spark.eventLog.enabled           true</span><br><span class="line"># spark.eventLog.dir               hdfs://namenode:8021/directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br><span class="line">[double_happy@hadoop101 conf]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">带来的好处是什么？</span><br><span class="line">默认情况下 你每次开启spark-shell  --master local[2]</span><br><span class="line"> 每次都要手动加参数 </span><br><span class="line"> 可以再 spark-defalut.xml 里加上即可。</span><br><span class="line"></span><br><span class="line">如果你的业务线非常多 你就多写几个spark-defalut.xml  通过 --files 传过去你想要的模式</span><br></pre></td></tr></table></figure></div>
<p>以上是最基本的操作</p>
<p>它默认走的是 spark-defalut.xml 那么底层的实现一定是走的默认参数的</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>上面跑的程序<br><img src="https://img-blog.csdnimg.cn/20191024203040986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">成功完之后 SC被干掉了 UI上面还能看到么？</span><br><span class="line">看不见了 如果半夜三点程序挂了 或者在调优的场景下  你程序跑完 UI就没了呀 该怎么解决呢？</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/monitoring.html#web-interfaces" target="_blank" rel="noopener">Monitoring</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;c&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;d&quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((d,1), (b,2), (a,2), (c,2))                  </span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024203854192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:<br>        A list of scheduler stages and tasks<br>        A summary of RDD sizes and memory usage<br>        Environmental information.<br>        Information about the running executors</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.页面上展示 有多少个job （就是代码里有多少个action）</span><br><span class="line">2.stages ---&gt;一个 job里面  有多少个stage    点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204201773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.一个stage有几个task呢？点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204355319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Information about the running executors：</span><br><span class="line">关于你运行的executors信息 </span><br><span class="line">1.你运行这个程序 设置了多少个executors 活的有多少 死的有多少</span><br><span class="line">2.对调优很重要 </span><br><span class="line">你肯定要看 ：</span><br><span class="line">   1.shuffle的数据量有多少 </span><br><span class="line">   2.经历多少算子</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204640805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这副图运行时间慢的原因是 ：<br>emmm 总有贪小便宜的人 去我云服务器上挖矿 悄踏玛真的很烦 因为我把端口全部开放了</p>
<p> If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc).</p>
<p>这句话挺重要的 因为之前在公司里 我喜欢用 yarn client模式 而我提交的任务比较多<br>达到特别多的时候 再提交任务 是排不上的哈 提交不上的 </p>
<p><strong>Note that this information is only available for the duration of the application by default</strong>. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Note that this information is only available for the duration of the application by default</span><br><span class="line"></span><br><span class="line">1.默认情况下 这个页面只能 在 application 生命周期内有效  所以运行完 sc.stop 之后</span><br><span class="line">你的UI界面就看不见了 </span><br><span class="line">2.spark.eventLog.enabled 这个参数设置为true  开启spark日志 当你再打开ui界面 可以看的到 运行完的application</span><br></pre></td></tr></table></figure></div>
<p>那么怎么构建 总结的UI呢？<br><img src="https://img-blog.csdnimg.cn/20191024224445859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.spark.eventLog.enabled true   开关打开</span><br><span class="line">2.spark.eventLog.dir hdfs://namenode/shared/spark-logs    </span><br><span class="line">记录spark运行过程中日志记录在这个目录  这是spark作业触发的</span><br><span class="line"></span><br><span class="line">3.使用start-history-server.sh 展示记录的spark日志  也需要一个目录  </span><br><span class="line">spark.history.fs.logDirectory 用这个参数 </span><br><span class="line">这个参数 是配置spark-env.sh里</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">这个跟配置压缩一样 打开开关 + 指定codec</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"> spark.master                     local[2]</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"> spark.eventLog.dir               hdfs://hadoop101:8020/spark_directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/monitoring.html#viewing-after-the-fact" target="_blank" rel="noopener">参数配置</a></p>
<p>This creates a web interface at http://<server-url>:18080 by default, listing <strong>incomplete</strong> and <strong>completed</strong> applications and attempts.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.completed 和 incomplete spark怎么区别呢 需要一个刷新时间 </span><br><span class="line">用这个参数 spark.history.fs.update.interval</span><br><span class="line"></span><br><span class="line">2.如果配置成功之后日志日积月累多了 数据量会很大 所以需要定期删除的</span><br><span class="line">spark.history.fs.cleaner.enabled     是否需要清理呢</span><br><span class="line">spark.history.fs.cleaner.interval         清理周期是多少</span><br><span class="line">spark.history.fs.cleaner.maxAge        一次清理几天的</span><br></pre></td></tr></table></figure></div>
<p>配置一下 ：<br><img src="https://img-blog.csdnimg.cn/20191024230341733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>启动：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.hdfs 上创建 log 文件夹</span><br><span class="line">2.启动./sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">测试查看是否成功</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ ./start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.deploy.history.HistoryServer-1-hadoop101.out</span><br><span class="line">[double_happy@hadoop101 sbin]$ tail -200f /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.deploy.history.HistoryServer-1-hadoop101.out</span><br><span class="line">Spark Command: /usr/java/java/bin/java -cp /home/double_happy/app/spark/conf/:/home/double_happy/app/spark/jars/*:/home/double_happy/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://hadoop101:8020/spark_directory -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">========================================</span><br><span class="line">19/10/24 23:06:35 INFO HistoryServer: Started daemon with process name: 6633@hadoop101</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for TERM</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for HUP</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for INT</span><br><span class="line">19/10/24 23:06:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/24 23:06:35 INFO FsHistoryProvider: History server ui acls disabled; users with admin permissions: ; groups with admin permissions</span><br><span class="line">19/10/24 23:06:37 INFO Utils: Successfully started service on port 18080.</span><br><span class="line">19/10/24 23:06:37 INFO HistoryServer: Bound HistoryServer to 0.0.0.0, and started at http://hadoop101:18080</span><br></pre></td></tr></table></figure></div>
<p>说明启动ok </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">测试：</span><br><span class="line">1.spark-shell 运行了一个东西</span><br><span class="line">2.spark-submit 提交了两次</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.</span><br><span class="line">scala&gt;  sc.parallelize(List(&quot;a&quot;,&quot;c&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;d&quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((d,1), (b,2), (a,2), (c,2))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.stop</span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">spark-submit  \</span><br><span class="line">&gt; --name InterviewApp03 \</span><br><span class="line">&gt; --class com.ruozedata.spark.spark05.InterviewApp03 \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; /data_spark/input/  /data_spark/output</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024231709105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Note that in all of these UIs, the tables are sortable by <strong>clicking their headers,</strong> making it easy to <strong>identify slow tasks, data skew, etc</strong>.</p>
<p>Note<br>1.<strong>The history server displays both completed and incomplete Spark jobs.</strong> If an application makes multiple attempts after failures, the failed attempts will be displayed, as well as any ongoing incomplete attempt or the final successful attempt.</p>
<p>2.Incomplete applications are only updated intermittently. The time between updates is defined by the interval between checks for changed files (spark.history.fs.update.interval). On larger clusters, the update interval may be set to large values. The way to view a running application is actually to view its own web UI.</p>
<p>3.Applications which exited without registering themselves as completed will be listed as incomplete —even though they are no longer running. This can happen if an application crashes.</p>
<p><strong>4.One way to signal the completion of a Spark job is to stop the Spark Context explicitly (sc.stop()), or in Python using the with SparkContext() as sc: construct to handle the Spark Context setup and tear down.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4.就是你代码里 sc.stop() 写了  程序完成后会显示在 completed 里面 如果不写会显示在incomplete </span><br><span class="line">所以 为了 好区分正在运行的作业还是 完成的作业 sc.stop() 要加上的</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191025111143777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191025111221278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.本地运行的作业 全部以 local开头的 </span><br><span class="line">2.ui上显示很多信息   点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102423185850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这个页面不就回来了么 程序已经运行完了   测试成功。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Download 之后就是个Json文件 也可以去HDFS上去看我配置的log目录 也可以下载的</span><br><span class="line">整个历史页面 就是靠 Json文件 来渲染的</span><br><span class="line"></span><br><span class="line">所以这个东西有了 spark调优就方便了很多</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024232531503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="start-history-server-sh"><a href="#start-history-server-sh" class="headerlink" title="start-history-server.sh"></a>start-history-server.sh</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ cat start-history-server.sh </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Starts the history server on the machine this script is executed on.</span><br><span class="line">#</span><br><span class="line"># Usage: start-history-server.sh</span><br><span class="line">#</span><br><span class="line"># Use the SPARK_HISTORY_OPTS environment variable to set history server configuration.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;/sbin/spark-config.sh&quot;</span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh&quot;</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;/sbin&quot;/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 &quot;$@&quot;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">去idea里找到 HistoryServer类：</span><br><span class="line">1.这个类一定有main方法</span><br><span class="line"></span><br><span class="line"> def main(argStrings: Array[String]): Unit = &#123;</span><br><span class="line">    Utils.initDaemon(log)</span><br><span class="line">    new HistoryServerArguments(conf, argStrings)</span><br><span class="line">    initSecurity()</span><br><span class="line">    val securityManager = createSecurityManager(conf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. new HistoryServerArguments(conf, argStrings)  点进去 </span><br><span class="line">里面的  解析</span><br><span class="line">// This mutates the SparkConf, so all accesses to it must be made after this line</span><br><span class="line">   Utils.loadDefaultSparkProperties(conf, propertiesFile)</span><br><span class="line"></span><br><span class="line">3.def loadDefaultSparkProperties(conf: SparkConf, filePath: String = null): String = &#123;</span><br><span class="line">    val path = Option(filePath).getOrElse(getDefaultPropertiesFile())</span><br><span class="line">    Option(path).foreach &#123; confFile =&gt;</span><br><span class="line">      getPropertiesFromFile(confFile).filter &#123; case (k, v) =&gt;</span><br><span class="line">        k.startsWith(&quot;spark.&quot;)</span><br><span class="line">      &#125;.foreach &#123; case (k, v) =&gt;</span><br><span class="line">        conf.setIfMissing(k, v)</span><br><span class="line">        sys.props.getOrElseUpdate(k, v)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    path</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">4.getDefaultPropertiesFile</span><br><span class="line"></span><br><span class="line">def getDefaultPropertiesFile(env: Map[String, String] = sys.env): String = &#123;</span><br><span class="line">    env.get(&quot;SPARK_CONF_DIR&quot;)</span><br><span class="line">      .orElse(env.get(&quot;SPARK_HOME&quot;).map &#123; t =&gt; s&quot;$t$&#123;File.separator&#125;conf&quot; &#125;)</span><br><span class="line">      .map &#123; t =&gt; new File(s&quot;$t$&#123;File.separator&#125;spark-defaults.conf&quot;)&#125;</span><br><span class="line">      .filter(_.isFile)</span><br><span class="line">      .map(_.getAbsolutePath)</span><br><span class="line">      .orNull</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">能知道 加载 spark-defaults.conf 文件 明白了吗 </span><br><span class="line">详细脚本实现 自己看源码</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024233636507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">源码里有写：</span><br><span class="line">spark.history.retainedApplications   ： 默认50个</span><br><span class="line"></span><br><span class="line">The number of applications to retain UI data for in the cache. If this cap is exceeded, then the oldest applications will be removed from the cache. If an application is not in the cache, it will have to be loaded from disk if it is accessed from the UI.</span><br><span class="line"></span><br><span class="line">1.The number of applications to retain UI data for in the cache</span><br><span class="line"> 是在内存中的</span><br><span class="line"> 这个参数 不是 ui上面只能展示50个意思哈   是内存里只放50个 超过了 removed from the cache </span><br><span class="line"> 2. 看解释 很清楚</span><br></pre></td></tr></table></figure></div>
<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p><img src="https://img-blog.csdnimg.cn/2019102510374444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Shared-Variables"><a href="#Shared-Variables" class="headerlink" title="Shared Variables"></a>Shared Variables</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables" target="_blank" rel="noopener">Shared Variables</a><br>Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. <strong>These variables are copied to each machine</strong>, and <strong>no updates</strong> to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: <strong>broadcast variables and accumulators.</strong></p>
<p>Accumulators are variables that are <strong>only “added”</strong> to through an associative and commutative operation and can therefore <strong>be efficiently supported in parallel.</strong><br>Spark natively supports accumulators of <strong>numeric types</strong>, and <strong>programmers can add support for new types.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accumulators：</span><br><span class="line">使用场景：ETl处理的时候 把正确的条数 和总的条数  统计出来 </span><br><span class="line">1.原生的支持数值类型 ，开发者开发可以支持别的类型</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val accum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res1: Long = 10</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102510514440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在的计数器都是AccumulatorV2 版本  官网上写了如何自定义累加器 生产上我没用用过</span><br></pre></td></tr></table></figure></div>
<p>案例：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    var cnts = 0</span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,7,8),3)</span><br><span class="line">    data.foreach(x =&gt; &#123;</span><br><span class="line">      cnts += 1</span><br><span class="line">      println(s&quot;cnts:$cnts&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line">    println(cnts+&quot;~~~~~~~&quot;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">cnts:1</span><br><span class="line">cnts:1</span><br><span class="line">cnts:2</span><br><span class="line">cnts:2</span><br><span class="line">cnts:3</span><br><span class="line">cnts:1</span><br><span class="line">cnts:2</span><br><span class="line">cnts:3</span><br><span class="line">0~~~~~~~</span><br><span class="line"></span><br><span class="line">这个结果什么意思？是想要的结果么？</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	上面这个 不是并行的 也就是 多个分区 是无法计算的 </span><br><span class="line">需要使用计数器 </span><br><span class="line">   计数器一定是在action算子之后使用   一定是要触发action的要不然拿不到结果 </span><br><span class="line">那么触发多个action可以么？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">broadcast variables ：</span><br><span class="line">类似的</span><br><span class="line">1.前面解析ip库的时候，使用的mapreduce的分布式缓存 </span><br><span class="line">2.在sql里的 </span><br><span class="line"></span><br><span class="line">a join b on a.id=b.id ==&gt; shuffle  (普通的join 必然有shuffle的)  </span><br><span class="line">会按照join的条件 作为key(就是id)，其他的值作为value </span><br><span class="line"> 经过shuffle到reduce端 把相同的key聚在一块 来做的</span><br><span class="line"></span><br><span class="line">大数据集和小数据集join ==&gt;采用 mapjoin</span><br><span class="line"> 小表放到缓存中，不会有真正的join发生，底层其实就是一个匹配  匹配上拿出来 匹配不上就滚蛋的</span><br><span class="line"> </span><br><span class="line"> 那么 上面的这些可以使用 broadcast variables 来实现</span><br></pre></td></tr></table></figure></div>
<p>Broadcast variables allow the programmer to keep a read-only variable <strong>cached on each machine rather than shipping a copy of it with tasks.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">eg:这个代码  </span><br><span class="line">val xx = new HashMap() // 10M</span><br><span class="line">rdd.map(x=&gt;&#123;</span><br><span class="line">    ....xx</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">如果有一万个task  每个task里 额外的  1w*10M</span><br><span class="line"></span><br><span class="line">Broadcast variables cached on each machine （理解为executor就可以）而不是 cp到tasks</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">object RDDOperationApp02 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    //广播的时候 要使用 kv的 最佳实践</span><br><span class="line">    val rdd1 = sc.parallelize(Array((&quot;23&quot;,&quot;smart&quot;),(&quot;9&quot;,&quot;愤怒的麻雀&quot;))).collectAsMap()</span><br><span class="line">    val rdd2 = sc.parallelize(Array((&quot;23&quot;,&quot;郑州&quot;),(&quot;9&quot;,&quot;蜀国&quot;),(&quot;14&quot;,&quot;魔都&quot;)))</span><br><span class="line">    val rdd1_bc = sc.broadcast(rdd1)</span><br><span class="line">    rdd2.map(x=&gt;(x._1,x)).mapPartitions(x =&gt; &#123;</span><br><span class="line">      val bc_value = rdd1_bc.value</span><br><span class="line">      for((k,v)&lt;- x if(bc_value.contains(k)))</span><br><span class="line">        yield (k, bc_value.get(k).getOrElse(&quot;&quot;), v._2)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(9,愤怒的麻雀,蜀国)</span><br><span class="line">(23,smart,郑州)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Scala中的yield的主要作用是记住每次迭代中的有关值，并逐一存入到一个数组中。</span><br><span class="line">要将结果存放到数组的变量或表达式必须放在yield&#123;&#125;里最后位置</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(Array((&quot;23&quot;,&quot;smart&quot;),(&quot;9&quot;,&quot;愤怒的麻雀&quot;))).collectAsMap()</span><br><span class="line">rdd1: scala.collection.Map[String,String] = Map(23 -&gt; smart, 9 -&gt; 愤怒的麻雀)</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(Array((&quot;23&quot;,&quot;郑州&quot;),(&quot;9&quot;,&quot;蜀国&quot;),(&quot;14&quot;,&quot;魔都&quot;)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1_bc = sc.broadcast(rdd1)</span><br><span class="line">rdd1_bc: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,String]] = Broadcast(3)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.map(x=&gt;(x._1,x)).mapPartitions(x =&gt; &#123;</span><br><span class="line">     |   val bc_value = rdd1_bc.value</span><br><span class="line">     |   for((k,v)&lt;- x if(bc_value.contains(k)))</span><br><span class="line">     |     yield (k, bc_value.get(k).getOrElse(&quot;&quot;), v._2)</span><br><span class="line">     | &#125;).foreach(println)</span><br><span class="line">(23,smart,郑州)</span><br><span class="line">(9,愤怒的麻雀,蜀国)</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">查看页面 是没有shuffle的 没有join的 就是mapjoin</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025114937388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>sparkcore之后sparksql 以及sparkstreaming 、sss、spark调优  的重要的文章是进行私密的 我写博客的目的是为了做笔记 为了学习</p>

      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/01/26/Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            Spark009--spark-shell执行流程
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/24/Spark007-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark007--综合案例</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://sxwanggit126.github.io/" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>