<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark003--Action  接着Spark002 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="回顾上篇文章：RDD:    是什么    五大特性对应五大方法    创建方式：3    操作：2 action &amp;amp; transformation Spark作业开发流程：也就是：数据源–&amp;gt;经过一堆transformtion–&amp;gt;action 触发spark作业 —&amp;gt;输出到某个地方 你的业务无论多么复杂 都是这样的。 Action（1）collect 1234567891">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark003--Action  接着Spark002">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;19&#x2F;Spark003-Action-%E6%8E%A5%E7%9D%80Spark002&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="回顾上篇文章：RDD:    是什么    五大特性对应五大方法    创建方式：3    操作：2 action &amp;amp; transformation Spark作业开发流程：也就是：数据源–&amp;gt;经过一堆transformtion–&amp;gt;action 触发spark作业 —&amp;gt;输出到某个地方 你的业务无论多么复杂 都是这样的。 Action（1）collect 1234567891">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022080418772.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022081833635.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022082000435.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022085445570.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022085458568.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022085757146.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022092526573.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:00:21.897Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191022080418772.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-Spark003-Action-接着Spark002" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      Spark003--Action  接着Spark002
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/19/Spark003-Action-%E6%8E%A5%E7%9D%80Spark002/" class="article-date">
  <time datetime="2018-01-19T11:59:54.000Z" itemprop="datePublished">2018-01-19</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="回顾上篇文章："><a href="#回顾上篇文章：" class="headerlink" title="回顾上篇文章："></a>回顾上篇文章：</h2><p>RDD:<br>    是什么<br>    五大特性对应五大方法<br>    创建方式：3<br>    操作：2 action &amp; transformation</p>
<p>Spark作业开发流程：<br><img src="https://img-blog.csdnimg.cn/20191022080418772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>也就是：<br>数据源–&gt;经过一堆transformtion–&gt;action 触发spark作业 —&gt;输出到某个地方</p>
<p>你的业务无论多么复杂 都是这样的。</p>
<h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p>（1）collect</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Return an array that contains all of the elements in this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">   * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">   */</span><br><span class="line">  def collect(): Array[T] = withScope &#123;</span><br><span class="line">    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)</span><br><span class="line">    Array.concat(results: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.Return an array that contains all of the elements in this RDD.</span><br><span class="line">2.resulting array is expected to be small</span><br><span class="line">3. the data is loaded into the driver&apos;s memory.</span><br><span class="line">所以生产上你想看这个rdd里的数据 是不太现实的 会导致某种oom的，(oom有好多种的)</span><br><span class="line">如果你还是想看rdd里的元素 该怎么办呢？</span><br><span class="line">两种方法：</span><br><span class="line">1) 取出部分数据</span><br><span class="line">2) 把rdd输出到文件系统</span><br><span class="line">真正生产上使用collect只有一个地方：？？？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure></div>
<p>(2)foreach<br><img src="https://img-blog.csdnimg.cn/20191022081833635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Applies a function f to all elements of this RDD.</span><br><span class="line">  */</span><br><span class="line"> def foreach(f: T =&gt; Unit): Unit = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreach(println)</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>注意：<br>我在spark-shell  –master local[2] 模式下 rdd.foreach(println) 会显示出结果，如果在<br>spark-shell  –master yarn 模式下 rdd.foreach(println) 会显示出结果么？为什么呢？</p>
<p>(3)foreachPartition<br><img src="https://img-blog.csdnimg.cn/20191022082000435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Applies a function f to each partition of this RDD.</span><br><span class="line">  */</span><br><span class="line"> def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreachPartition(println)</span><br><span class="line">non-empty iterator</span><br><span class="line">non-empty iterator</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res5: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">返回的是non-empty iterator 怎么才能把里面的内容输出出来呢？</span><br><span class="line"></span><br><span class="line">如果这样写呢？</span><br><span class="line"> rdd.foreachPartition(paritition =&gt; paritition.map(println))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreachPartition(paritition =&gt; paritition.map(println))</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>能不能想到这是什么问题导致的？<br>foreachPartition(paritition =&gt; paritition.map(println)) 输出结果在正在执行的机器上面是有的<br>而控制台看到的是driver的 </p>
<p>正好引入一个东西：<br>sortBy 上次的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2.sortBy(_._2,false)</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">sortBy是全局排序的还是分区排序的？</span><br><span class="line"></span><br><span class="line">上面的两行代码看仔细了 ， 是两个分区 ,按照降序排</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">同样的代码我再运行一次：</span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>sortBy是全局排序的还是分区排序的？通过上面的测试知道了吗？ 知道个鬼<br>是不是感觉是分区排序</p>
<p>去idea上输出结果看一下：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line"></span><br><span class="line">object ActionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">    rdd2.sortBy(_._2,false).saveAsTextFile(&quot;file:///Users/double_happy/zz/G7-03/工程/scala-spark/doc/out&quot;)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191022085445570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191022085458568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>难道真的是分区排序么？在进行测试。<br><img src="https://img-blog.csdnimg.cn/20191022085757146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>为什么rdd2.sortBy(<em>.</em>2,false).foreach(println)的结果不一样？<br>所以使用foreach在这里根本看不出来sortBy是全局排序还是分区排序</p>
<p><strong>因为 rdd2是两个分区的 ，foreach执行的时候 不确定是哪个task先println 出来 明白吗？</strong></p>
<p><strong>所以sortBy 到底是什么排序？</strong><br>全局排序      你看idea里的 </p>
<p>所以你测试的时候 sortBy 后面不能跟着 foreach 来测试 要输出文件</p>
<p>通过 读取文件  来测试 </p>
<p>(3)count</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Return the number of elements in the RDD.</span><br><span class="line"> */</span><br><span class="line">def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(4) reduce   两两做操作</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce(_+_)</span><br><span class="line">res6: Int = 15</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(5) first</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return the first element in this RDD.</span><br><span class="line">  */</span><br><span class="line"> def first(): T = withScope &#123;</span><br><span class="line">   take(1) match &#123;</span><br><span class="line">     case Array(t) =&gt; t</span><br><span class="line">     case _ =&gt; throw new UnsupportedOperationException(&quot;empty collection&quot;)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>(6)take</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Take the first num elements of the RDD. It works by first scanning one partition, and use the</span><br><span class="line">  * results from that partition to estimate the number of additional partitions needed to satisfy</span><br><span class="line">  * the limit.</span><br><span class="line">  *</span><br><span class="line">  * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">  * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">  *</span><br><span class="line">  * @note Due to complications in the internal implementation, this method will raise</span><br><span class="line">  * an exception if called on an RDD of `Nothing` or `Null`.</span><br><span class="line">  */</span><br><span class="line"> def take(num: Int): Array[T] = withScope &#123;</span><br><span class="line">   val scaleUpFactor = Math.max(conf.getInt(&quot;spark.rdd.limit.scaleUpFactor&quot;, 4), 2)</span><br><span class="line">   if (num == 0) &#123;</span><br><span class="line">     new Array[T](0)</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     val buf = new ArrayBuffer[T]</span><br><span class="line">     val totalParts = this.partitions.length</span><br><span class="line">     var partsScanned = 0</span><br><span class="line">     while (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123;</span><br><span class="line">       // The number of partitions to try in this iteration. It is ok for this number to be</span><br><span class="line">       // greater than totalParts because we actually cap it at totalParts in runJob.</span><br><span class="line">       var numPartsToTry = 1L</span><br><span class="line">       val left = num - buf.size</span><br><span class="line">       if (partsScanned &gt; 0) &#123;</span><br><span class="line">         // If we didn&apos;t find any rows after the previous iteration, quadruple and retry.</span><br><span class="line">         // Otherwise, interpolate the number of partitions we need to try, but overestimate</span><br><span class="line">         // it by 50%. We also cap the estimation in the end.</span><br><span class="line">         if (buf.isEmpty) &#123;</span><br><span class="line">           numPartsToTry = partsScanned * scaleUpFactor</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">           // As left &gt; 0, numPartsToTry is always &gt;= 1</span><br><span class="line">           numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt</span><br><span class="line">           numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)</span><br><span class="line">       val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p)</span><br><span class="line"></span><br><span class="line">       res.foreach(buf ++= _.take(num - buf.size))</span><br><span class="line">       partsScanned += p.size</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     buf.toArray</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>first底层调用take方法</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce(_+_)</span><br><span class="line">res6: Int = 15</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first</span><br><span class="line">res7: Int = 1</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(2)</span><br><span class="line">res8: Array[Int] = Array(1, 2)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(7) top<br>里面肯定是做了排序的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Returns the top k (largest) elements from this RDD as defined by the specified</span><br><span class="line">   * implicit Ordering[T] and maintains the ordering. This does the opposite of</span><br><span class="line">   * [[takeOrdered]]. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)</span><br><span class="line">   *   // returns Array(12)</span><br><span class="line">   *</span><br><span class="line">   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)</span><br><span class="line">   *   // returns Array(6, 5)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">   * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">   *</span><br><span class="line">   * @param num k, the number of top elements to return</span><br><span class="line">   * @param ord the implicit ordering for T</span><br><span class="line">   * @return an array of top elements</span><br><span class="line">   */</span><br><span class="line">  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123;</span><br><span class="line">    takeOrdered(num)(ord.reverse)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1. This does the opposite of</span><br><span class="line">   * [[takeOrdered]].</span><br><span class="line"></span><br><span class="line">2.top 底层调用的是 takeOrdered</span><br><span class="line"></span><br><span class="line">3.top 柯里化的 Ordering 看scala篇这部分 讲的很详细</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.top(2)</span><br><span class="line">res9: Array[Int] = Array(5, 4)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeOrdered(2)</span><br><span class="line">res10: Array[Int] = Array(1, 2)</span><br></pre></td></tr></table></figure></div>

<p>(8)zipWithIndex</p>
<p>给你一个算子 你怎么知道他是 action还是 transformation？？</p>
<p><strong>action算子里面是有sc.runJob()方法的</strong>  </p>
<p>eg：<br><img src="https://img-blog.csdnimg.cn/20191022092526573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>所以zipWithIndex 它不是action算子</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Zips this RDD with its element indices. The ordering is first based on the partition index</span><br><span class="line">  * and then the ordering of items within each partition. So the first item in the first</span><br><span class="line">  * partition gets index 0, and the last item in the last partition receives the largest index.</span><br><span class="line">  *</span><br><span class="line">  * This is similar to Scala&apos;s zipWithIndex but it uses Long instead of Int as the index type.</span><br><span class="line">  * This method needs to trigger a spark job when this RDD contains more than one partitions.</span><br><span class="line">  *</span><br><span class="line">  * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of</span><br><span class="line">  * elements in a partition. The index assigned to each element is therefore not guaranteed,</span><br><span class="line">  * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee</span><br><span class="line">  * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.</span><br><span class="line">  */</span><br><span class="line"> def zipWithIndex(): RDD[(T, Long)] = withScope &#123;</span><br><span class="line">   new ZippedWithIndexRDD(this)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex</span><br><span class="line">res11: org.apache.spark.rdd.RDD[(Int, Long)] = ZippedWithIndexRDD[31] at zipWithIndex at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex.collect</span><br><span class="line">res12: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4))</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(9)countByKey</p>
<p>这是action算子</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Count the number of elements for each key, collecting the results to a local Map.</span><br><span class="line">  *</span><br><span class="line">  * @note This method should only be used if the resulting map is expected to be small, as</span><br><span class="line">  * the whole thing is loaded into the driver&apos;s memory.</span><br><span class="line">  * To handle very large results, consider using rdd.mapValues(_ =&gt; 1L).reduceByKey(_ + _), which</span><br><span class="line">  * returns an RDD[T, Long] instead of a map.</span><br><span class="line">  */</span><br><span class="line"> def countByKey(): Map[K, Long] = self.withScope &#123;</span><br><span class="line">   self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>(10)collectAsMap  针对kv类型的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return the key-value pairs in this RDD to the master as a Map.</span><br><span class="line">  *</span><br><span class="line">  * Warning: this doesn&apos;t return a multimap (so if you have multiple values to the same key, only</span><br><span class="line">  *          one value per key is preserved in the map returned)</span><br><span class="line">  *</span><br><span class="line">  * @note this method should only be used if the resulting data is expected to be small, as</span><br><span class="line">  * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">  */</span><br><span class="line"> def collectAsMap(): Map[K, V] = self.withScope &#123;</span><br><span class="line">   val data = self.collect()</span><br><span class="line">   val map = new mutable.HashMap[K, V]</span><br><span class="line">   map.sizeHint(data.length)</span><br><span class="line">   data.foreach &#123; pair =&gt; map.put(pair._1, pair._2) &#125;</span><br><span class="line">   map</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex.collect</span><br><span class="line">res12: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex().countByKey()</span><br><span class="line">res13: scala.collection.Map[Int,Long] = Map(5 -&gt; 1, 1 -&gt; 1, 2 -&gt; 1, 3 -&gt; 1, 4 -&gt; 1)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex().collectAsMap()</span><br><span class="line">res14: scala.collection.Map[Int,Long] = Map(2 -&gt; 1, 5 -&gt; 4, 4 -&gt; 3, 1 -&gt; 0, 3 -&gt; 2)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<p>Action算子官网：<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" target="_blank" rel="noopener">Action 算子</a></p>

      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/01/21/Spark002-transform-action/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            Spark002-transform&amp;action
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/15/Spark001-double-happy/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark001--double_happy</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/27/k8s-Spark-doublehappy/">k8s-Spark-doublehappy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Kudu-Impala%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B01-double-happy/">Kudu+Impala故障案例01--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink04-double-happy/">Flink04--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink03-double-happy/">Flink03--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink02-double-happy/">Flink02--double_happy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>