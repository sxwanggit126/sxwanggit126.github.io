<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark11-shuffle | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="123456shuffle：	算子：groupByKey、reduceByKey、countByKey、join(不是所有的join)	遇到宽依赖就会有shuffle的产生	遇到shuffle 就会切出新的stage数据倾斜：相同key的数据会分发到同一个task中去执行  12345678找源码 要到 SparkEnv里面找Spark发展到现在经过几种类型的shufle1.  hash ： Ha">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark11-shuffle">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;28&#x2F;Spark11-shuffle&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="123456shuffle：	算子：groupByKey、reduceByKey、countByKey、join(不是所有的join)	遇到宽依赖就会有shuffle的产生	遇到shuffle 就会切出新的stage数据倾斜：相同key的数据会分发到同一个task中去执行  12345678找源码 要到 SparkEnv里面找Spark发展到现在经过几种类型的shufle1.  hash ： Ha">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025204517264.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025210825723.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191026114018994.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191026115743915.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:06:36.454Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025204517264.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-Spark11-shuffle" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      Spark11-shuffle
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/28/Spark11-shuffle/" class="article-date">
  <time datetime="2018-01-28T12:06:12.000Z" itemprop="datePublished">2018-01-28</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shuffle：</span><br><span class="line">	算子：groupByKey、reduceByKey、countByKey、join(不是所有的join)</span><br><span class="line">	遇到宽依赖就会有shuffle的产生</span><br><span class="line">	遇到shuffle 就会切出新的stage</span><br><span class="line"></span><br><span class="line">数据倾斜：相同key的数据会分发到同一个task中去执行</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">找源码 要到 SparkEnv里面找</span><br><span class="line">Spark发展到现在经过几种类型的shufle</span><br><span class="line">1.  hash ： HashShuffleManager    现在源码里已经没有了 这个shuffle 存在一些弊端 所以不用了   1.2版本之前 默认是它</span><br><span class="line">2.   sort ： SortShuffleManager   &gt;1.2版本 底层默认是它</span><br><span class="line">3.tungsten-sort ：  SortShuffleManager   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">调优：代码、资源、数据倾斜(Skew)、Shuffle(这个影响面 不是太大的 )</span><br></pre></td></tr></table></figure></div>

<p><strong>HashShuffleManager</strong><br>为了看源码 idea里pom使用老点的版本</p>
<p><img src="https://img-blog.csdnimg.cn/20191025204517264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">HashShuffleManager    ：</span><br><span class="line">    每一个maptask 要为每个reduceTask 创建“东西”</span><br><span class="line">	东西：内存(bucket) + 落地的文件file(就是 bucket内存满了 会文件落地)</span><br><span class="line"></span><br><span class="line">产生的问题：</span><br><span class="line">	1.产生过多的file   maptask数 * reducetask数</span><br><span class="line">	    一般情况下 maptask 和reducetask数 都是比较大的 所以中间产生的磁盘文件数量惊人</span><br><span class="line"></span><br><span class="line">	那么bucket产生多少个呢？</span><br><span class="line">	   bucket用完是可以释放的</span><br><span class="line">	2.耗费过多的内存空间</span><br><span class="line">		每个maptask需要开启 reduce个数的 bucket   会开启 M*R 的bucket数量 虽然是可以释放</span><br><span class="line">		但是同一时间点 同时存在的bucket的数量  reducetask个数 * core的数量 </span><br><span class="line">	3.bucket多大？</span><br><span class="line">		官网 </span><br><span class="line">		spark.shuffle.file.buffer  默认32k</span><br><span class="line"></span><br><span class="line">HashShuffleManager    优化：</span><br><span class="line">优化后：</span><br><span class="line">一个core上连续执行的shufflemaptask可以共用一个输出文件</span><br><span class="line"></span><br><span class="line">core第一次的生成的文件 会形成 reduce个数 的shuffleFile文件 </span><br><span class="line">core第二次 就是运行下一个shufflemaptask 的时候 生成的文件会追加到第一次生成的shuffleBlock文件</span><br><span class="line"></span><br><span class="line">文件数 ： core数 *reduce task数</span><br><span class="line">优化后的文件数减少了好多 </span><br><span class="line">怎么开启这个优化呢？</span><br><span class="line">配置一个参数即可 spark.shuffle.consolidateFiles 为true</span><br><span class="line"></span><br><span class="line">这个shuffle 了解和面试即可 不作为重点</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025210825723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.自己梳理下SortShuffleManner</span><br><span class="line">2.为什么说调优的时候 Shuffle不作为重点 因为 新版底层就两个 sort(默认) 和 tungsten-sort</span><br><span class="line">3.hash 、 sort 要观察shuffle产生的文件数到底是多少个？ 使用spark-shell就可以测试 eg ：wc为例  设置maptask的数量  和reducetask的数量</span><br><span class="line">( textFile算子可以设置 maptask数量   reduceByKey可以设置 reducetask数量)</span><br><span class="line">shuffle的数据是落地的 落地到哪里呢？spark.local.dir 参数 控制的落到那个文件夹</span><br><span class="line">  测试：</span><br><span class="line">  	1.普通的HashShuffleMannager</span><br><span class="line">  	2.consolidateFiles的HashShuffleManager</span><br><span class="line">  	3.sort</span><br><span class="line">  	shuffle生成的文件数</span><br><span class="line">  	参数的设置会吧 ！！！</span><br></pre></td></tr></table></figure></div>

<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>你首先要知道spark哪些地方需要优化？<br><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Tuning Spark</a></p>
<p>Because of the in-memory nature of most Spark computations, Spark programs can be <strong>bottlenecked</strong> by any resource in the cluster: CPU, network bandwidth, or memory. Most often, <strong>if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you also need to do some tuning, such as storing RDDs in serialized form, to decrease memory usage.</strong> This guide will cover two main topics: <strong>data serialization</strong>, which is crucial for good network performance and can also reduce memory use, and <strong>memory tuning.</strong> We also sketch several smaller topics.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bottlenecked ：瓶颈</span><br><span class="line">1.CPU, network bandwidth, or memory</span><br><span class="line"></span><br><span class="line">也就是说你公司集群资源不够就别搞spark了 --&gt;cpu+mem</span><br><span class="line">带宽 现在一般都是万兆的 千兆的就有点扯</span><br><span class="line"></span><br><span class="line">官方从两个方面：</span><br><span class="line">1.data serialization</span><br><span class="line">2.memory tuning</span><br><span class="line"></span><br><span class="line">1.数据序列化  可以内存的使用 就是数据体积变小了 占的内存少</span><br><span class="line">2.内存调优</span><br></pre></td></tr></table></figure></div>
<p><strong>Data Serialization</strong><br>看官网 写的很清楚<br>Serialization plays an important role in the performance of any distributed application.<br>因为算子里用到的东西 都是要经过序列化才可以<br><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Tuning Spark</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Java serialization: </span><br><span class="line">	Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.</span><br><span class="line">Kryo serialization: </span><br><span class="line">	Spark can also use the Kryo library (version 4) to serialize objects more quickly. </span><br><span class="line">	Kryo is significantly faster and more compact than Java serialization (often as much as 10x),</span><br><span class="line">	 but does not support all Serializable types and requires you to register the classes</span><br><span class="line">	  you’ll use in the program in advance for best performance.</span><br><span class="line"></span><br><span class="line">java：slow、large</span><br><span class="line">Kryo：</span><br><span class="line">		1. requires you to register the classes</span><br><span class="line">			quickly、compact </span><br><span class="line">		2.not register the classes</span><br><span class="line">			也能运行 性能刚好相反</span><br><span class="line"></span><br><span class="line">使用  </span><br><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) 来更换序列化的方式  因为默认是java的 </span><br><span class="line">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br><span class="line">把你自己定义的类 MyClass1 有几个就写进去几个  </span><br><span class="line"></span><br><span class="line">1.代码里是这样写的 但是建议代码里别写死 最好别写 我说的是 序列化的方式 而不是注册</span><br><span class="line">2,。最好  配置在 spark-default.conf 下面  ***</span><br><span class="line">或者</span><br><span class="line">3. spark-submit --conf k=v 也可以  ***</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"> spark.master                     local[2]</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"> spark.eventLog.dir               hdfs://hadoop101:8020/spark_directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br><span class="line">[double_happy@hadoop101 conf]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">测试 通过cache 看看页面  data 大小就ok了 </span><br><span class="line"> 看看ui  cache的data大小：</span><br><span class="line">1.data=&gt;rdd==&gt;rdd.cache   ==&gt;action     这种 默认  cache 存储级别是 内存</span><br><span class="line">2.data=&gt;rdd==&gt;rdd.persisit(mem_only_ser)==&gt;action  默认java序列化</span><br><span class="line">3.data=&gt;rdd==&gt;rdd.persisit(mem_only_ser)==&gt;action   把这个参数 spark.serializer  打开 设置 kryo   这种就是没有注册</span><br><span class="line">4.data=&gt;rdd==&gt;rdd.persisit(mem_only_ser)==&gt;action  把这个参数 spark.serializer设置 kryo  +register  注册</span><br></pre></td></tr></table></figure></div>
<p><strong>Determining Memory Consumption</strong><br>你怎么知道一个数据集或者对象的内存占用了多少了呢？</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">两种方法：</span><br><span class="line">1.The best way to size the amount of memory consumption a dataset will require is to create an RDD, </span><br><span class="line">	put it into cache, and look at the    “Storage” page in the web UI. </span><br><span class="line">	The page will tell you how much memory the RDD is occupying.</span><br><span class="line"></span><br><span class="line">2.To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method. </span><br><span class="line"></span><br><span class="line">第二种就是使用 SizeEstimator类</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">-r-xr-xr-x 1 double_happy double_happy  14M Sep 21 12:33 ip.txt</span><br><span class="line">scala&gt; import org.apache.spark.util.SizeEstimator</span><br><span class="line">import org.apache.spark.util.SizeEstimator</span><br><span class="line"></span><br><span class="line">scala&gt;  SizeEstimator.estimate(&quot;file:///home/double_happy/data/ip.txt&quot;)</span><br><span class="line">res0: Long = 120</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">我的文件是14m  预估占用内存是 120m   </span><br><span class="line"></span><br><span class="line">测试第一种方法</span><br><span class="line">scala&gt; import org.apache.spark.util.SizeEstimator</span><br><span class="line">import org.apache.spark.util.SizeEstimator</span><br><span class="line"></span><br><span class="line">scala&gt;  SizeEstimator.estimate(&quot;file:///home/double_happy/data/ip.txt&quot;)</span><br><span class="line">res0: Long = 120</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd = sc.textFile(&quot;file:///home/double_happy/data/ip.txt&quot;)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[String] = file:///home/double_happy/data/ip.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cache</span><br><span class="line">res1: rdd.type = file:///home/double_happy/data/ip.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">[Stage 0:&gt;                                                          (0 + 2) / 2[Stage 0:=============================&gt;                             (1 + 1) / 2                                                                               res2: Long = 115395</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191026114018994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为什么第二种方法多这么多？</span><br><span class="line">页面上的肯定是膨胀的变大是必然的  为什么 第二种多这么多？可以去看源码 查查资料</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">If you have less than 32 GB of RAM, set the JVM flag -XX:+UseCompressedOops to make pointers be four bytes instead of eight. You can add these options in spark-env.sh.</span><br><span class="line"></span><br><span class="line">如果你的内存小于32G 设置JVM  -XX:+UseCompressedOops 会好一些</span><br></pre></td></tr></table></figure></div>

<p><strong>Garbage Collection Tuning</strong><br>需要先了解内存管理 和 jvm 才能 gc调优</p>
<p><a href="http://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning" target="_blank" rel="noopener">Advanced GC Tuning</a></p>
<p>gc我没有复习 所以这块之后补上。</p>
<p><strong>Data Locality</strong><br> 数据本地化 了解即可 很难实现<br>   <strong>If data and the code that operates on it are together then computation tends to be fast.</strong><br>   <img src="https://img-blog.csdnimg.cn/20191026115743915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>But if code and data are separated, <strong>one must move to the other.</strong> Typically it is faster to ship serialized code from place to place than a chunk of data because code size is much smaller than data. Spark builds its scheduling around this general principle of data locality.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">one must move to the other 就是这两个总有一个要移动的</span><br><span class="line">1.代码是 driver端发过去的 ， 首先代码移动是没有难度的 ，但是</span><br><span class="line">你的作业申请到资源后 就把作业定到申请的executor上去了 所以 正常情况下 是数据的移动  不存在代码的移动</span><br></pre></td></tr></table></figure></div>
<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from <strong>closest to farthest</strong>:</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">levels of locality：</span><br><span class="line">	PROCESS_LOCAL</span><br><span class="line">	NODE_LOCAL </span><br><span class="line">	NO_PREF </span><br><span class="line">	RACK_LOCAL </span><br><span class="line">	ANY </span><br><span class="line"></span><br><span class="line">这个等级spark默认从近--&gt;远 选取 知道可以运行   毕竟越近越好嘛 生产上 达到NO_PREF 就挺好的 </span><br><span class="line"></span><br><span class="line">可以把这个理解成 妹子找对象  富二代 --&gt;高富帅--&gt;xxx---&gt;屌丝---&gt;男的     最后30岁了 依旧单身</span><br></pre></td></tr></table></figure></div>
<p><strong>总结shuffle参数：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1.spark.shuffle.consolidateFiles     hashshuffle优化开启参数</span><br><span class="line">2.spark.shuffle.file.buffer.kb       提高buffer 默认32k 可以调64 再128   相当于map端的</span><br><span class="line"></span><br><span class="line">reduce端的也有  实际上 reduce 是 拉去file 到一个缓存区的 </span><br><span class="line">3.spark.reducer.maxMbInFlight     reduce的缓冲区大小 默认48m 稍微调大一点 96m</span><br><span class="line">既然有map端的和reduce端的 那么中间 应该也有的</span><br><span class="line"></span><br><span class="line">4.spark.shuffle.io.maxRetries   最大次数是默认是3 就是shuffle 或者reduce端失败了 中间这块要去map端再去做一遍</span><br><span class="line">5.spark.shuffle.io.retryWait   这是上面 要等待多久去retry一次呢 默认5s</span><br><span class="line">6.spark.shuffle.sort.bypassMergeThreshold  这是 sort shuffle 阈值是200 合并数据 就是sort需要排序的 但是在有些场景不需要排序 由这参数控制的</span><br><span class="line">7.spark.shuffle.spill.compress   shuffle的时候需不需压缩</span><br><span class="line">8.spark.io.compression.codec  需要压缩 你得配置一个 codec吧 </span><br><span class="line">9.spark.shuffle.compress 和 7 有什么区别呀 ？map output files 和 compress data spilled during shuffles的区别</span><br><span class="line">10.spark.shuffle.manager  官网上没有 源码里的 默认走的是sortshuffle </span><br><span class="line"></span><br><span class="line">这些参数就是 shuffle过程中我们要用到的调优参数</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/02/01/SparkSQL01/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            SparkSQL01
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/26/Spark10-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark10--内存管理</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>