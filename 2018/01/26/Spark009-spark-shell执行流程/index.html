<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark009--spark-shell执行流程 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="spark-shell脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark009--spark-shell执行流程">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;26&#x2F;Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="spark-shell脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025135328497.png">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025145541791.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025152410447.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:05:08.335Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191025135328497.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-Spark009-spark-shell执行流程" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      Spark009--spark-shell执行流程
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/26/Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/" class="article-date">
  <time datetime="2018-01-26T12:04:44.000Z" itemprop="datePublished">2018-01-26</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="spark-shell脚本"><a href="#spark-shell脚本" class="headerlink" title="spark-shell脚本"></a>spark-shell脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-shell </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"># Shell script for starting the Spark Shell REPL</span><br><span class="line"></span><br><span class="line">cygwin=false     </span><br><span class="line">case &quot;$(uname)&quot; in     </span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"># Enter posix mode for bash</span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE=&quot;Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation&quot;</span><br><span class="line"></span><br><span class="line"># SPARK-4161: scala does not assume use of the java classpath,</span><br><span class="line"># so we need to add the &quot;-Dscala.usejavacp=true&quot; flag manually. We</span><br><span class="line"># do this specifically for the Spark shell because the scala REPL</span><br><span class="line"># has its own class loader, and any additional classpath specified</span><br><span class="line"># through spark.driver.extraClassPath is not automatically propagated.</span><br><span class="line">SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # Workaround for issue involving JLine and Cygwin</span><br><span class="line">    # (see http://sourceforge.net/p/jline/bugs/40/).</span><br><span class="line">    # If you&apos;re using the Mintty terminal emulator in Cygwin, may need to set the</span><br><span class="line">    # &quot;Backspace sends ^H&quot; setting in &quot;Keys&quot; section of the Mintty options</span><br><span class="line">    # (see https://github.com/sbt/sbt/issues/562).</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot;</span><br><span class="line">    &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in</span><br><span class="line"># binary distribution of Spark where Scala is not installed</span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=&quot;&quot;</span><br><span class="line"></span><br><span class="line"># restore stty settings (echo in particular)</span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=&quot;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ &quot;$saved_stty&quot; != &quot;&quot; ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># to reenable echo if we are interrupted before completing.</span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"># save terminal settings</span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"># clear on error so we don&apos;t later try to restore them</span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line"></span><br><span class="line"># record the exit status lest it be overwritten:</span><br><span class="line"># then reenable echo and propagate the code.</span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.  cygwin=false  //windows 电脑操作linux东西 要安装cygwin  </span><br><span class="line">2.  case &quot;$(uname)&quot; in       //uname 知道我们是什么操作系统</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025135328497.png" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ uname -r        //内核的版本</span><br><span class="line">3.10.0-514.26.2.el7.x86_64</span><br><span class="line">[double_happy@hadoop101 bin]$ uname -a      //打印所有的信息</span><br><span class="line">Linux hadoop101 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>
<p><strong>case in</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">      模式1)</span><br><span class="line">      		command</span><br><span class="line">      ;;</span><br><span class="line">      模式2)</span><br><span class="line">      		command</span><br><span class="line">      ;;</span><br><span class="line">      *)</span><br><span class="line">      	command</span><br><span class="line">      	;;</span><br><span class="line">      	esac</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat test.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">cygwin=false     </span><br><span class="line">case &quot;$(uname)&quot; in     </span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line">echo $cygwin</span><br><span class="line">[double_happy@hadoop101 script]$ sh test.sh </span><br><span class="line">false</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat case.sh </span><br><span class="line">read -p &quot;press key , then press return:&quot; KEY</span><br><span class="line">case $KEY in</span><br><span class="line">        [a-z]|[A-Z])</span><br><span class="line">echo &quot;this is a letter..&quot;;;</span><br><span class="line">        [0-9])</span><br><span class="line">echo &quot;this is a digit...&quot; ;;</span><br><span class="line">        *)</span><br><span class="line">echo &quot;other..&quot; ;;</span><br><span class="line">esac</span><br><span class="line">[double_happy@hadoop101 script]$ sh case.sh </span><br><span class="line">press key , then press return:1</span><br><span class="line">this is a digit...</span><br><span class="line">[double_happy@hadoop101 script]$ sh case.sh </span><br><span class="line">press key , then press return:a</span><br><span class="line">this is a letter..</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>
<p><strong>if -z</strong><br><a href="https://www.cnblogs.com/new-journey/p/11017659.html" target="_blank" rel="noopener">shell 参数</a></p>
<p>if [ -z “${SPARK_HOME}” ]; then<br>  source “$(dirname “$0”)”/find-spark-home<br>fi</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[ -z STRING ]  “STRING” 的长度为零则为真。  </span><br><span class="line">dirname 是什么？</span><br><span class="line">获取当前的路径</span><br><span class="line"></span><br><span class="line">find-spark-home是在bin目录下  这脚本里有 就是去export SPARK_HOME</span><br><span class="line"> source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home    目的就是找 SPARK_HOME</span><br><span class="line"> 所以你环境里配置了 SPARK_HOME if那个shell 就直接跳过去 就不用找SPARK_HOME</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat dirname.sh </span><br><span class="line">HOME=`cd $(dirname &quot;$0&quot;);pwd`</span><br><span class="line">echo $HOME</span><br><span class="line">[double_happy@hadoop101 script]$ sh dirname.sh </span><br><span class="line">/home/double_happy/script</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>

<p><strong>main</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">main &quot;$@&quot;  main方法里 ：</span><br><span class="line"></span><br><span class="line"> &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line"></span><br><span class="line">知道 spark-shell底层调用的是  spark-submit</span><br></pre></td></tr></table></figure></div>
<p><strong>$@</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat main.sh </span><br><span class="line">function main()&#123;</span><br><span class="line">        echo &quot;.....&quot;$@</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 script]$ sh main.sh abc </span><br><span class="line">.....abc</span><br><span class="line">[double_happy@hadoop101 script]$ sh main.sh abc 123 456</span><br><span class="line">.....abc 123 456</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line">$@ :就是把一堆输入的参数  带走</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat main.sh </span><br><span class="line">function main()&#123;</span><br><span class="line">        echo &quot;.....&quot;$@</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 script]$ mv main.sh spark-shell</span><br><span class="line">[double_happy@hadoop101 script]$ chmod +x spark-shell </span><br><span class="line">[double_happy@hadoop101 script]$ ./spark-shell --master yarn --jars mysql.driver.jar</span><br><span class="line">.....--master yarn --jars mysql.driver.jar</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">明白了吗 spark-shell的参数 就是这么传进去的</span><br></pre></td></tr></table></figure></div>
<h2 id="spark-submit脚本"><a href="#spark-submit脚本" class="headerlink" title="spark-submit脚本"></a>spark-submit脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-submit </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># disable randomized hash for string in Python 3.3+</span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br><span class="line"></span><br><span class="line">exec 是做什么的？</span><br><span class="line">就是一个执行的命令</span><br><span class="line">例如在当前shell中执行 exec ls  表示执行ls这条命令来替换当前的shell ，即为执行完后会退出当前shell。</span><br><span class="line"></span><br><span class="line">为了避免这个结果的影响，一般将exec命令放到一个shell脚本中，用主脚本调用这个脚本，调用处可以用bash  xx.sh(xx.sh为存放exec命令的脚本)，这样会为xx.sh建立一个子shell去执行，当执行exec后该子脚本进程就被替换成相应的exec的命令。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025145541791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>直接退出了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat exec.sh </span><br><span class="line">exec ls</span><br><span class="line">[double_happy@hadoop101 script]$ sh exec.sh </span><br><span class="line">azkaban-job  case.sh  dirname.sh  exec.sh  flume-agent  spark-shell  test.sh</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line">明白了吗？把exec 封装在一个shell 里 去调用别的脚本</span><br></pre></td></tr></table></figure></div>
<h2 id="spark-class脚本"><a href="#spark-class脚本" class="headerlink" title="spark-class脚本"></a>spark-class脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-class </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;&quot;/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"># Find the java binary</span><br><span class="line">if [ -n &quot;$&#123;JAVA_HOME&#125;&quot; ]; then</span><br><span class="line">  RUNNER=&quot;$&#123;JAVA_HOME&#125;/bin/java&quot;</span><br><span class="line">else</span><br><span class="line">  if [ &quot;$(command -v java)&quot; ]; then</span><br><span class="line">    RUNNER=&quot;java&quot;</span><br><span class="line">  else</span><br><span class="line">    echo &quot;JAVA_HOME is not set&quot; &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Find Spark jars.</span><br><span class="line">if [ -d &quot;$&#123;SPARK_HOME&#125;/jars&quot; ]; then</span><br><span class="line">  SPARK_JARS_DIR=&quot;$&#123;SPARK_HOME&#125;/jars&quot;</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR=&quot;$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d &quot;$SPARK_JARS_DIR&quot; ] &amp;&amp; [ -z &quot;$SPARK_TESTING$SPARK_SQL_TESTING&quot; ]; then</span><br><span class="line">  echo &quot;Failed to find Spark jars directory ($SPARK_JARS_DIR).&quot; 1&gt;&amp;2</span><br><span class="line">  echo &quot;You need to build Spark with the target \&quot;package\&quot; before running this program.&quot; 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH=&quot;$SPARK_JARS_DIR/*&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Add the launcher build dir to the classpath if requested.</span><br><span class="line">if [ -n &quot;$SPARK_PREPEND_CLASSES&quot; ]; then</span><br><span class="line">  LAUNCH_CLASSPATH=&quot;$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># For tests</span><br><span class="line">if [[ -n &quot;$SPARK_TESTING&quot; ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># The launcher library will print arguments separated by a NULL character, to allow arguments with</span><br><span class="line"># characters that would be otherwise interpreted by the shell. Read that in a while loop, populating</span><br><span class="line"># an array that will be used to exec the final command.</span><br><span class="line">#</span><br><span class="line"># The exit code of the launcher is appended to the output, so the parent shell removes it from the</span><br><span class="line"># command array and checks the value to see if the launcher succeeded.</span><br><span class="line">build_command() &#123;</span><br><span class="line">  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">  printf &quot;%d\0&quot; $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Turn off posix mode since it does not allow process substitution</span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d &apos;&apos; -r ARG; do</span><br><span class="line">  CMD+=(&quot;$ARG&quot;)</span><br><span class="line">done &lt; &lt;(build_command &quot;$@&quot;)</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"># Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes</span><br><span class="line"># the code that parses the output of the launcher to get confused. In those cases, check if the</span><br><span class="line"># exit code is an integer, and if it&apos;s not, handle it as a special error case.</span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo &quot;$&#123;CMD[@]&#125;&quot; | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=(&quot;$&#123;CMD[@]:0:$LAST&#125;&quot;)</span><br><span class="line">exec &quot;$&#123;CMD[@]&#125;&quot;</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">. &quot;$&#123;SPARK_HOME&#125;&quot;/bin/load-spark-env.sh  这是在做什么？</span><br><span class="line">就是把环境里的scala版本找到</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">build_command() &#123;</span><br><span class="line">  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">  printf &quot;%d\0&quot; $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RUNNER=java 去看一下就知道</span><br></pre></td></tr></table></figure></div>
<p>整个spark-shell流程：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">spark-shell &#123;</span><br><span class="line"></span><br><span class="line">&quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.repl.Main \</span><br><span class="line"> --name &quot;Spark shell&quot; \</span><br><span class="line"> &quot;$@&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">==&gt; spark-submit&#123;</span><br><span class="line">	exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class \</span><br><span class="line">	 org.apache.spark.deploy.SparkSubmit \</span><br><span class="line">	 &quot;$@&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">==&gt;spark-class&#123;</span><br><span class="line">	build_command() &#123;</span><br><span class="line">	  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">	  printf &quot;%d\0&quot; $?</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell最底层就是使用Jave来启动 org.apache.spark.launcher.Main类</span><br><span class="line">REPL  ==》交互式解释器</span><br><span class="line"></span><br><span class="line">看org.apache.spark.launcher.Main 源码 需要这个依赖</span><br><span class="line"></span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-launcher_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line"> &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">idea ：</span><br><span class="line">control + shift +n </span><br><span class="line">control + shift +f</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025152410447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/01/26/Spark10-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            Spark10--内存管理
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/25/Spark008-%E8%A1%A5%E5%85%85Spark007/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark008--补充Spark007</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://sxwanggit126.github.io/" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>