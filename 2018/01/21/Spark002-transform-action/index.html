<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark002-transform&amp;action | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="RDD操作转换转换操作不会立即执行的，不触发作业的执行  123RDD操作    transformation  转换    它不会立即执行  你写了1亿个转换  白写   lazy    action          动作    只有遇到action才会提交作业开始执行      eager  官网RDD算子介绍：RDD Operations：RDDs support two types of">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark002-transform&amp;action">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;21&#x2F;Spark002-transform-action&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="RDD操作转换转换操作不会立即执行的，不触发作业的执行  123RDD操作    transformation  转换    它不会立即执行  你写了1亿个转换  白写   lazy    action          动作    只有遇到action才会提交作业开始执行      eager  官网RDD算子介绍：RDD Operations：RDDs support two types of">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019101211271935.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012113652954.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012114402942.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019101211493594.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012115049522.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012115140493.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012115918554.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_30,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019101212020396.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019101212080774.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012120846720.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_50,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012121548376.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019101212132486.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012121907803.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012122329639.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_60,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012123922980.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191012124445377.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T11:59:35.659Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019101211271935.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-Spark002-transform-action" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      Spark002-transform&amp;action
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/21/Spark002-transform-action/" class="article-date">
  <time datetime="2018-01-21T11:58:56.000Z" itemprop="datePublished">2018-01-21</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>转换操作不会立即执行的，不触发作业的执行 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RDD操作</span><br><span class="line">    transformation  转换    它不会立即执行  你写了1亿个转换  白写   lazy</span><br><span class="line">    action          动作    只有遇到action才会提交作业开始执行      eager</span><br></pre></td></tr></table></figure></div>

<h2 id="官网RDD算子介绍："><a href="#官网RDD算子介绍：" class="headerlink" title="官网RDD算子介绍："></a>官网RDD算子介绍：</h2><p>RDD Operations：<br>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).</p>
<p>All <strong>transformations</strong> in Spark are <strong>lazy</strong>, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. <strong>This design enables Spark to run more efficiently.</strong> For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>
<p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">This design enables Spark to run more efficiently：</span><br><span class="line">对比Hadoop举一个例子：</span><br><span class="line">	1+1+1+1 这个动作</span><br><span class="line">MR：1+1=2  --&gt;落地--&gt;读取 +1=3  --&gt;落地---&gt;读取+1=4</span><br><span class="line">Spark：1+1+1+1 这块全部用transformations 来完成 ，真正计算的时候，才一次性提交上去。一个流水先就全都执行完了。</span><br></pre></td></tr></table></figure></div>
<h2 id="Transformations讲解前的说明"><a href="#Transformations讲解前的说明" class="headerlink" title="Transformations讲解前的说明"></a>Transformations讲解前的说明</h2><p>先说明我们的程序里创建SparkContex的方式，由于每次创建都要写appname，master，以及RDD数据集在Driver端打印出来查看都要写foreach(println)，每次都要写很麻烦，这里我们给封装一下。<br>效果展示：<br>    <img src="https://img-blog.csdnimg.cn/2019101211271935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如果我不想输出 我输入一个1进去即可：<br><img src="https://img-blog.csdnimg.cn/20191012113652954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是自己写的哈 ，老师上课留的作业 人家要求动手能力。不会全部给你，让你做一个伸手党就没意义。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.homework.utils</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">object ContextUtils &#123;</span><br><span class="line">  /**</span><br><span class="line">    * 获取sc</span><br><span class="line">    */</span><br><span class="line">  def getSparkContext(appname:String,defalut:String = &quot;local[2]&quot;): SparkContext = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line">    new SparkContext(sparkConf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>下面这个主要使用隐式转换，看不懂可以查看Scala博客：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.homework.utils</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">object ImplicitAspect &#123;</span><br><span class="line">  implicit def rdd2RichRDD[T](rdd : RDD[T]) : RichRDD[T] = new RichRDD[T](rdd)</span><br><span class="line">&#125;</span><br><span class="line">class RichRDD[T](rdd : RDD[T])&#123;</span><br><span class="line"> def printInfo(num : Int =0): Unit =&#123;</span><br><span class="line">    num match &#123;</span><br><span class="line">      case 0 =&gt; rdd.foreach(println);println(&quot;-------------------------&quot;)</span><br><span class="line">      case _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>源码面前，了无秘密。通过源码进行学习</p>
<h2 id="（1）Map相关的算子"><a href="#（1）Map相关的算子" class="headerlink" title="（1）Map相关的算子"></a>（1）Map相关的算子</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1.makeRDD / parallelize </span><br><span class="line"></span><br><span class="line"> def makeRDD[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">  &#125;</span><br><span class="line">注意：makeRDD底层调用的就是parallelize </span><br><span class="line"></span><br><span class="line"> /** Distribute a local Scala collection to form an RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call</span><br><span class="line">   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the</span><br><span class="line">   * modified collection. Pass a copy of the argument to avoid this.</span><br><span class="line">   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an</span><br><span class="line">   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.</span><br><span class="line">   * @param seq Scala collection to distribute</span><br><span class="line">   * @param numSlices number of partitions to divide the collection into</span><br><span class="line">   * @return RDD representing distributed collection</span><br><span class="line">   */</span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2.map : 处理每一条数据</span><br><span class="line">/**</span><br><span class="line">   * Return a new RDD by applying a function to all elements of this RDD.</span><br><span class="line">   */</span><br><span class="line">  def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012114402942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">RDD是有多个partition所构成的，</span><br><span class="line">3.mapPartitions: Return a new RDD by applying a function to each partition of this RDD. 对分区做处理的，一个分区里有很多的元素的。</span><br><span class="line"> /**</span><br><span class="line">   * Return a new RDD by applying a function to each partition of this RDD.</span><br><span class="line">   *</span><br><span class="line">   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span><br><span class="line">   * should be `false` unless this is a pair RDD and the input function doesn&apos;t modify the keys.</span><br><span class="line">   */</span><br><span class="line">  def mapPartitions[U: ClassTag](</span><br><span class="line">      f: Iterator[T] =&gt; Iterator[U],</span><br><span class="line">      preservesPartitioning: Boolean = false): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanedF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD(</span><br><span class="line">      this,</span><br><span class="line">      (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019101211493594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>一个是作用于每个元素，一个是作用于每个分区。<br><img src="https://img-blog.csdnimg.cn/20191012115049522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这是作用于每个分区里的每个元素<br><img src="https://img-blog.csdnimg.cn/20191012115140493.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结果和map是一样的。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line"> /**</span><br><span class="line">      * map 处理每一条数据</span><br><span class="line">      * mapPartitions 对每个分区进行处理</span><br><span class="line">      *</span><br><span class="line">      * map：100个元素  10个分区 ==&gt; 知识点：要把RDD的数据写入MySQL  Connection次数</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">4.mapPartitionsWithIndex</span><br><span class="line">你想看哪个分区里的东西 这个算子可以拿的到</span><br><span class="line">/**</span><br><span class="line">   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index</span><br><span class="line">   * of the original partition.</span><br><span class="line">   *</span><br><span class="line">   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span><br><span class="line">   * should be `false` unless this is a pair RDD and the input function doesn&apos;t modify the keys.</span><br><span class="line">   */</span><br><span class="line">  def mapPartitionsWithIndex[U: ClassTag](</span><br><span class="line">      f: (Int, Iterator[T]) =&gt; Iterator[U],</span><br><span class="line">      preservesPartitioning: Boolean = false): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanedF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD(</span><br><span class="line">      this,</span><br><span class="line">      (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(index, iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012115918554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_30,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2019101212020396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>元素为什么这么存放的呢？之后再来讲解。<br>生产上是不关注这个分区里的哪个元素的 只是用来学。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">5.mapValues</span><br><span class="line"> /**</span><br><span class="line">   * Pass each value in the key-value pair RDD through a map function without changing the keys;</span><br><span class="line">   * this also retains the original RDD&apos;s partitioning.</span><br><span class="line">   */</span><br><span class="line">  def mapValues[U](f: V =&gt; U): RDD[(K, U)] = self.withScope &#123;</span><br><span class="line">    val cleanF = self.context.clean(f)</span><br><span class="line">    new MapPartitionsRDD[(K, U), (K, V)](self,</span><br><span class="line">      (context, pid, iter) =&gt; iter.map &#123; case (k, v) =&gt; (k, cleanF(v)) &#125;,</span><br><span class="line">      preservesPartitioning = true)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019101212080774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191012120846720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_50,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">6.flatmap = map + flatten   就是打扁以后 做map</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   *  Return a new RDD by first applying a function to all elements of this</span><br><span class="line">   *  RDD, and then flattening the results.</span><br><span class="line">   */</span><br><span class="line">  def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TraversableOnce:  一次遍历的意思   flattening the results.也就是压扁</span><br></pre></td></tr></table></figure></div>
<p>对比map：<br><img src="https://img-blog.csdnimg.cn/20191012121548376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2019101212132486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>map对里面元素做处理，是不会改变 内部的结构的 。</p>
<p>flatMap:<br><img src="https://img-blog.csdnimg.cn/20191012121907803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结果：<br><img src="https://img-blog.csdnimg.cn/20191012122329639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_60,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">秀操作的：没什么用</span><br><span class="line">scala&gt; sc.parallelize(1 to 5).flatMap(1 to _).collect</span><br><span class="line">res3: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure></div>
<h2 id="（2）glom"><a href="#（2）glom" class="headerlink" title="（2）glom"></a>（2）glom</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">glom：把每一个分区里的数据 形成一个数组  比mapwithindex好用</span><br><span class="line"> /**</span><br><span class="line">   * Return an RDD created by coalescing all elements within each partition into an array.</span><br><span class="line">   */</span><br><span class="line">  def glom(): RDD[Array[T]] = withScope &#123;</span><br><span class="line">    new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) =&gt; Iterator(iter.toArray))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 30).glom().collect</span><br><span class="line">res4: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), Array(16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="3-sample"><a href="#3-sample" class="headerlink" title="(3)sample"></a>(3)sample</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sample:</span><br><span class="line">/**</span><br><span class="line">   * Return a sampled subset of this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span><br><span class="line">   * @param fraction expected size of the sample as a fraction of this RDD&apos;s size</span><br><span class="line">   *  without replacement: probability that each element is chosen; fraction must be [0, 1]</span><br><span class="line">   *  with replacement: expected number of times each element is chosen; fraction must be greater</span><br><span class="line">   *  than or equal to 0</span><br><span class="line">   * @param seed seed for the random number generator</span><br><span class="line">   *</span><br><span class="line">   * @note This is NOT guaranteed to provide exactly the fraction of the count</span><br><span class="line">   * of the given [[RDD]].</span><br><span class="line">   */</span><br><span class="line">  def sample(</span><br><span class="line">      withReplacement: Boolean,</span><br><span class="line">      fraction: Double,</span><br><span class="line">      seed: Long = Utils.random.nextLong): RDD[T] = &#123;</span><br><span class="line">    require(fraction &gt;= 0,</span><br><span class="line">      s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    withScope &#123;</span><br><span class="line">      require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction)</span><br><span class="line">      if (withReplacement) &#123;</span><br><span class="line">        new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012123922980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">	withReplacement：抽样的时候要不要放回去</span><br></pre></td></tr></table></figure></div>
<h2 id="（4）filter"><a href="#（4）filter" class="headerlink" title="（4）filter"></a>（4）filter</h2><p><img src="https://img-blog.csdnimg.cn/20191012124445377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 30).filter(_ &gt; 20).collect</span><br><span class="line">res5: Array[Int] = Array(21, 22, 23, 24, 25, 26, 27, 28, 29, 30)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(1 to 30).filter(x=&gt; x % 2 ==0 &amp;&amp; x &gt;10).collect</span><br><span class="line">res6: Array[Int] = Array(12, 14, 16, 18, 20, 22, 24, 26, 28, 30)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<h2 id="5-other类型的"><a href="#5-other类型的" class="headerlink" title="(5)other类型的"></a>(5)other类型的</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">union:  就是简单的合并  是不去重的哈 </span><br><span class="line">/**</span><br><span class="line">   * Return the union of this RDD and another one. Any identical elements will appear multiple</span><br><span class="line">   * times (use `.distinct()` to eliminate them).</span><br><span class="line">   */</span><br><span class="line">  def union(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    sc.union(this, other)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; a.union(b).collect</span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 4, 5, 6, 77, 7, 7)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">intersection:  交集</span><br><span class="line">  /**</span><br><span class="line">   * Return the intersection of this RDD and another one. The output will not contain any duplicate</span><br><span class="line">   * elements, even if the input RDDs did.</span><br><span class="line">   *</span><br><span class="line">   * @note This method performs a shuffle internally.</span><br><span class="line">   */</span><br><span class="line">  def intersection(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null)))</span><br><span class="line">        .filter &#123; case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;</span><br><span class="line">        .keys</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; a.intersection(b).collect</span><br><span class="line">res8: Array[Int] = Array(4, 6, 5)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">subtract:差集  出现在a里面的没有出现在b里面的 叫差集</span><br><span class="line">/**</span><br><span class="line">   * Return an RDD with the elements from `this` that are not in `other`.</span><br><span class="line">   *</span><br><span class="line">   * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting</span><br><span class="line">   * RDD will be &amp;lt;= us.</span><br><span class="line">   */</span><br><span class="line">  def subtract(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; a.subtract(b).collect</span><br><span class="line">res9: Array[Int] = Array(2, 1, 3)</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">去重：distinct</span><br><span class="line">	 /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(): RDD[T] = withScope &#123;</span><br><span class="line">    distinct(partitions.length)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; b.distinct.collect</span><br><span class="line">res10: Array[Int] = Array(4, 6, 77, 7, 5)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">去重这块 是可以传入分区参数的 ： 也可以没有的 没有的就是默认的分区</span><br><span class="line">你传进来多少分区就意味着 重新分区了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">scala&gt;    b.distinct(4).mapPartitionsWithIndex((index,partition)=&gt;&#123;</span><br><span class="line">     |       partition.map(x=&gt; s&quot;分区是$index, 元素是 $x&quot;)</span><br><span class="line">     |     &#125;).collect()</span><br><span class="line">res11: Array[String] = Array(分区是0, 元素是 4, 分区是1, 元素是 77, 分区是1, 元素是 5, 分区是2, 元素是 6, 分区是3, 元素是 7)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">四个分区： 那是怎么分区的呢？  元素%partitions</span><br><span class="line">即元素对分区个数取模 来分的</span><br><span class="line"></span><br><span class="line">分区是0, 元素是 4,    4%4=0</span><br><span class="line">分区是1, 元素是 77, </span><br><span class="line">分区是1, 元素是 5,  5%4 =1</span><br><span class="line">分区是2, 元素是 6,  6%4=2</span><br><span class="line">分区是3, 元素是 7  7%4 = 3</span><br><span class="line"></span><br><span class="line">明白了吧  通过这个例子 知道是怎么进行分区的。</span><br></pre></td></tr></table></figure></div>
<h2 id="KV类型的"><a href="#KV类型的" class="headerlink" title="KV类型的"></a>KV类型的</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">groupKeyKey：不怎么用的哈</span><br><span class="line">把key相同的分到一组</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Group the values for each key in the RDD into a single sequence. Hash-partitions the</span><br><span class="line">   * resulting RDD with the existing partitioner/parallelism level. The ordering of elements</span><br><span class="line">   * within each group is not guaranteed, and may even differ each time the resulting RDD is</span><br><span class="line">   * evaluated.</span><br><span class="line">   *</span><br><span class="line">   * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">   */</span><br><span class="line">  def groupByKey(): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Group the values for each key in the RDD into a single sequence. Allows controlling the</span><br><span class="line">   * partitioning of the resulting key-value pair RDD by passing a Partitioner.</span><br><span class="line">   * The ordering of elements within each group is not guaranteed, and may even differ</span><br><span class="line">   * each time the resulting RDD is evaluated.</span><br><span class="line">   *</span><br><span class="line">   * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">   *</span><br><span class="line">   * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span><br><span class="line">   * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`.</span><br><span class="line">   */</span><br><span class="line">  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    // groupByKey shouldn&apos;t use map side combine because map side combine does not</span><br><span class="line">    // reduce the amount of data shuffled and requires all map side data be inserted</span><br><span class="line">    // into a hash table, leading to more objects in the old gen.</span><br><span class="line">    val createCombiner = (v: V) =&gt; CompactBuffer(v)</span><br><span class="line">    val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v</span><br><span class="line">    val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2</span><br><span class="line">    val bufs = combineByKeyWithClassTag[CompactBuffer[V]](</span><br><span class="line">      createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span><br><span class="line">    bufs.asInstanceOf[RDD[(K, Iterable[V])]]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey()</span><br><span class="line">res12: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[35] at groupByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().collect</span><br><span class="line">res13: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(2)), (a,CompactBuffer(1, 99)), (c,CompactBuffer(3)))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>接着上面的值 求相同的key的和 是多少使用什么算子呢？ 上面讲过了哈</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().collect</span><br><span class="line">res13: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(2)), (a,CompactBuffer(1, 99)), (c,CompactBuffer(3)))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().mapValues(x=&gt;x.sum).collect</span><br><span class="line">res14: Array[(String, Int)] = Array((b,2), (a,100), (c,3))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey：</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).reduceByKey(_+_)</span><br><span class="line">res15: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at reduceByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).reduceByKey(_+_).collect</span><br><span class="line">res16: Array[(String, Int)] = Array((b,2), (a,100), (c,3))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="distinct-底层"><a href="#distinct-底层" class="headerlink" title="distinct 底层"></a>distinct 底层</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">  &#125;</span><br><span class="line">注意：</span><br><span class="line">distinct底层是使用 map+reduceByKey 的 是不是很简单</span><br><span class="line">reduceByKey就把两两相同的东西 丢到一块去</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      * distinct 去重</span><br><span class="line">      * 不允许使用distinct做去重</span><br><span class="line">      *</span><br><span class="line">      * x =&gt; (x,null)</span><br><span class="line">      *</span><br><span class="line">      * 8 =&gt; (8,null)</span><br><span class="line">      * 8 =&gt; (8,null)</span><br><span class="line">      */</span><br><span class="line">    val b = sc.parallelize(List(3,4,5,6,7,8,8))</span><br><span class="line">    b.map(x =&gt; (x,null)).reduceByKey((x,y) =&gt; x).map(_._1)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.rdd.RDD[(Int, Null)] :一定要看清reduceByKey的数据结构哈</span><br><span class="line"></span><br><span class="line">scala&gt; val r1 = sc.parallelize(List(1,1,12,3,3,4,6,6,6))</span><br><span class="line">r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; r1.map(x=&gt;(x,null)).reduceByKey((x,y)=&gt;x)</span><br><span class="line">res17: org.apache.spark.rdd.RDD[(Int, Null)] = ShuffledRDD[49] at reduceByKey at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">实际上到这一步 去重已经去掉了吧 明白吗   再把null去掉就可以了</span><br><span class="line"></span><br><span class="line">scala&gt; r1.map(x=&gt;(x,null)).reduceByKey((x,y)=&gt;x).map(_._1).collect</span><br><span class="line">res18: Array[Int] = Array(4, 6, 12, 1, 3)</span><br></pre></td></tr></table></figure></div>
<p>所以使用常用的算子一定要手点进去看看底层的实现哈 。</p>
<h2 id="个人理解-这个算子超重要"><a href="#个人理解-这个算子超重要" class="headerlink" title="个人理解 这个算子超重要"></a>个人理解 这个算子超重要</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">groupBy：自定义分组  分组条件就是自定义传进去的</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">  * Return an RDD of grouped items. Each group consists of a key and a sequence of elements</span><br><span class="line">  * mapping to that key. The ordering of elements within each group is not guaranteed, and</span><br><span class="line">  * may even differ each time the resulting RDD is evaluated.</span><br><span class="line">  *</span><br><span class="line">  * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">  * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">  * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">  */</span><br><span class="line"> def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)</span><br><span class="line">     : RDD[(K, Iterable[T])] = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   this.map(t =&gt; (cleanF(t), t)).groupByKey(p)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">参数是传入一个分组条件 怎么传呀？不要紧 可以测试</span><br><span class="line">1.传自己进去</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x)</span><br><span class="line">res19: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[55] at groupBy at &lt;console&gt;:25</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x).collect</span><br><span class="line">res20: Array[(String, Iterable[String])] = Array((b,CompactBuffer(b, b)), (a,CompactBuffer(a, a, a)), (c,CompactBuffer(c)))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">那我给一个需求：把上面的字母次数算出来</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x)</span><br><span class="line">res19: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[55] at groupBy at &lt;console&gt;:25</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x).mapValues(x=&gt;x.size).collect</span><br><span class="line">res21: Array[(String, Int)] = Array((b,2), (a,3), (c,1))</span><br><span class="line"></span><br><span class="line">注意：所以 算子都会用 怎么串起来 很重要的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sortBy：自定义排序   你想怎么排序就怎么排序  默认是升序的</span><br><span class="line">/**</span><br><span class="line">   * Return this RDD sorted by the given key function.</span><br><span class="line">   */</span><br><span class="line">  def sortBy[K](</span><br><span class="line">      f: (T) =&gt; K,</span><br><span class="line">      ascending: Boolean = true,</span><br><span class="line">      numPartitions: Int = this.partitions.length)</span><br><span class="line">      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope &#123;</span><br><span class="line">    this.keyBy[K](f)</span><br><span class="line">        .sortByKey(ascending, numPartitions)</span><br><span class="line">        .values</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2)</span><br><span class="line">res22: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[68] at sortBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2).collect</span><br><span class="line">res23: Array[(String, Int)] = Array((老哥,18), (double_happy,30), (娜娜,60))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2,false).collect</span><br><span class="line">res24: Array[(String, Int)] = Array((娜娜,60), (double_happy,30), (老哥,18))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(-_._2).collect</span><br><span class="line">res25: Array[(String, Int)] = Array((娜娜,60), (double_happy,30), (老哥,18))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sortBykey:是按key进行排序的哈 注意和sortby的区别   sortby是自定义排序 非常的灵活</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling</span><br><span class="line">   * `collect` or `save` on the resulting RDD will return or output an ordered list of records</span><br><span class="line">   * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in</span><br><span class="line">   * order of the keys).</span><br><span class="line">   */</span><br><span class="line">  // TODO: this currently doesn&apos;t work on P other than Tuple2!</span><br><span class="line">  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</span><br><span class="line">      : RDD[(K, V)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    val part = new RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">    new ShuffledRDD[K, V, V](self, part)</span><br><span class="line">      .setKeyOrdering(if (ascending) ordering else ordering.reverse)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortByKey().collect</span><br><span class="line">res29: Array[(String, Int)] = Array((double_happy,30), (娜娜,60), (老哥,18))</span><br><span class="line"></span><br><span class="line">如果要求 就是按年龄来排 应该怎么排序：</span><br><span class="line">  反转</span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).map(x=&gt;(x._2,x._1)).sortByKey().collect</span><br><span class="line">res30: Array[(Int, String)] = Array((18,老哥), (30,double_happy), (60,娜娜))</span><br><span class="line"></span><br><span class="line">数据是要和开始格式类似 再转回来就可以了：</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).map(x=&gt;(x._2,x._1)).sortByKey().map(x=&gt;(x._2,x._1)).collect</span><br><span class="line">res31: Array[(String, Int)] = Array((老哥,18), (double_happy,30), (娜娜,60))</span><br></pre></td></tr></table></figure></div>
<h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">join:  默认是内连接</span><br><span class="line">一定是需要条件的，条件就是key的</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each</span><br><span class="line">   * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and</span><br><span class="line">   * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD.</span><br><span class="line">   */</span><br><span class="line">  def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope &#123;</span><br><span class="line">    this.cogroup(other, partitioner).flatMapValues( pair =&gt;</span><br><span class="line">      for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">第一个：名字  第二个：城市  第三个：年龄  (B,(上海,18))</span><br><span class="line"></span><br><span class="line">scala&gt; val j1 = sc.parallelize(List((&quot;A&quot;,&quot;北京&quot;),(&quot;B&quot;,&quot;上海&quot;),(&quot;C&quot;,&quot;杭州&quot;)))</span><br><span class="line">j1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[106] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val j2 = sc.parallelize(List((&quot;A&quot;,&quot;30&quot;),(&quot;B&quot;,&quot;18&quot;),(&quot;D&quot;,&quot;60&quot;)))</span><br><span class="line">j2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[107] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;   j1.join(j2).collect</span><br><span class="line">res32: Array[(String, (String, String))] = Array((B,(上海,18)), (A,(北京,30)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.leftOuterJoin(j2).collect</span><br><span class="line">res33: Array[(String, (String, Option[String]))] = Array((B,(上海,Some(18))), (A,(北京,Some(30))), (C,(杭州,None)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.rightOuterJoin(j2).collect</span><br><span class="line">res34: Array[(String, (Option[String], String))] = Array((B,(Some(上海),18)), (D,(None,60)), (A,(Some(北京),30)))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      * join底层就是使用了cogroup</span><br><span class="line">      * RDD[K,V]</span><br><span class="line">      *</span><br><span class="line">      * 根据key进行关联，返回两边RDD的记录，没关联上的是空</span><br><span class="line">      * join返回值类型  RDD[(K, (Option[V], Option[W]))]      这块参数的类型是option要注意 </span><br><span class="line">      * cogroup返回值类型  RDD[(K, (Iterable[V], Iterable[W]))]</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cogroup:</span><br><span class="line"> /**</span><br><span class="line">   * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the</span><br><span class="line">   * list of values for that key in `this` as well as `other`.</span><br><span class="line">   */</span><br><span class="line">  def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner)</span><br><span class="line">      : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope &#123;</span><br><span class="line">    if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) &#123;</span><br><span class="line">      throw new SparkException(&quot;HashPartitioner cannot partition array keys.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val cg = new CoGroupedRDD[K](Seq(self, other), partitioner)</span><br><span class="line">    cg.mapValues &#123; case Array(vs, w1s) =&gt;</span><br><span class="line">      (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]])</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     j1.fullOuterJoin(j2).collect</span><br><span class="line">res35: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(上海),Some(18))), (D,(None,Some(60))), (A,(Some(北京),Some(30))), (C,(Some(杭州),None)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.cogroup(j2).collect</span><br><span class="line">res36: Array[(String, (Iterable[String], Iterable[String]))] = Array((B,(CompactBuffer(上海),CompactBuffer(18))), (D,(CompactBuffer(),CompactBuffer(60))), (A,(CompactBuffer(北京),CompactBuffer(30))), (C,(CompactBuffer(杭州),CompactBuffer())))</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/01/22/Spark004-%E6%80%BB%E7%BB%93%E5%89%8D%E9%9D%A2%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            Spark004--总结前面的基本操作
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/19/Spark003-Action-%E6%8E%A5%E7%9D%80Spark002/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark003--Action  接着Spark002</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://sxwanggit126.github.io/" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>