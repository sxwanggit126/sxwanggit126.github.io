<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Spark06--依赖关系 | DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta name="description" content="依赖关系12345678910111213rdd ==&amp;gt; transformation s ==&amp;gt; action就是rdd经过一系列的转换 最后触发actioneg：textFile(path) ==&amp;gt; map ==&amp;gt; filter ==&amp;gt; ...  ==&amp;gt; collect每一步转换都会形成一个rdd RDDA   RDDB   RDDCeg：一个rdd 三个分">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark06--依赖关系">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;24&#x2F;Spark06-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:description" content="依赖关系12345678910111213rdd ==&amp;gt; transformation s ==&amp;gt; action就是rdd经过一系列的转换 最后触发actioneg：textFile(path) ==&amp;gt; map ==&amp;gt; filter ==&amp;gt; ...  ==&amp;gt; collect每一步转换都会形成一个rdd RDDA   RDDB   RDDCeg：一个rdd 三个分">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023093733887.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023095648257.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023101103284.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023101642501.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023101727428.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102310260984.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023111033655.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023105631974.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023105652721.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023105850276.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023110813166.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023111336288.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023114118106.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023145325933.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023145545411.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;2019102314564550.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023145840158.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023150726195.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023151458914.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023152323222.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023154639524.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
<meta property="og:updated_time" content="2019-11-17T12:02:42.130Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;img-blog.csdnimg.cn&#x2F;20191023093733887.png?x-oss-process=image&#x2F;watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main"><article id="post-Spark06-依赖关系" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      Spark06--依赖关系
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/24/Spark06-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/" class="article-date">
  <time datetime="2018-01-24T12:02:17.000Z" itemprop="datePublished">2018-01-24</time>
</a>
    
    
  </div>
  
    <span id="busuanzi_container_page_pv">
      本文总阅读量<span id="busuanzi_value_page_pv"></span>次
    </span>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rdd ==&gt; transformation s ==&gt; action</span><br><span class="line">就是rdd经过一系列的转换 最后触发action</span><br><span class="line">eg：</span><br><span class="line">textFile(path) ==&gt; map ==&gt; filter ==&gt; ...  ==&gt; collect</span><br><span class="line">每一步转换都会形成一个rdd </span><br><span class="line">RDDA   RDDB   RDDC</span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">一个rdd 三个分区 经过一个map之后 分区不会发生变化的 再filter 分区也是三个</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.你对rdd做一个map操作  其实是对rdd内部的所有数据做map操作  ----RDD篇</span><br><span class="line">2.窄依赖操作 默认不会造成分区的个数发生变化</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023093733887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对于这个场景他们之间是有一个<strong>依赖关系</strong>的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1.假如说 RDDB 分区里 6，8 这元素在计算的时候挂了</span><br><span class="line">那么spark再重新计算的时候 它只需要重新计算这一个分区就可以了</span><br><span class="line">2.这个分区里的数据怎么来的呢？</span><br><span class="line">直接从上一个rddA分区 里拿过来 计算就可以 其他分区不会做处理  所以这里面存在依赖关系的</span><br><span class="line">3.6和8这个元素的这个分区 到底从RDDA的哪一个分区过来的 </span><br><span class="line">这个是必然知道的 再spark里叫Lineage</span><br><span class="line">4.Lineage ： 一个rdd 是如何从父RDD计算的来的</span><br><span class="line">5.RDD里的五大特性的其中一个特性 是可以得到依赖关系的 </span><br><span class="line"></span><br><span class="line">eg：因为你每次transformation的时候会把这个依赖关系记录下来的   这样就知道父rdd是谁</span><br><span class="line">就是自己数据坏了 去爸爸那计算恢复 总有源头可以计算恢复 </span><br><span class="line">这个机制</span><br><span class="line">就是Spark性能高的一个非常重要的原因</span><br><span class="line"></span><br><span class="line">6. 性能 + 容错 （容错也体现在 数据坏了 重新算一下就ok）</span><br><span class="line">7. 整个过程就是一个计算链</span><br><span class="line">8. 如果转换非常多 </span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">	这一个链路 100个转换 算到第99个数据坏了 ，如果要重头算 也是挺麻烦的一件事 </span><br><span class="line">	core里面 提供 checkpoint（根本用不到 了解即可）</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023095648257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>(1)idea中debug是可以看到依赖关系的<br><img src="https://img-blog.csdnimg.cn/20191023101103284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>所以整个过程中 你的RDD是怎么来的 spark是知道的</p>
<p>(2) spark-shell中</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.map(_*2)</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; val c = b.filter(_ &gt; 6)</span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: Array[Int] = Array(8, 10)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023101642501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023101727428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">那么这个过程中到底产生多少个RDD呢？</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: Array[Int] = Array(8, 10)</span><br><span class="line"></span><br><span class="line">scala&gt; c.toDebugString</span><br><span class="line">res1: String =</span><br><span class="line">(2) MapPartitionsRDD[2] at filter at &lt;console&gt;:25 []</span><br><span class="line"> |  MapPartitionsRDD[1] at map at &lt;console&gt;:25 []</span><br><span class="line"> |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">parallelize  --》ParallelCollectionRDD</span><br><span class="line">map  --》MapPartitionsRDD</span><br><span class="line">filter  ---》MapPartitionsRDD</span><br><span class="line"></span><br><span class="line">那么这几个东西哪里来的呢？看源码</span><br><span class="line"></span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">parallelize 返回的是一个RDD 而真正的类型是 ParallelCollectionRDD 其他同理</span><br></pre></td></tr></table></figure></div>
<h2 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile() 这一个过程产生多少个RDD呢？</span><br><span class="line"></span><br><span class="line">scala&gt; sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).collect()</span><br><span class="line">res2: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">结果出来了 到页面上看一下。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102310260984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">这个过程产生了多少rdd呢？</span><br><span class="line">scala&gt; sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).toDebugString</span><br><span class="line">res3: String =</span><br><span class="line">(2) ShuffledRDD[12] at reduceByKey at &lt;console&gt;:25 []</span><br><span class="line"> +-(2) MapPartitionsRDD[11] at map at &lt;console&gt;:25 []</span><br><span class="line">    |  MapPartitionsRDD[10] at flatMap at &lt;console&gt;:25 []</span><br><span class="line">    |  file:///home/double_happy/data/double_happy.txt MapPartitionsRDD[9] at textFile at &lt;console&gt;:25 []</span><br><span class="line">    |  file:///home/double_happy/data/double_happy.txt HadoopRDD[8] at textFile at &lt;console&gt;:25 []</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">textFile ：  HadoopRDD +  MapPartitionsRDD</span><br><span class="line">flatMap  ： MapPartitionsRDD</span><br><span class="line">map  ： MapPartitionsRDD</span><br><span class="line">reduceByKey  ： ShuffledRDD</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">textFile  过程：</span><br><span class="line"></span><br><span class="line">1.textFile</span><br><span class="line"> def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2.hadoopFile</span><br><span class="line"> def hadoopFile[K, V](</span><br><span class="line">      path: String,</span><br><span class="line">      inputFormatClass: Class[_ &lt;: InputFormat[K, V]],</span><br><span class="line">      keyClass: Class[K],</span><br><span class="line">      valueClass: Class[V],</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">hadoopFile 我没用拷贝全 但是足够了 返回值是一个 kv类型的   真正的返回的是HadoopRDD</span><br><span class="line"></span><br><span class="line">3.hadoopFile 就是 mapreduce里的 读取文本文件的mapper过程</span><br><span class="line"></span><br><span class="line">通过：mapper</span><br><span class="line">TextInputFormat</span><br><span class="line">mapper: LongWritable（每行数据的偏移量）  Text(每行数据的内容)</span><br><span class="line">那么 RDD[(K, V) 就是 （偏移量，Text）</span><br><span class="line"></span><br><span class="line">4.</span><br><span class="line">    hadoopFile(path, classOf[TextInputFormat],</span><br><span class="line">     classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions)</span><br><span class="line">      .map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">      </span><br><span class="line">这个HadoopRDD 之后的map操作 所以会产生MapPartitionsRDD</span><br><span class="line">map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">就是把读取的内容拿出来</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	rdd里的分区数据对应hdfs上一个文件有多少个block</span><br><span class="line"></span><br><span class="line">所以HadoopRDD底层实现可以去看一下 </span><br><span class="line"></span><br><span class="line"> override def getPartitions: Array[Partition] = &#123;</span><br><span class="line">    val jobConf = getJobConf()</span><br><span class="line">    // add the credentials here as this can be called before SparkContext initialized</span><br><span class="line">    SparkHadoopUtil.get.addCredentials(jobConf)</span><br><span class="line">    val inputFormat = getInputFormat(jobConf)</span><br><span class="line">    val inputSplits = inputFormat.getSplits(jobConf, minPartitions)</span><br><span class="line">    val array = new Array[Partition](inputSplits.size)</span><br><span class="line">    for (i &lt;- 0 until inputSplits.size) &#123;</span><br><span class="line">      array(i) = new HadoopPartition(id, i, inputSplits(i))</span><br><span class="line">    &#125;</span><br><span class="line">    array</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">getInputFormat(jobConf).getSplits(jobConf, minPartitions) 明白了吗</span><br></pre></td></tr></table></figure></div>
<h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">窄依赖</span><br><span class="line">       一个父RDD的partition至多被子RDD的partition使用一次</span><br><span class="line">       OneToOneDependency</span><br><span class="line">       都在一个stage中完成</span><br><span class="line">宽依赖   &lt;= 会产生shuffle 会有新的stage</span><br><span class="line">       一个父RDD的partition会被子RDD的partition使用多次</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191023111033655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105631974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105652721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105850276.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023110813166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果经过宽依赖之后的RDD的某一个分区数据挂掉</span><br><span class="line">需要去父RDD重新计算 会把父亲所有分区都会算一下才行 </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.所有分区都要重算</span><br><span class="line">从容错的角度来说，在开发过程，能使用窄依赖就使用窄依赖 emm这就话 不全对</span><br><span class="line">在某些情况下 会把窄依赖改成宽依赖 来实现。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023111336288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="解析wc过程"><a href="#解析wc过程" class="headerlink" title="解析wc过程"></a>解析wc过程</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;)</span><br><span class="line"> val words = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line"> val pair = words.map((_,1))</span><br><span class="line"> val result = pair.reduceByKey(_+_)</span><br><span class="line">   result.collect()</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023114118106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">为什么会多一个conbine操作呢？</span><br><span class="line">reduceBykey算子底层封装好的</span><br><span class="line"></span><br><span class="line">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> def combineByKeyWithClassTag[C](</span><br><span class="line">      createCombiner: V =&gt; C,</span><br><span class="line">      mergeValue: (C, V) =&gt; C,</span><br><span class="line">      mergeCombiners: (C, C) =&gt; C,</span><br><span class="line">      partitioner: Partitioner,</span><br><span class="line">      mapSideCombine: Boolean = true,</span><br><span class="line">      serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != null, &quot;mergeCombiners must be defined&quot;) // required as of Spark 0.9.0</span><br><span class="line">    if (keyClass.isArray) &#123;</span><br><span class="line">      if (mapSideCombine) &#123;</span><br><span class="line">        throw new SparkException(&quot;Cannot use map-side combining with array keys.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      if (partitioner.isInstanceOf[HashPartitioner]) &#123;</span><br><span class="line">        throw new SparkException(&quot;HashPartitioner cannot partition array keys.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    val aggregator = new Aggregator[K, V, C](</span><br><span class="line">      self.context.clean(createCombiner),</span><br><span class="line">      self.context.clean(mergeValue),</span><br><span class="line">      self.context.clean(mergeCombiners))</span><br><span class="line">    if (self.partitioner == Some(partitioner)) &#123;</span><br><span class="line">      self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        val context = TaskContext.get()</span><br><span class="line">        new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">      &#125;, preservesPartitioning = true)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new ShuffledRDD[K, V, C](self, partitioner)</span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">combineByKeyWithClassTag中的 mapSideCombine: Boolean = true</span><br><span class="line"></span><br><span class="line">map端输出 设置Combine为true </span><br><span class="line"></span><br><span class="line">reduceBykey这个算子有Combine，那么groupBykey算子有么？</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    // groupByKey shouldn&apos;t use map side combine because map side combine does not</span><br><span class="line">    // reduce the amount of data shuffled and requires all map side data be inserted</span><br><span class="line">    // into a hash table, leading to more objects in the old gen.</span><br><span class="line">    val createCombiner = (v: V) =&gt; CompactBuffer(v)</span><br><span class="line">    val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v</span><br><span class="line">    val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2</span><br><span class="line">    val bufs = combineByKeyWithClassTag[CompactBuffer[V]](</span><br><span class="line">      createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span><br><span class="line">    bufs.asInstanceOf[RDD[(K, Iterable[V])]]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mapSideCombine = false </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">reduceByKey</span><br><span class="line">	有map端输出预聚合功能的</span><br><span class="line">groupBykey</span><br><span class="line">	全数据shuffle的 ，没有预聚合</span><br></pre></td></tr></table></figure></div>
<h2 id="shuffle-operations"><a href="#shuffle-operations" class="headerlink" title="shuffle operations"></a>shuffle operations</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">shuffle operations</a></p>
<p>The shuffle is Spark’s mechanism for <strong>re-distributing data</strong> so that it’s grouped differently across partitions. This typically involves <strong>copying data across executors and machines</strong>, making the shuffle a complex and costly operation.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re-distributing data：</span><br><span class="line">数据重新分区 --就是shuffle过程  我画的wc那个图</span><br></pre></td></tr></table></figure></div>
<p>Operations which can cause a shuffle include repartition operations like repartition and coalesce, ‘ByKey operations (except for counting) like groupByKey and reduceByKey, and join operations like cogroup and join.</p>
<p>上面这句话是不严谨的 之后测试证实。</p>
<p><strong>The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O.</strong> </p>
<p>这块官网好好读读</p>
<h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">    val lines = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;)</span><br><span class="line">    val words = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">    val pair = words.map((_,1))</span><br><span class="line">    </span><br><span class="line">   val result2 = pair.groupByKey()</span><br><span class="line">    val result1 = pair.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">假设pair之后还有其他的业务逻辑</span><br><span class="line">这里是：</span><br><span class="line">	groupByKey</span><br><span class="line">	reduceByKey</span><br><span class="line"></span><br><span class="line">到pair为止 大家都是公用的 这块就有必要使用cache机制 </span><br><span class="line"></span><br><span class="line">如果不做这个操作和做了 有什么区别呢？</span><br></pre></td></tr></table></figure></div>
<p>（1）没有做cache测试</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[16] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res4: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br><span class="line">查看4040页面</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023145325933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再执行一遍 re.collect 页面还是这样的 </p>
<p>（2）做cache</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res0: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res1: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.cache</span><br><span class="line">res2: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line">查看页面</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023145545411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再执行一边</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102314564550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>会发现执行了两次 就根本不是一个东西了 ，做了cache已经把我们的东西持久化到默认的存储级别里去了，下次就会去缓存里读取数据了</strong><br><img src="https://img-blog.csdnimg.cn/20191023145840158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.不做cache 如果你对同一个操作执行多次 下一次会从头开始执行</span><br><span class="line">2.如果做了cache （lazy 的操作并不会触发）</span><br><span class="line">3.cache后 默认的存储级别后 为什么数据量会变大了呢？</span><br><span class="line">	之后再说。</span><br></pre></td></tr></table></figure></div>
<h2 id="persist和cache的区别"><a href="#persist和cache的区别" class="headerlink" title="persist和cache的区别"></a>persist和cache的区别</h2><p>You can mark an RDD to be persisted using the persist() or cache() methods on it. <strong>The first time it is computed in an action,</strong> it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.<br> If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the <strong>RDD.unpersist() method.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The first time it is computed in an action </span><br><span class="line">就是说 sparkcore里的</span><br><span class="line">persist、cache 执行是在遇到action算子 才触发</span><br><span class="line"></span><br><span class="line">在迭代次数比较多的场景下 使用</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res0: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res1: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.cache</span><br><span class="line">res2: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.unpersist()</span><br><span class="line">res5: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023150726195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cache 、 persist   是 lazy的 </span><br><span class="line">2.unpersist  是 eager的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def cache(): this.type = persist()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</span><br><span class="line"></span><br><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,    // _deserialized 不序列化</span><br><span class="line">    private var _replication: Int = 1)</span><br><span class="line">  extends Externalizable</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.cache</span><br><span class="line">    ==&gt; persist</span><br><span class="line">        ==&gt; persist(MEMORY_ONLY)</span><br><span class="line">2.cache、persist 默认都走的是MEMORY_ONLY</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023151458914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>换一种存储级别<br><img src="https://img-blog.csdnimg.cn/20191023152323222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>数据是不是小了</strong> 序列化的会节省空间</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; re.collect</span><br><span class="line">res7: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.unpersist()</span><br><span class="line">res8: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line"></span><br><span class="line">scala&gt; re.persist(StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">res9: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res10: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<p><strong>Which Storage Level to Choose?</strong><br><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#which-storage-level-to-choose" target="_blank" rel="noopener">Which Storage Level to Choose?</a></p>
<p>一定要做序列化么？这和压缩是一个道理<br><strong>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency.</strong></p>
<h2 id="coalesce和repartition"><a href="#coalesce和repartition" class="headerlink" title="coalesce和repartition"></a>coalesce和repartition</h2><p>repartition</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">   * Return a new RDD that has exactly numPartitions partitions.</span><br><span class="line">   *</span><br><span class="line">   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses</span><br><span class="line">   * a shuffle to redistribute data.</span><br><span class="line">   *</span><br><span class="line">   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,</span><br><span class="line">   * which can avoid performing a shuffle.</span><br><span class="line">   */</span><br><span class="line">  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = true)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Can increase or decrease the level of parallelism in this RDD</span><br><span class="line"> shuffle = true</span><br><span class="line">就是</span><br><span class="line">repartition 无论增大还是减少 分区数 它都走shuffle</span><br><span class="line"></span><br><span class="line">增大分区数 我们使用repartition  </span><br><span class="line">repartition 底层调用coalesce</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark03</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object CoalesceAndRepartitionApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val data = sc.parallelize(List(1 to 9: _*),3)</span><br><span class="line"></span><br><span class="line">    data.mapPartitionsWithIndex((index,partition) =&gt; &#123;</span><br><span class="line">      partition.map(x=&gt;s&quot;分区是$index,元素是$x&quot;)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    val repartitionRDD = data.repartition(4)</span><br><span class="line">    repartitionRDD.mapPartitionsWithIndex((index,partition) =&gt; &#123;</span><br><span class="line">      partition.map(x=&gt;s&quot;分区是$index,元素是$x&quot;)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">分区是1,元素是4</span><br><span class="line">分区是0,元素是1</span><br><span class="line">分区是1,元素是5</span><br><span class="line">分区是1,元素是6</span><br><span class="line">分区是0,元素是2</span><br><span class="line">分区是0,元素是3</span><br><span class="line">分区是2,元素是7</span><br><span class="line">分区是2,元素是8</span><br><span class="line">分区是2,元素是9</span><br><span class="line">-------------------------</span><br><span class="line">分区是1,元素是3</span><br><span class="line">分区是0,元素是2</span><br><span class="line">分区是1,元素是6</span><br><span class="line">分区是0,元素是5</span><br><span class="line">分区是1,元素是9</span><br><span class="line">分区是0,元素是8</span><br><span class="line">分区是3,元素是1</span><br><span class="line">分区是3,元素是4</span><br><span class="line">分区是3,元素是7</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1 to 9: _*),3)</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; data.repartition(4)</span><br><span class="line">res11: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at repartition at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; data.repartition(4).collect</span><br><span class="line">res12: Array[Int] = Array(2, 7, 3, 4, 8, 5, 9, 1, 6)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023154639524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.能够说明 repartition是走shuffle的</span><br></pre></td></tr></table></figure></div>

<p><strong>coalesce</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Return a new RDD that is reduced into `numPartitions` partitions.</span><br><span class="line">   *</span><br><span class="line">   * This results in a narrow dependency, e.g. if you go from 1000 partitions</span><br><span class="line">   * to 100 partitions, there will not be a shuffle, instead each of the 100</span><br><span class="line">   * new partitions will claim 10 of the current partitions. If a larger number</span><br><span class="line">   * of partitions is requested, it will stay at the current number of partitions.</span><br><span class="line">   *</span><br><span class="line">   * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1,</span><br><span class="line">   * this may result in your computation taking place on fewer nodes than</span><br><span class="line">   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span><br><span class="line">   * you can pass shuffle = true. This will add a shuffle step, but means the</span><br><span class="line">   * current upstream partitions will be executed in parallel (per whatever</span><br><span class="line">   * the current partitioning is).</span><br><span class="line">   *</span><br><span class="line">   * @note With shuffle = true, you can actually coalesce to a larger number</span><br><span class="line">   * of partitions. This is useful if you have a small number of partitions,</span><br><span class="line">   * say 100, potentially with a few partitions being abnormally large. Calling</span><br><span class="line">   * coalesce(1000, shuffle = true) will result in 1000 partitions with the</span><br><span class="line">   * data distributed using a hash partitioner. The optional partition coalescer</span><br><span class="line">   * passed in must be serializable.</span><br><span class="line">   */</span><br><span class="line">  def coalesce(numPartitions: Int, shuffle: Boolean = false,</span><br><span class="line">               partitionCoalescer: Option[PartitionCoalescer] = Option.empty)</span><br><span class="line">              (implicit ord: Ordering[T] = null)</span><br><span class="line">      : RDD[T] = withScope &#123;</span><br><span class="line">    require(numPartitions &gt; 0, s&quot;Number of partitions ($numPartitions) must be positive.&quot;)</span><br><span class="line">    if (shuffle) &#123;</span><br><span class="line">      /** Distributes elements evenly across output partitions, starting from a random partition. */</span><br><span class="line">      val distributePartition = (index: Int, items: Iterator[T]) =&gt; &#123;</span><br><span class="line">        var position = (new Random(index)).nextInt(numPartitions)</span><br><span class="line">        items.map &#123; t =&gt;</span><br><span class="line">          // Note that the hash code of the key will just be the key itself. The HashPartitioner</span><br><span class="line">          // will mod it with the number of total partitions.</span><br><span class="line">          position = position + 1</span><br><span class="line">          (position, t)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; : Iterator[(Int, T)]</span><br><span class="line"></span><br><span class="line">      // include a shuffle step so that our upstream tasks are still distributed</span><br><span class="line">      new CoalescedRDD(</span><br><span class="line">        new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        new HashPartitioner(numPartitions)),</span><br><span class="line">        numPartitions,</span><br><span class="line">        partitionCoalescer).values</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new CoalescedRDD(this, numPartitions, partitionCoalescer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Return a new RDD that is reduced into `numPartitions` partitions.</span><br><span class="line">1.coalesce 用来 reduced numPartitions </span><br><span class="line"></span><br><span class="line">shuffle: Boolean = false,</span><br><span class="line"></span><br><span class="line">2。coalesce 默认是不走shuffle的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; data.partitions.size</span><br><span class="line">res15: Int = 3</span><br><span class="line"></span><br><span class="line">scala&gt; data.coalesce(4).partitions.size</span><br><span class="line">res16: Int = 3</span><br><span class="line"></span><br><span class="line">scala&gt; data.coalesce(2).partitions.size</span><br><span class="line">res17: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt;data.coalesce(4,true).partitions.size    //这样就走shuffle啦</span><br><span class="line">res18: Int = 4</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">repartition调用的是coalesce算子，shuffle默认为true    会产生新的 stage</span><br><span class="line">coalesce  shuffle默认为false    传shuffle为true，就和repartition一样</span><br></pre></td></tr></table></figure></div>
<p>Operations which can cause a shuffle include repartition operations like repartition and coalesce, </p>
<p>所以这句话 coalesce 默认是不会产生shuffle的 官网这话不严谨。</p>

      
    </div>
    
      <footer class="article-footer">
        完
      </footer>
    
  </div>
  
    
<nav id="article-nav">
  <div class="article-nav-block">
    
      <a href="/2018/01/24/Spark007-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption"></strong>
        <div class="article-nav-title">
          
            Spark007--综合案例
          
        </div>
      </a>
    
  </div>
  <div class="article-nav-block">
    
      <a href="/2018/01/23/Spark005-%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Spark005--核心架构</div>
        <strong class="article-nav-caption"></strong>
      </a>
    
  </div>
</nav>

    <link rel="stylesheet" href="/css/gitment.css"> 
<script src="/js/gitment.js"></script>

<div id="gitmentContainer"></div>

<script>
var gitment = new Gitment({
  owner: '',
  repo: '',
  oauth: {
    client_id: '',
    client_secret: '',
  },
})
gitment.render('gitmentContainer')
</script>

  
  
</article>
</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://sxwanggit126.github.io/" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>