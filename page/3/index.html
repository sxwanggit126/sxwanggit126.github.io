<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;3&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main">
  
    <article id="post-Spark004-总结前面的基本操作" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/22/Spark004-%E6%80%BB%E7%BB%93%E5%89%8D%E9%9D%A2%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">Spark004--总结前面的基本操作</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/22/Spark004-%E6%80%BB%E7%BB%93%E5%89%8D%E9%9D%A2%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" class="article-date">
  <time datetime="2018-01-22T12:00:42.000Z" itemprop="datePublished">2018-01-22</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="实现排序功能"><a href="#实现排序功能" class="headerlink" title="实现排序功能"></a>实现排序功能</h2><p>（1）按照价格排序</p>
<p><strong>第一种</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;))</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          (name, price, amount)</span><br><span class="line">        &#125;).sortBy(-_._2).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">输出的结果：</span><br><span class="line">(iphone11,7000.0,1000)</span><br><span class="line">(皮鞭,20.0,10)</span><br><span class="line">(蜡烛,20.0,100)</span><br><span class="line">(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p> 这里一定要注意：<br> <strong>你运行出来的结果可能顺序不对，sortBy是全局排序的 所以你测试的时候 可以设置分区数为 1</strong></p>
<p>上面的结果当然也可以按多个排：<br>价格相同，再按照库存的数量排序</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          (name, price, amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;(-x._2,-x._3)).printInfo()</span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">(iphone11,7000.0,1000)</span><br><span class="line">(蜡烛,20.0,100)</span><br><span class="line">(皮鞭,20.0,10)</span><br><span class="line">(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>上面那种是使用Tuple的方式 ，生产上还是要封装一个类来实现的 </p>
<p><strong>第二种</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp02 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          new Products(name,price,amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>直接报错：<br>Error:(21, 18) No implicit Ordering defined for com.ruozedata.spark.spark02.Products.<br>        }).sortBy(x=&gt;x).printInfo()<br>Error:(21, 18) not enough arguments for method sortBy: (implicit ord: Ordering[com.ruozedata.spark.spark02.Products], implicit ctag: scala.reflect.ClassTag[com.ruozedata.spark.spark02.Products])org.apache.spark.rdd.RDD[com.ruozedata.spark.spark02.Products].<br>Unspecified value parameters ord, ctag.<br>        }).sortBy(x=&gt;x).printInfo()</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">按上面那么写有什么问题？</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Return this RDD sorted by the given key function.</span><br><span class="line">   */</span><br><span class="line">  def sortBy[K](</span><br><span class="line">      f: (T) =&gt; K,</span><br><span class="line">      ascending: Boolean = true,</span><br><span class="line">      numPartitions: Int = this.partitions.length)</span><br><span class="line">      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope &#123;</span><br><span class="line">    this.keyBy[K](f)</span><br><span class="line">        .sortByKey(ascending, numPartitions)</span><br><span class="line">        .values</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.有一个隐式转换的 需要一个Ordering 这个东西 ，而我们传进去的是x ==》 product </span><br><span class="line">它根本找不到任何排序规则</span><br></pre></td></tr></table></figure></div>

<p>因为：<br>Error:(21, 18) not enough arguments for method sortBy: (implicit ord: Ordering[com.ruozedata.spark.spark02.Products], implicit ctag: scala.reflect.ClassTag[com.ruozedata.spark.spark02.Products])org.apache.spark.rdd.RDD[com.ruozedata.spark.spark02.Products].<br>所以：<br>1.not enough arguments<br>2.(implicit ord: Ordering[com.ruozedata.spark.spark02.Products]   一个Products类型</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">class Products(val name:String , val price:Double,val amount:Int) </span><br><span class="line">      extends  Ordered[Products]&#123;</span><br><span class="line">  override def compare(that: Products): Int = &#123;</span><br><span class="line">    </span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>修改之后再看：<br><img src="https://img-blog.csdnimg.cn/2019102213123198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>又报错了：<br>Serialization stack:<br>    - object not serializable (class: com.ruozedata.spark.spark02.Products, value: com.ruozedata.spark.spark02.Products@237c4154)</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Products(val name:String , val price:Double,val amount:Int)</span><br><span class="line">      extends  Ordered[Products] with Serializable&#123;</span><br><span class="line">  override def compare(that: Products): Int = &#123;</span><br><span class="line"></span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>修改之后：<br><img src="https://img-blog.csdnimg.cn/20191022131456435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>没问题了 那么我想看输出结果 该怎么办呢？<br>toString 就可以了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Products(val name:String , val price:Double,val amount:Int)</span><br><span class="line">      extends  Ordered[Products] with Serializable&#123;</span><br><span class="line">  override def compare(that: Products): Int = &#123;</span><br><span class="line"></span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def toString: String = name + &quot;\t&quot; + price + &quot;\t&quot; +amount</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp02 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          new Products(name,price,amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">皮鞭	20.0	10</span><br><span class="line">蜡烛	20.0	100</span><br><span class="line">iphone11	7000.0	1000</span><br><span class="line">扑克牌	5.0	2000</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>工作当中别这么用 毕竟自己实现类 还需要序列化 挺麻烦的</p>
<p><strong>第三种  case class</strong><br>生产上用的比较多</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">case  class  Products02( name:String,  price:Double,  amount:Int)</span><br><span class="line">      extends  Ordered[Products02] &#123;</span><br><span class="line">  override def compare(that: Products02): Int = &#123;</span><br><span class="line"></span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp02 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">//          new Products(name,price,amount)</span><br><span class="line">          Products02(name,price,amount)</span><br><span class="line"></span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">Products02(皮鞭,20.0,10)</span><br><span class="line">Products02(蜡烛,20.0,100)</span><br><span class="line">Products02(iphone11,7000.0,1000)</span><br><span class="line">Products02(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>那么 class  和case class 的区别是什么？ 看scala篇 </p>
<p>*<em>第四种     使用隐式转换的方式 *</em></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line">          new Products03(name,price,amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">class Products03( name:String,  price:Double,  amount:Int)</span><br></pre></td></tr></table></figure></div>
<p><strong>不准对Products03 做任何修改 完成排序功能该怎么办？</strong><br>Product3默认是不能排序的 ====implicit==&gt; 能排序的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          new Products03(name,price,amount)</span><br><span class="line"></span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * Product3默认是不能排序的 ====implicit==&gt; 能排序的</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">    implicit def product2Ordered(products: Products03):Ordered[Products03] = new Ordered[Products03]&#123;</span><br><span class="line">      override def compare(that: Products03): Int = &#123;</span><br><span class="line">        products.amount - that.amount</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Products03( val  name:String, val  price:Double,  val amount:Int) extends Serializable &#123;</span><br><span class="line">  override def toString: String = name + &quot;\t&quot; + price + &quot;\t&quot; + amount</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><strong>第五种 花里胡哨</strong><br> 能看懂就行 不需要掌握<br> 实际上也是 隐式转换   隐式变量</p>
<p>思路就是 ：<br>(implicit ord: Ordering[com.ruozedata.spark.spark02.Products]   一个Products类型</p>
<p>既然你却这个 就隐式转换给你 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line"></span><br><span class="line">object SortApp04 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">    val product = products.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot; &quot;)</span><br><span class="line">      val name = splits(0)</span><br><span class="line">      val price = splits(1).toDouble</span><br><span class="line">      val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">      (name, price, amount)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * Ordering on</span><br><span class="line">      *</span><br><span class="line">      * -x._2, -x._3  排序规则</span><br><span class="line">      * (Double,Int) 定义的是规则的返回值的类型  就是参与排序的 类型</span><br><span class="line">      * (String,Double,Int) 数据的类型</span><br><span class="line">      */</span><br><span class="line">    implicit val ord = Ordering[(Double,Int)].on[(String,Double,Int)](x=&gt;(-x._2, -x._3))</span><br><span class="line">    product.sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(iphone11,7000.0,1000)</span><br><span class="line">(蜡烛,20.0,100)</span><br><span class="line">(皮鞭,20.0,10)</span><br><span class="line">(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line"></span><br><span class="line">object SortApp04 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">    val product = products.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot; &quot;)</span><br><span class="line">      val name = splits(0)</span><br><span class="line">      val price = splits(1).toDouble</span><br><span class="line">      val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">      Products02(name, price, amount)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * Ordering on</span><br><span class="line">      *</span><br><span class="line">      * -x._2, -x._3  排序规则</span><br><span class="line">      * (Double,Int) 定义的是规则的返回值的类型  就是参与排序的 类型</span><br><span class="line">      * (String,Double,Int) 数据的类型</span><br><span class="line">      */</span><br><span class="line">    implicit val ord = Ordering[(Double,Int)].on[Products02](x=&gt;(-x.price, -x.amount))</span><br><span class="line">    product.sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">后面两种都是隐式转换：</span><br><span class="line">一个是增强类</span><br><span class="line">一个是隐式参数 </span><br><span class="line"></span><br><span class="line">隐式转换没什么 听着挺吓人的</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark002-transform-action" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/21/Spark002-transform-action/">Spark002-transform&amp;action</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/21/Spark002-transform-action/" class="article-date">
  <time datetime="2018-01-21T11:58:56.000Z" itemprop="datePublished">2018-01-21</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>转换操作不会立即执行的，不触发作业的执行 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RDD操作</span><br><span class="line">    transformation  转换    它不会立即执行  你写了1亿个转换  白写   lazy</span><br><span class="line">    action          动作    只有遇到action才会提交作业开始执行      eager</span><br></pre></td></tr></table></figure></div>

<h2 id="官网RDD算子介绍："><a href="#官网RDD算子介绍：" class="headerlink" title="官网RDD算子介绍："></a>官网RDD算子介绍：</h2><p>RDD Operations：<br>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).</p>
<p>All <strong>transformations</strong> in Spark are <strong>lazy</strong>, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. <strong>This design enables Spark to run more efficiently.</strong> For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>
<p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">This design enables Spark to run more efficiently：</span><br><span class="line">对比Hadoop举一个例子：</span><br><span class="line">	1+1+1+1 这个动作</span><br><span class="line">MR：1+1=2  --&gt;落地--&gt;读取 +1=3  --&gt;落地---&gt;读取+1=4</span><br><span class="line">Spark：1+1+1+1 这块全部用transformations 来完成 ，真正计算的时候，才一次性提交上去。一个流水先就全都执行完了。</span><br></pre></td></tr></table></figure></div>
<h2 id="Transformations讲解前的说明"><a href="#Transformations讲解前的说明" class="headerlink" title="Transformations讲解前的说明"></a>Transformations讲解前的说明</h2><p>先说明我们的程序里创建SparkContex的方式，由于每次创建都要写appname，master，以及RDD数据集在Driver端打印出来查看都要写foreach(println)，每次都要写很麻烦，这里我们给封装一下。<br>效果展示：<br>    <img src="https://img-blog.csdnimg.cn/2019101211271935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如果我不想输出 我输入一个1进去即可：<br><img src="https://img-blog.csdnimg.cn/20191012113652954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是自己写的哈 ，老师上课留的作业 人家要求动手能力。不会全部给你，让你做一个伸手党就没意义。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.homework.utils</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">object ContextUtils &#123;</span><br><span class="line">  /**</span><br><span class="line">    * 获取sc</span><br><span class="line">    */</span><br><span class="line">  def getSparkContext(appname:String,defalut:String = &quot;local[2]&quot;): SparkContext = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line">    new SparkContext(sparkConf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>下面这个主要使用隐式转换，看不懂可以查看Scala博客：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.homework.utils</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">object ImplicitAspect &#123;</span><br><span class="line">  implicit def rdd2RichRDD[T](rdd : RDD[T]) : RichRDD[T] = new RichRDD[T](rdd)</span><br><span class="line">&#125;</span><br><span class="line">class RichRDD[T](rdd : RDD[T])&#123;</span><br><span class="line"> def printInfo(num : Int =0): Unit =&#123;</span><br><span class="line">    num match &#123;</span><br><span class="line">      case 0 =&gt; rdd.foreach(println);println(&quot;-------------------------&quot;)</span><br><span class="line">      case _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>源码面前，了无秘密。通过源码进行学习</p>
<h2 id="（1）Map相关的算子"><a href="#（1）Map相关的算子" class="headerlink" title="（1）Map相关的算子"></a>（1）Map相关的算子</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1.makeRDD / parallelize </span><br><span class="line"></span><br><span class="line"> def makeRDD[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">  &#125;</span><br><span class="line">注意：makeRDD底层调用的就是parallelize </span><br><span class="line"></span><br><span class="line"> /** Distribute a local Scala collection to form an RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call</span><br><span class="line">   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the</span><br><span class="line">   * modified collection. Pass a copy of the argument to avoid this.</span><br><span class="line">   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an</span><br><span class="line">   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.</span><br><span class="line">   * @param seq Scala collection to distribute</span><br><span class="line">   * @param numSlices number of partitions to divide the collection into</span><br><span class="line">   * @return RDD representing distributed collection</span><br><span class="line">   */</span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2.map : 处理每一条数据</span><br><span class="line">/**</span><br><span class="line">   * Return a new RDD by applying a function to all elements of this RDD.</span><br><span class="line">   */</span><br><span class="line">  def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012114402942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">RDD是有多个partition所构成的，</span><br><span class="line">3.mapPartitions: Return a new RDD by applying a function to each partition of this RDD. 对分区做处理的，一个分区里有很多的元素的。</span><br><span class="line"> /**</span><br><span class="line">   * Return a new RDD by applying a function to each partition of this RDD.</span><br><span class="line">   *</span><br><span class="line">   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span><br><span class="line">   * should be `false` unless this is a pair RDD and the input function doesn&apos;t modify the keys.</span><br><span class="line">   */</span><br><span class="line">  def mapPartitions[U: ClassTag](</span><br><span class="line">      f: Iterator[T] =&gt; Iterator[U],</span><br><span class="line">      preservesPartitioning: Boolean = false): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanedF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD(</span><br><span class="line">      this,</span><br><span class="line">      (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019101211493594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>一个是作用于每个元素，一个是作用于每个分区。<br><img src="https://img-blog.csdnimg.cn/20191012115049522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这是作用于每个分区里的每个元素<br><img src="https://img-blog.csdnimg.cn/20191012115140493.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结果和map是一样的。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line"> /**</span><br><span class="line">      * map 处理每一条数据</span><br><span class="line">      * mapPartitions 对每个分区进行处理</span><br><span class="line">      *</span><br><span class="line">      * map：100个元素  10个分区 ==&gt; 知识点：要把RDD的数据写入MySQL  Connection次数</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">4.mapPartitionsWithIndex</span><br><span class="line">你想看哪个分区里的东西 这个算子可以拿的到</span><br><span class="line">/**</span><br><span class="line">   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index</span><br><span class="line">   * of the original partition.</span><br><span class="line">   *</span><br><span class="line">   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span><br><span class="line">   * should be `false` unless this is a pair RDD and the input function doesn&apos;t modify the keys.</span><br><span class="line">   */</span><br><span class="line">  def mapPartitionsWithIndex[U: ClassTag](</span><br><span class="line">      f: (Int, Iterator[T]) =&gt; Iterator[U],</span><br><span class="line">      preservesPartitioning: Boolean = false): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanedF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD(</span><br><span class="line">      this,</span><br><span class="line">      (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(index, iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012115918554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_30,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2019101212020396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>元素为什么这么存放的呢？之后再来讲解。<br>生产上是不关注这个分区里的哪个元素的 只是用来学。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">5.mapValues</span><br><span class="line"> /**</span><br><span class="line">   * Pass each value in the key-value pair RDD through a map function without changing the keys;</span><br><span class="line">   * this also retains the original RDD&apos;s partitioning.</span><br><span class="line">   */</span><br><span class="line">  def mapValues[U](f: V =&gt; U): RDD[(K, U)] = self.withScope &#123;</span><br><span class="line">    val cleanF = self.context.clean(f)</span><br><span class="line">    new MapPartitionsRDD[(K, U), (K, V)](self,</span><br><span class="line">      (context, pid, iter) =&gt; iter.map &#123; case (k, v) =&gt; (k, cleanF(v)) &#125;,</span><br><span class="line">      preservesPartitioning = true)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019101212080774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191012120846720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_50,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">6.flatmap = map + flatten   就是打扁以后 做map</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   *  Return a new RDD by first applying a function to all elements of this</span><br><span class="line">   *  RDD, and then flattening the results.</span><br><span class="line">   */</span><br><span class="line">  def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TraversableOnce:  一次遍历的意思   flattening the results.也就是压扁</span><br></pre></td></tr></table></figure></div>
<p>对比map：<br><img src="https://img-blog.csdnimg.cn/20191012121548376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2019101212132486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>map对里面元素做处理，是不会改变 内部的结构的 。</p>
<p>flatMap:<br><img src="https://img-blog.csdnimg.cn/20191012121907803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结果：<br><img src="https://img-blog.csdnimg.cn/20191012122329639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_60,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">秀操作的：没什么用</span><br><span class="line">scala&gt; sc.parallelize(1 to 5).flatMap(1 to _).collect</span><br><span class="line">res3: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure></div>
<h2 id="（2）glom"><a href="#（2）glom" class="headerlink" title="（2）glom"></a>（2）glom</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">glom：把每一个分区里的数据 形成一个数组  比mapwithindex好用</span><br><span class="line"> /**</span><br><span class="line">   * Return an RDD created by coalescing all elements within each partition into an array.</span><br><span class="line">   */</span><br><span class="line">  def glom(): RDD[Array[T]] = withScope &#123;</span><br><span class="line">    new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) =&gt; Iterator(iter.toArray))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 30).glom().collect</span><br><span class="line">res4: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), Array(16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="3-sample"><a href="#3-sample" class="headerlink" title="(3)sample"></a>(3)sample</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sample:</span><br><span class="line">/**</span><br><span class="line">   * Return a sampled subset of this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span><br><span class="line">   * @param fraction expected size of the sample as a fraction of this RDD&apos;s size</span><br><span class="line">   *  without replacement: probability that each element is chosen; fraction must be [0, 1]</span><br><span class="line">   *  with replacement: expected number of times each element is chosen; fraction must be greater</span><br><span class="line">   *  than or equal to 0</span><br><span class="line">   * @param seed seed for the random number generator</span><br><span class="line">   *</span><br><span class="line">   * @note This is NOT guaranteed to provide exactly the fraction of the count</span><br><span class="line">   * of the given [[RDD]].</span><br><span class="line">   */</span><br><span class="line">  def sample(</span><br><span class="line">      withReplacement: Boolean,</span><br><span class="line">      fraction: Double,</span><br><span class="line">      seed: Long = Utils.random.nextLong): RDD[T] = &#123;</span><br><span class="line">    require(fraction &gt;= 0,</span><br><span class="line">      s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    withScope &#123;</span><br><span class="line">      require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction)</span><br><span class="line">      if (withReplacement) &#123;</span><br><span class="line">        new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012123922980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">	withReplacement：抽样的时候要不要放回去</span><br></pre></td></tr></table></figure></div>
<h2 id="（4）filter"><a href="#（4）filter" class="headerlink" title="（4）filter"></a>（4）filter</h2><p><img src="https://img-blog.csdnimg.cn/20191012124445377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 30).filter(_ &gt; 20).collect</span><br><span class="line">res5: Array[Int] = Array(21, 22, 23, 24, 25, 26, 27, 28, 29, 30)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(1 to 30).filter(x=&gt; x % 2 ==0 &amp;&amp; x &gt;10).collect</span><br><span class="line">res6: Array[Int] = Array(12, 14, 16, 18, 20, 22, 24, 26, 28, 30)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<h2 id="5-other类型的"><a href="#5-other类型的" class="headerlink" title="(5)other类型的"></a>(5)other类型的</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">union:  就是简单的合并  是不去重的哈 </span><br><span class="line">/**</span><br><span class="line">   * Return the union of this RDD and another one. Any identical elements will appear multiple</span><br><span class="line">   * times (use `.distinct()` to eliminate them).</span><br><span class="line">   */</span><br><span class="line">  def union(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    sc.union(this, other)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; a.union(b).collect</span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 4, 5, 6, 77, 7, 7)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">intersection:  交集</span><br><span class="line">  /**</span><br><span class="line">   * Return the intersection of this RDD and another one. The output will not contain any duplicate</span><br><span class="line">   * elements, even if the input RDDs did.</span><br><span class="line">   *</span><br><span class="line">   * @note This method performs a shuffle internally.</span><br><span class="line">   */</span><br><span class="line">  def intersection(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null)))</span><br><span class="line">        .filter &#123; case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;</span><br><span class="line">        .keys</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; a.intersection(b).collect</span><br><span class="line">res8: Array[Int] = Array(4, 6, 5)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">subtract:差集  出现在a里面的没有出现在b里面的 叫差集</span><br><span class="line">/**</span><br><span class="line">   * Return an RDD with the elements from `this` that are not in `other`.</span><br><span class="line">   *</span><br><span class="line">   * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting</span><br><span class="line">   * RDD will be &amp;lt;= us.</span><br><span class="line">   */</span><br><span class="line">  def subtract(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; a.subtract(b).collect</span><br><span class="line">res9: Array[Int] = Array(2, 1, 3)</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">去重：distinct</span><br><span class="line">	 /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(): RDD[T] = withScope &#123;</span><br><span class="line">    distinct(partitions.length)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; b.distinct.collect</span><br><span class="line">res10: Array[Int] = Array(4, 6, 77, 7, 5)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">去重这块 是可以传入分区参数的 ： 也可以没有的 没有的就是默认的分区</span><br><span class="line">你传进来多少分区就意味着 重新分区了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">scala&gt;    b.distinct(4).mapPartitionsWithIndex((index,partition)=&gt;&#123;</span><br><span class="line">     |       partition.map(x=&gt; s&quot;分区是$index, 元素是 $x&quot;)</span><br><span class="line">     |     &#125;).collect()</span><br><span class="line">res11: Array[String] = Array(分区是0, 元素是 4, 分区是1, 元素是 77, 分区是1, 元素是 5, 分区是2, 元素是 6, 分区是3, 元素是 7)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">四个分区： 那是怎么分区的呢？  元素%partitions</span><br><span class="line">即元素对分区个数取模 来分的</span><br><span class="line"></span><br><span class="line">分区是0, 元素是 4,    4%4=0</span><br><span class="line">分区是1, 元素是 77, </span><br><span class="line">分区是1, 元素是 5,  5%4 =1</span><br><span class="line">分区是2, 元素是 6,  6%4=2</span><br><span class="line">分区是3, 元素是 7  7%4 = 3</span><br><span class="line"></span><br><span class="line">明白了吧  通过这个例子 知道是怎么进行分区的。</span><br></pre></td></tr></table></figure></div>
<h2 id="KV类型的"><a href="#KV类型的" class="headerlink" title="KV类型的"></a>KV类型的</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">groupKeyKey：不怎么用的哈</span><br><span class="line">把key相同的分到一组</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Group the values for each key in the RDD into a single sequence. Hash-partitions the</span><br><span class="line">   * resulting RDD with the existing partitioner/parallelism level. The ordering of elements</span><br><span class="line">   * within each group is not guaranteed, and may even differ each time the resulting RDD is</span><br><span class="line">   * evaluated.</span><br><span class="line">   *</span><br><span class="line">   * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">   */</span><br><span class="line">  def groupByKey(): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Group the values for each key in the RDD into a single sequence. Allows controlling the</span><br><span class="line">   * partitioning of the resulting key-value pair RDD by passing a Partitioner.</span><br><span class="line">   * The ordering of elements within each group is not guaranteed, and may even differ</span><br><span class="line">   * each time the resulting RDD is evaluated.</span><br><span class="line">   *</span><br><span class="line">   * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">   *</span><br><span class="line">   * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span><br><span class="line">   * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`.</span><br><span class="line">   */</span><br><span class="line">  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    // groupByKey shouldn&apos;t use map side combine because map side combine does not</span><br><span class="line">    // reduce the amount of data shuffled and requires all map side data be inserted</span><br><span class="line">    // into a hash table, leading to more objects in the old gen.</span><br><span class="line">    val createCombiner = (v: V) =&gt; CompactBuffer(v)</span><br><span class="line">    val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v</span><br><span class="line">    val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2</span><br><span class="line">    val bufs = combineByKeyWithClassTag[CompactBuffer[V]](</span><br><span class="line">      createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span><br><span class="line">    bufs.asInstanceOf[RDD[(K, Iterable[V])]]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey()</span><br><span class="line">res12: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[35] at groupByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().collect</span><br><span class="line">res13: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(2)), (a,CompactBuffer(1, 99)), (c,CompactBuffer(3)))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>接着上面的值 求相同的key的和 是多少使用什么算子呢？ 上面讲过了哈</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().collect</span><br><span class="line">res13: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(2)), (a,CompactBuffer(1, 99)), (c,CompactBuffer(3)))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().mapValues(x=&gt;x.sum).collect</span><br><span class="line">res14: Array[(String, Int)] = Array((b,2), (a,100), (c,3))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey：</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).reduceByKey(_+_)</span><br><span class="line">res15: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at reduceByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).reduceByKey(_+_).collect</span><br><span class="line">res16: Array[(String, Int)] = Array((b,2), (a,100), (c,3))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="distinct-底层"><a href="#distinct-底层" class="headerlink" title="distinct 底层"></a>distinct 底层</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">  &#125;</span><br><span class="line">注意：</span><br><span class="line">distinct底层是使用 map+reduceByKey 的 是不是很简单</span><br><span class="line">reduceByKey就把两两相同的东西 丢到一块去</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      * distinct 去重</span><br><span class="line">      * 不允许使用distinct做去重</span><br><span class="line">      *</span><br><span class="line">      * x =&gt; (x,null)</span><br><span class="line">      *</span><br><span class="line">      * 8 =&gt; (8,null)</span><br><span class="line">      * 8 =&gt; (8,null)</span><br><span class="line">      */</span><br><span class="line">    val b = sc.parallelize(List(3,4,5,6,7,8,8))</span><br><span class="line">    b.map(x =&gt; (x,null)).reduceByKey((x,y) =&gt; x).map(_._1)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.rdd.RDD[(Int, Null)] :一定要看清reduceByKey的数据结构哈</span><br><span class="line"></span><br><span class="line">scala&gt; val r1 = sc.parallelize(List(1,1,12,3,3,4,6,6,6))</span><br><span class="line">r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; r1.map(x=&gt;(x,null)).reduceByKey((x,y)=&gt;x)</span><br><span class="line">res17: org.apache.spark.rdd.RDD[(Int, Null)] = ShuffledRDD[49] at reduceByKey at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">实际上到这一步 去重已经去掉了吧 明白吗   再把null去掉就可以了</span><br><span class="line"></span><br><span class="line">scala&gt; r1.map(x=&gt;(x,null)).reduceByKey((x,y)=&gt;x).map(_._1).collect</span><br><span class="line">res18: Array[Int] = Array(4, 6, 12, 1, 3)</span><br></pre></td></tr></table></figure></div>
<p>所以使用常用的算子一定要手点进去看看底层的实现哈 。</p>
<h2 id="个人理解-这个算子超重要"><a href="#个人理解-这个算子超重要" class="headerlink" title="个人理解 这个算子超重要"></a>个人理解 这个算子超重要</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">groupBy：自定义分组  分组条件就是自定义传进去的</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">  * Return an RDD of grouped items. Each group consists of a key and a sequence of elements</span><br><span class="line">  * mapping to that key. The ordering of elements within each group is not guaranteed, and</span><br><span class="line">  * may even differ each time the resulting RDD is evaluated.</span><br><span class="line">  *</span><br><span class="line">  * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">  * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">  * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">  */</span><br><span class="line"> def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)</span><br><span class="line">     : RDD[(K, Iterable[T])] = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   this.map(t =&gt; (cleanF(t), t)).groupByKey(p)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">参数是传入一个分组条件 怎么传呀？不要紧 可以测试</span><br><span class="line">1.传自己进去</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x)</span><br><span class="line">res19: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[55] at groupBy at &lt;console&gt;:25</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x).collect</span><br><span class="line">res20: Array[(String, Iterable[String])] = Array((b,CompactBuffer(b, b)), (a,CompactBuffer(a, a, a)), (c,CompactBuffer(c)))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">那我给一个需求：把上面的字母次数算出来</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x)</span><br><span class="line">res19: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[55] at groupBy at &lt;console&gt;:25</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x).mapValues(x=&gt;x.size).collect</span><br><span class="line">res21: Array[(String, Int)] = Array((b,2), (a,3), (c,1))</span><br><span class="line"></span><br><span class="line">注意：所以 算子都会用 怎么串起来 很重要的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sortBy：自定义排序   你想怎么排序就怎么排序  默认是升序的</span><br><span class="line">/**</span><br><span class="line">   * Return this RDD sorted by the given key function.</span><br><span class="line">   */</span><br><span class="line">  def sortBy[K](</span><br><span class="line">      f: (T) =&gt; K,</span><br><span class="line">      ascending: Boolean = true,</span><br><span class="line">      numPartitions: Int = this.partitions.length)</span><br><span class="line">      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope &#123;</span><br><span class="line">    this.keyBy[K](f)</span><br><span class="line">        .sortByKey(ascending, numPartitions)</span><br><span class="line">        .values</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2)</span><br><span class="line">res22: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[68] at sortBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2).collect</span><br><span class="line">res23: Array[(String, Int)] = Array((老哥,18), (double_happy,30), (娜娜,60))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2,false).collect</span><br><span class="line">res24: Array[(String, Int)] = Array((娜娜,60), (double_happy,30), (老哥,18))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(-_._2).collect</span><br><span class="line">res25: Array[(String, Int)] = Array((娜娜,60), (double_happy,30), (老哥,18))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sortBykey:是按key进行排序的哈 注意和sortby的区别   sortby是自定义排序 非常的灵活</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling</span><br><span class="line">   * `collect` or `save` on the resulting RDD will return or output an ordered list of records</span><br><span class="line">   * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in</span><br><span class="line">   * order of the keys).</span><br><span class="line">   */</span><br><span class="line">  // TODO: this currently doesn&apos;t work on P other than Tuple2!</span><br><span class="line">  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</span><br><span class="line">      : RDD[(K, V)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    val part = new RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">    new ShuffledRDD[K, V, V](self, part)</span><br><span class="line">      .setKeyOrdering(if (ascending) ordering else ordering.reverse)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortByKey().collect</span><br><span class="line">res29: Array[(String, Int)] = Array((double_happy,30), (娜娜,60), (老哥,18))</span><br><span class="line"></span><br><span class="line">如果要求 就是按年龄来排 应该怎么排序：</span><br><span class="line">  反转</span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).map(x=&gt;(x._2,x._1)).sortByKey().collect</span><br><span class="line">res30: Array[(Int, String)] = Array((18,老哥), (30,double_happy), (60,娜娜))</span><br><span class="line"></span><br><span class="line">数据是要和开始格式类似 再转回来就可以了：</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).map(x=&gt;(x._2,x._1)).sortByKey().map(x=&gt;(x._2,x._1)).collect</span><br><span class="line">res31: Array[(String, Int)] = Array((老哥,18), (double_happy,30), (娜娜,60))</span><br></pre></td></tr></table></figure></div>
<h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">join:  默认是内连接</span><br><span class="line">一定是需要条件的，条件就是key的</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each</span><br><span class="line">   * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and</span><br><span class="line">   * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD.</span><br><span class="line">   */</span><br><span class="line">  def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope &#123;</span><br><span class="line">    this.cogroup(other, partitioner).flatMapValues( pair =&gt;</span><br><span class="line">      for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">第一个：名字  第二个：城市  第三个：年龄  (B,(上海,18))</span><br><span class="line"></span><br><span class="line">scala&gt; val j1 = sc.parallelize(List((&quot;A&quot;,&quot;北京&quot;),(&quot;B&quot;,&quot;上海&quot;),(&quot;C&quot;,&quot;杭州&quot;)))</span><br><span class="line">j1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[106] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val j2 = sc.parallelize(List((&quot;A&quot;,&quot;30&quot;),(&quot;B&quot;,&quot;18&quot;),(&quot;D&quot;,&quot;60&quot;)))</span><br><span class="line">j2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[107] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;   j1.join(j2).collect</span><br><span class="line">res32: Array[(String, (String, String))] = Array((B,(上海,18)), (A,(北京,30)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.leftOuterJoin(j2).collect</span><br><span class="line">res33: Array[(String, (String, Option[String]))] = Array((B,(上海,Some(18))), (A,(北京,Some(30))), (C,(杭州,None)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.rightOuterJoin(j2).collect</span><br><span class="line">res34: Array[(String, (Option[String], String))] = Array((B,(Some(上海),18)), (D,(None,60)), (A,(Some(北京),30)))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      * join底层就是使用了cogroup</span><br><span class="line">      * RDD[K,V]</span><br><span class="line">      *</span><br><span class="line">      * 根据key进行关联，返回两边RDD的记录，没关联上的是空</span><br><span class="line">      * join返回值类型  RDD[(K, (Option[V], Option[W]))]      这块参数的类型是option要注意 </span><br><span class="line">      * cogroup返回值类型  RDD[(K, (Iterable[V], Iterable[W]))]</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cogroup:</span><br><span class="line"> /**</span><br><span class="line">   * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the</span><br><span class="line">   * list of values for that key in `this` as well as `other`.</span><br><span class="line">   */</span><br><span class="line">  def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner)</span><br><span class="line">      : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope &#123;</span><br><span class="line">    if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) &#123;</span><br><span class="line">      throw new SparkException(&quot;HashPartitioner cannot partition array keys.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val cg = new CoGroupedRDD[K](Seq(self, other), partitioner)</span><br><span class="line">    cg.mapValues &#123; case Array(vs, w1s) =&gt;</span><br><span class="line">      (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]])</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     j1.fullOuterJoin(j2).collect</span><br><span class="line">res35: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(上海),Some(18))), (D,(None,Some(60))), (A,(Some(北京),Some(30))), (C,(Some(杭州),None)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.cogroup(j2).collect</span><br><span class="line">res36: Array[(String, (Iterable[String], Iterable[String]))] = Array((B,(CompactBuffer(上海),CompactBuffer(18))), (D,(CompactBuffer(),CompactBuffer(60))), (A,(CompactBuffer(北京),CompactBuffer(30))), (C,(CompactBuffer(杭州),CompactBuffer())))</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark003-Action-接着Spark002" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/19/Spark003-Action-%E6%8E%A5%E7%9D%80Spark002/">Spark003--Action  接着Spark002</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/19/Spark003-Action-%E6%8E%A5%E7%9D%80Spark002/" class="article-date">
  <time datetime="2018-01-19T11:59:54.000Z" itemprop="datePublished">2018-01-19</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="回顾上篇文章："><a href="#回顾上篇文章：" class="headerlink" title="回顾上篇文章："></a>回顾上篇文章：</h2><p>RDD:<br>    是什么<br>    五大特性对应五大方法<br>    创建方式：3<br>    操作：2 action &amp; transformation</p>
<p>Spark作业开发流程：<br><img src="https://img-blog.csdnimg.cn/20191022080418772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>也就是：<br>数据源–&gt;经过一堆transformtion–&gt;action 触发spark作业 —&gt;输出到某个地方</p>
<p>你的业务无论多么复杂 都是这样的。</p>
<h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p>（1）collect</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Return an array that contains all of the elements in this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">   * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">   */</span><br><span class="line">  def collect(): Array[T] = withScope &#123;</span><br><span class="line">    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)</span><br><span class="line">    Array.concat(results: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.Return an array that contains all of the elements in this RDD.</span><br><span class="line">2.resulting array is expected to be small</span><br><span class="line">3. the data is loaded into the driver&apos;s memory.</span><br><span class="line">所以生产上你想看这个rdd里的数据 是不太现实的 会导致某种oom的，(oom有好多种的)</span><br><span class="line">如果你还是想看rdd里的元素 该怎么办呢？</span><br><span class="line">两种方法：</span><br><span class="line">1) 取出部分数据</span><br><span class="line">2) 把rdd输出到文件系统</span><br><span class="line">真正生产上使用collect只有一个地方：？？？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure></div>
<p>(2)foreach<br><img src="https://img-blog.csdnimg.cn/20191022081833635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Applies a function f to all elements of this RDD.</span><br><span class="line">  */</span><br><span class="line"> def foreach(f: T =&gt; Unit): Unit = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreach(println)</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>注意：<br>我在spark-shell  –master local[2] 模式下 rdd.foreach(println) 会显示出结果，如果在<br>spark-shell  –master yarn 模式下 rdd.foreach(println) 会显示出结果么？为什么呢？</p>
<p>(3)foreachPartition<br><img src="https://img-blog.csdnimg.cn/20191022082000435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Applies a function f to each partition of this RDD.</span><br><span class="line">  */</span><br><span class="line"> def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreachPartition(println)</span><br><span class="line">non-empty iterator</span><br><span class="line">non-empty iterator</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res5: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">返回的是non-empty iterator 怎么才能把里面的内容输出出来呢？</span><br><span class="line"></span><br><span class="line">如果这样写呢？</span><br><span class="line"> rdd.foreachPartition(paritition =&gt; paritition.map(println))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreachPartition(paritition =&gt; paritition.map(println))</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>能不能想到这是什么问题导致的？<br>foreachPartition(paritition =&gt; paritition.map(println)) 输出结果在正在执行的机器上面是有的<br>而控制台看到的是driver的 </p>
<p>正好引入一个东西：<br>sortBy 上次的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2.sortBy(_._2,false)</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">sortBy是全局排序的还是分区排序的？</span><br><span class="line"></span><br><span class="line">上面的两行代码看仔细了 ， 是两个分区 ,按照降序排</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">同样的代码我再运行一次：</span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>sortBy是全局排序的还是分区排序的？通过上面的测试知道了吗？ 知道个鬼<br>是不是感觉是分区排序</p>
<p>去idea上输出结果看一下：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line"></span><br><span class="line">object ActionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">    rdd2.sortBy(_._2,false).saveAsTextFile(&quot;file:///Users/double_happy/zz/G7-03/工程/scala-spark/doc/out&quot;)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191022085445570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191022085458568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>难道真的是分区排序么？在进行测试。<br><img src="https://img-blog.csdnimg.cn/20191022085757146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>为什么rdd2.sortBy(<em>.</em>2,false).foreach(println)的结果不一样？<br>所以使用foreach在这里根本看不出来sortBy是全局排序还是分区排序</p>
<p><strong>因为 rdd2是两个分区的 ，foreach执行的时候 不确定是哪个task先println 出来 明白吗？</strong></p>
<p><strong>所以sortBy 到底是什么排序？</strong><br>全局排序      你看idea里的 </p>
<p>所以你测试的时候 sortBy 后面不能跟着 foreach 来测试 要输出文件</p>
<p>通过 读取文件  来测试 </p>
<p>(3)count</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Return the number of elements in the RDD.</span><br><span class="line"> */</span><br><span class="line">def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(4) reduce   两两做操作</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce(_+_)</span><br><span class="line">res6: Int = 15</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(5) first</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return the first element in this RDD.</span><br><span class="line">  */</span><br><span class="line"> def first(): T = withScope &#123;</span><br><span class="line">   take(1) match &#123;</span><br><span class="line">     case Array(t) =&gt; t</span><br><span class="line">     case _ =&gt; throw new UnsupportedOperationException(&quot;empty collection&quot;)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>(6)take</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Take the first num elements of the RDD. It works by first scanning one partition, and use the</span><br><span class="line">  * results from that partition to estimate the number of additional partitions needed to satisfy</span><br><span class="line">  * the limit.</span><br><span class="line">  *</span><br><span class="line">  * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">  * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">  *</span><br><span class="line">  * @note Due to complications in the internal implementation, this method will raise</span><br><span class="line">  * an exception if called on an RDD of `Nothing` or `Null`.</span><br><span class="line">  */</span><br><span class="line"> def take(num: Int): Array[T] = withScope &#123;</span><br><span class="line">   val scaleUpFactor = Math.max(conf.getInt(&quot;spark.rdd.limit.scaleUpFactor&quot;, 4), 2)</span><br><span class="line">   if (num == 0) &#123;</span><br><span class="line">     new Array[T](0)</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     val buf = new ArrayBuffer[T]</span><br><span class="line">     val totalParts = this.partitions.length</span><br><span class="line">     var partsScanned = 0</span><br><span class="line">     while (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123;</span><br><span class="line">       // The number of partitions to try in this iteration. It is ok for this number to be</span><br><span class="line">       // greater than totalParts because we actually cap it at totalParts in runJob.</span><br><span class="line">       var numPartsToTry = 1L</span><br><span class="line">       val left = num - buf.size</span><br><span class="line">       if (partsScanned &gt; 0) &#123;</span><br><span class="line">         // If we didn&apos;t find any rows after the previous iteration, quadruple and retry.</span><br><span class="line">         // Otherwise, interpolate the number of partitions we need to try, but overestimate</span><br><span class="line">         // it by 50%. We also cap the estimation in the end.</span><br><span class="line">         if (buf.isEmpty) &#123;</span><br><span class="line">           numPartsToTry = partsScanned * scaleUpFactor</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">           // As left &gt; 0, numPartsToTry is always &gt;= 1</span><br><span class="line">           numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt</span><br><span class="line">           numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)</span><br><span class="line">       val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p)</span><br><span class="line"></span><br><span class="line">       res.foreach(buf ++= _.take(num - buf.size))</span><br><span class="line">       partsScanned += p.size</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     buf.toArray</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>first底层调用take方法</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce(_+_)</span><br><span class="line">res6: Int = 15</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first</span><br><span class="line">res7: Int = 1</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(2)</span><br><span class="line">res8: Array[Int] = Array(1, 2)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(7) top<br>里面肯定是做了排序的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Returns the top k (largest) elements from this RDD as defined by the specified</span><br><span class="line">   * implicit Ordering[T] and maintains the ordering. This does the opposite of</span><br><span class="line">   * [[takeOrdered]]. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)</span><br><span class="line">   *   // returns Array(12)</span><br><span class="line">   *</span><br><span class="line">   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)</span><br><span class="line">   *   // returns Array(6, 5)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">   * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">   *</span><br><span class="line">   * @param num k, the number of top elements to return</span><br><span class="line">   * @param ord the implicit ordering for T</span><br><span class="line">   * @return an array of top elements</span><br><span class="line">   */</span><br><span class="line">  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123;</span><br><span class="line">    takeOrdered(num)(ord.reverse)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1. This does the opposite of</span><br><span class="line">   * [[takeOrdered]].</span><br><span class="line"></span><br><span class="line">2.top 底层调用的是 takeOrdered</span><br><span class="line"></span><br><span class="line">3.top 柯里化的 Ordering 看scala篇这部分 讲的很详细</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.top(2)</span><br><span class="line">res9: Array[Int] = Array(5, 4)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeOrdered(2)</span><br><span class="line">res10: Array[Int] = Array(1, 2)</span><br></pre></td></tr></table></figure></div>

<p>(8)zipWithIndex</p>
<p>给你一个算子 你怎么知道他是 action还是 transformation？？</p>
<p><strong>action算子里面是有sc.runJob()方法的</strong>  </p>
<p>eg：<br><img src="https://img-blog.csdnimg.cn/20191022092526573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>所以zipWithIndex 它不是action算子</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Zips this RDD with its element indices. The ordering is first based on the partition index</span><br><span class="line">  * and then the ordering of items within each partition. So the first item in the first</span><br><span class="line">  * partition gets index 0, and the last item in the last partition receives the largest index.</span><br><span class="line">  *</span><br><span class="line">  * This is similar to Scala&apos;s zipWithIndex but it uses Long instead of Int as the index type.</span><br><span class="line">  * This method needs to trigger a spark job when this RDD contains more than one partitions.</span><br><span class="line">  *</span><br><span class="line">  * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of</span><br><span class="line">  * elements in a partition. The index assigned to each element is therefore not guaranteed,</span><br><span class="line">  * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee</span><br><span class="line">  * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.</span><br><span class="line">  */</span><br><span class="line"> def zipWithIndex(): RDD[(T, Long)] = withScope &#123;</span><br><span class="line">   new ZippedWithIndexRDD(this)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex</span><br><span class="line">res11: org.apache.spark.rdd.RDD[(Int, Long)] = ZippedWithIndexRDD[31] at zipWithIndex at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex.collect</span><br><span class="line">res12: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4))</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(9)countByKey</p>
<p>这是action算子</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Count the number of elements for each key, collecting the results to a local Map.</span><br><span class="line">  *</span><br><span class="line">  * @note This method should only be used if the resulting map is expected to be small, as</span><br><span class="line">  * the whole thing is loaded into the driver&apos;s memory.</span><br><span class="line">  * To handle very large results, consider using rdd.mapValues(_ =&gt; 1L).reduceByKey(_ + _), which</span><br><span class="line">  * returns an RDD[T, Long] instead of a map.</span><br><span class="line">  */</span><br><span class="line"> def countByKey(): Map[K, Long] = self.withScope &#123;</span><br><span class="line">   self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>(10)collectAsMap  针对kv类型的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return the key-value pairs in this RDD to the master as a Map.</span><br><span class="line">  *</span><br><span class="line">  * Warning: this doesn&apos;t return a multimap (so if you have multiple values to the same key, only</span><br><span class="line">  *          one value per key is preserved in the map returned)</span><br><span class="line">  *</span><br><span class="line">  * @note this method should only be used if the resulting data is expected to be small, as</span><br><span class="line">  * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">  */</span><br><span class="line"> def collectAsMap(): Map[K, V] = self.withScope &#123;</span><br><span class="line">   val data = self.collect()</span><br><span class="line">   val map = new mutable.HashMap[K, V]</span><br><span class="line">   map.sizeHint(data.length)</span><br><span class="line">   data.foreach &#123; pair =&gt; map.put(pair._1, pair._2) &#125;</span><br><span class="line">   map</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex.collect</span><br><span class="line">res12: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex().countByKey()</span><br><span class="line">res13: scala.collection.Map[Int,Long] = Map(5 -&gt; 1, 1 -&gt; 1, 2 -&gt; 1, 3 -&gt; 1, 4 -&gt; 1)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex().collectAsMap()</span><br><span class="line">res14: scala.collection.Map[Int,Long] = Map(2 -&gt; 1, 5 -&gt; 4, 4 -&gt; 3, 1 -&gt; 0, 3 -&gt; 2)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<p>Action算子官网：<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" target="_blank" rel="noopener">Action 算子</a></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark001-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/15/Spark001-double-happy/">Spark001--double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/15/Spark001-double-happy/" class="article-date">
  <time datetime="2018-01-15T11:57:42.000Z" itemprop="datePublished">2018-01-15</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="Speed"><a href="#Speed" class="headerlink" title="Speed"></a>Speed</h2><p>Spark是支持pipline操作的，根据Shufle进行切分的，中间的过程是不落地的。<br>运行的角度来说：<br>    线程的<br>    mapreduce是进程的   map task 、reduce task</p>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>1.Represents an immutable,<br>   partitioned collection of elements that can be operated on in parallel.<br>2. </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">5大特性： 弹性分布式数据集</span><br><span class="line">	1）一系列的partition     分区里是有index的</span><br><span class="line">      protected def getPartitions: Array[Partition]</span><br><span class="line"></span><br><span class="line">解释：</span><br><span class="line">	 *  - A list of partitions</span><br><span class="line">	 *  - A function for computing each split</span><br><span class="line"> scala中 List（1，2，3，4).map(_*2)   scala中的list是单机的</span><br><span class="line"> 而RDD中 数据是分区的，如果rdd.map(_*2)是对每个分区里的元素做计算 是 分布式的</span><br><span class="line"></span><br><span class="line">    2）针对RDD做操作其实就是针对RDD底层的partition进行操作</span><br><span class="line">    rdd.map(_*2)</span><br><span class="line">    def compute(split: Partition, context: TaskContext): Iterator[T]</span><br><span class="line"></span><br><span class="line">	3）rdd之间的依赖（血缘关系）</span><br><span class="line">      protected def getDependencies: Seq[Dependency[_]] = deps</span><br><span class="line"></span><br><span class="line">	4）partitioner（针对 kv类型的rdd）</span><br><span class="line">      @transient val partitioner: Option[Partitioner] = None</span><br><span class="line"></span><br><span class="line">	5）locations （优先把作业调度到数据所在节点）</span><br><span class="line">      protected def getPreferredLocations(split: Partition): Seq[String] = Nil</span><br><span class="line">	好处是 如果你的数据不在这个节点上 优先把作业调度到数据所在节点 好处是 直接本地读数据就可以了</span><br><span class="line">	理想化状态。</span><br><span class="line">	 也有 作业调度在别的节点上 数据在另一台节点上，那么 只能把数据通过网络把数据传到 作业调度的节点上去，进行计算。那么5这个特性就是减少网络数据传输。</span><br></pre></td></tr></table></figure></div>
<h2 id="程序开发入口"><a href="#程序开发入口" class="headerlink" title="程序开发入口"></a>程序开发入口</h2><p>开发Spark应用程序<br>    1）SparkConf<br>        appName<br>        master<br>    2）SparkContext(sparkConf)<br>    3）spark-shell –master local[2] 底层自动为我们创建了SparkContext sc</p>
<p><img src="https://img-blog.csdnimg.cn/2019092812275919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190928122827724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RDD:创建</span><br><span class="line"> parallelize :</span><br><span class="line"> 	sc.parallelize(List(1,2,3,4))</span><br><span class="line"> textFile:</span><br><span class="line"> 	sc.textFile(path)</span><br><span class="line"> 通过RDD转换生成的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">collect   collectAsync</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3, 4)                                            </span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1 = sc.textFile(&quot;file:///home/sxwang/data&quot;)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[String] = file:///home/sxwang/data MapPartitionsRDD[2] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect</span><br><span class="line">res1: Array[String] = Array(spark       flink   hadoop, spark   kafka   scala)</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = rdd.map(_*2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">collect   collectAsync</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res2: Array[Int] = Array(2, 4, 6, 8)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.toDebugString</span><br><span class="line">res3: String =</span><br><span class="line">(2) MapPartitionsRDD[3] at map at &lt;console&gt;:26 []</span><br><span class="line"> |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br></pre></td></tr></table></figure></div>

<h2 id="并行度-–简单版"><a href="#并行度-–简单版" class="headerlink" title="并行度 –简单版"></a>并行度 –简单版</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd</span><br><span class="line">res7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res8: Array[Int] = Array(1, 2, 3, 4)</span><br></pre></td></tr></table></figure></div>
<p>产看ui界面：<br><img src="https://img-blog.csdnimg.cn/20190928124416679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为什么是2呢？<br>查看源码：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def parallelize[T: ClassTag](</span><br><span class="line">    seq: Seq[T],</span><br><span class="line">    numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">defaultParallelism：</span><br><span class="line">	</span><br><span class="line">	 /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */</span><br><span class="line">  def defaultParallelism: Int = &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    taskScheduler.defaultParallelism</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p>看他的实现的<br><img src="https://img-blog.csdnimg.cn/20190928124821226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">taskScheduler.defaultParallelism：</span><br><span class="line">	override def defaultParallelism(): Int = backend.defaultParallelism()</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019092812500940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">查看local的点进去：</span><br><span class="line">  override def defaultParallelism(): Int =</span><br><span class="line">    scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)</span><br><span class="line">默认从配置文件里去  ，但是我们没有设置，</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Used when running a local version of Spark where the executor, backend, and master all run in</span><br><span class="line"> * the same JVM. It sits behind a [[TaskSchedulerImpl]] and handles launching tasks on a single</span><br><span class="line"> * Executor (created by the [[LocalSchedulerBackend]]) running locally.</span><br><span class="line"> */</span><br><span class="line">private[spark] class LocalSchedulerBackend(</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    scheduler: TaskSchedulerImpl,</span><br><span class="line">    val totalCores: Int)</span><br><span class="line">  extends SchedulerBackend with ExecutorBackend with Logging &#123;</span><br><span class="line"></span><br><span class="line">totalCores 说明是我们构建的时候传进来的 </span><br><span class="line"></span><br><span class="line">可以看：</span><br><span class="line">	scala&gt; sc.parallelize(List(1,2,3,4),3).collect</span><br><span class="line">res9: Array[Int] = Array(1, 2, 3, 4)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20190928125348377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Hadoop项目流程：Hive2MySQL-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/03/Hadoop%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%EF%BC%9AHive2MySQL-double-happy/">Hadoop项目流程：Hive2MySQL--double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/03/Hadoop%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%EF%BC%9AHive2MySQL-double-happy/" class="article-date">
  <time datetime="2018-01-03T11:55:22.000Z" itemprop="datePublished">2018-01-03</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="Hive统计表"><a href="#Hive统计表" class="headerlink" title="Hive统计表"></a>Hive统计表</h2><p>不管使用Hive统计的结果是什么维度，统计的结果表选择：<br>    1.external<br>    2.partition<br>    就是分区+外部表</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">平台报表别人如何访问呢：</span><br><span class="line">	1.使用hiveservie2 使用jdbc方式去连接     可以    生产上不这么用</span><br><span class="line">	2.最好把数据倒入mysql里面   *****  那么下面就是基于这个思路进行讲解</span><br><span class="line"></span><br><span class="line">Hdfs --》Mysql  这里使用Sqoop     到Spark那块就不用Sqoop了 太low了</span><br></pre></td></tr></table></figure></div>
<h2 id="使用Sqoop把Hive里数据倒入MySQL-注意的点"><a href="#使用Sqoop把Hive里数据倒入MySQL-注意的点" class="headerlink" title="使用Sqoop把Hive里数据倒入MySQL 注意的点"></a>使用Sqoop把Hive里数据倒入MySQL 注意的点</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">Hive里表的设计 ：分区字段 要在Hive建表的时候 多一个字段 代表分区字段</span><br><span class="line">	hdfs上的数据是三个字段不算分区的 mySQL里面要对应上 </span><br><span class="line">	分区字段不能作为最终字段来使用的在mysql中（sqoop）</span><br><span class="line">	所以在Sqoop的时候要指定上字段而且还要指定分割符</span><br><span class="line">	</span><br><span class="line">	对于hive来讲默认的分割符是\001</span><br><span class="line">	不然会报错 如下：	</span><br><span class="line">	解决办法就是：</span><br><span class="line">		Hive表 + Sqoop  使用</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20190923164039631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MySQL里表：需要Hive表里的哪几个字段就创建什么样的表</span><br><span class="line">create table  platform_stat(</span><br><span class="line">platform  varchar(20),</span><br><span class="line">cnt  int,</span><br><span class="line">d varchar(8)</span><br><span class="line">)engine=innodb default charset=utf8;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Hive里表设计：</span><br><span class="line">create table dwd_platform_stat_info(</span><br><span class="line">platform string,</span><br><span class="line">cnt int,</span><br><span class="line">d string</span><br><span class="line">) partitioned by(day string)</span><br><span class="line">location &apos;/hadoop/project/platform_stat&apos;;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Sqoop使用：Sqoop 在什么地方执行 那么 能够地方会有一个java文件生成</span><br><span class="line"></span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/hive_dwd \</span><br><span class="line">--password wsx123$%^ \</span><br><span class="line">--username root \</span><br><span class="line">--mapreduce-job-name  Platform_info_Hive2MySQL \</span><br><span class="line">--columns &quot;platform,cnt,d&quot;   \</span><br><span class="line">--input-fields-terminated-by &apos;\001&apos; \</span><br><span class="line">--table platform_stat \</span><br><span class="line">--export-dir /hadoop/project/platform_stat/day=20190921</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">查看MySQL表结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20190923164443832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">脚本封装：</span><br><span class="line">	#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ]; then</span><br><span class="line">    time=$1</span><br><span class="line">else</span><br><span class="line">    time=`date -d &quot;yesterday&quot; +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#platforma_state 数据统计</span><br><span class="line">hive -e &quot;</span><br><span class="line"></span><br><span class="line">use homework;</span><br><span class="line">insert overwrite table dwd_platform_stat_info partition(day=$&#123;time&#125;)</span><br><span class="line">select platform , count(1) ,day as d  from access_wide where day =&apos;$&#123;time&#125;&apos;  group by platform,day ;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">#Sqoop platforma_state_Hive2MySQL </span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/hive_dwd \</span><br><span class="line">--password wsx123$%^ \</span><br><span class="line">--username root \</span><br><span class="line">--mapreduce-job-name  Platform_info_Hive2MySQL \</span><br><span class="line">--columns &quot;platform,cnt,d&quot;   \</span><br><span class="line">--input-fields-terminated-by &apos;\001&apos; \ </span><br><span class="line">--table platform_stat \</span><br><span class="line">--export-dir /hadoop/project/platform_stat/day=$&#123;time&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">存在的问题：</span><br><span class="line">	按上面的脚本走 如果多次导入同一天的数据</span><br><span class="line">	MySQL里面会有重复数据的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20190923165037161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>数据是有重复的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">解决MySQL数据重复：</span><br><span class="line">	这块只要把上次导入的日期数据删掉就可以了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">脚本封装：</span><br><span class="line">	#!/bin/bash</span><br><span class="line"></span><br><span class="line">if [ $# -eq 1 ]; then</span><br><span class="line">    time=$1</span><br><span class="line">else</span><br><span class="line">    time=`date -d &quot;yesterday&quot; +%Y%m%d`</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#platforma_state 数据统计</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">echo &quot;step  : hive -e ---&gt;  insert data to platforma_state table&quot;</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">hive -e &quot;</span><br><span class="line"></span><br><span class="line">use homework;</span><br><span class="line">insert overwrite table dwd_platform_stat_info partition(day=$&#123;time&#125;)</span><br><span class="line">select platform , count(1) ,day as d  from access_wide  where day =&apos;$&#123;time&#125;&apos; group by platform,day ;</span><br><span class="line">&quot;</span><br><span class="line"></span><br><span class="line">#登陆MySQL 删除上次 处理日期 表中数据</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">echo &quot;step : MySQL ---&gt; delete  last time process $&#123;time&#125; data&quot;</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line"></span><br><span class="line">MySQL_USER=&apos;root&apos;</span><br><span class="line">MySQL_PASSWD=&apos;wsx123$%^&apos;</span><br><span class="line"></span><br><span class="line">SQL_RESULT=`</span><br><span class="line">mysql \</span><br><span class="line">--user=&quot;$&#123;MySQL_USER&#125;&quot; \</span><br><span class="line">--password=&quot;$&#123;MySQL_PASSWD&#125;&quot;  \</span><br><span class="line">-e &quot;select count(1) as cnt from hive_dwd.platform_stat where d = $&#123;time&#125;;&quot; | tail -1</span><br><span class="line">`</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">echo &quot;MySQL last time $&#123;time&#125; data size ：$&#123;SQL_RESULT&#125;&quot;</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">if [ $&#123;SQL_RESULT&#125; -ne 0 ] ; then</span><br><span class="line">    mysql --user=&quot;$&#123;MySQL_USER&#125;&quot; --password=&quot;$&#123;MySQL_PASSWD&#125;&quot;  \</span><br><span class="line">    -e &quot;delete from hive_dwd.platform_stat where d = $&#123;time&#125;;&quot;</span><br><span class="line">    echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">    echo &quot;Delete Mysql last time  $&#123;time&#125; data done ,next step do Sqoop : Hive 2 MySQL&quot;</span><br><span class="line">    echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Sqoop platforma_state_Hive2MySQL </span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">echo &quot;step : sqoop --&gt; Hive 2 MySQL &quot;</span><br><span class="line">echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://localhost:3306/hive_dwd \</span><br><span class="line">--password wsx123$%^ \</span><br><span class="line">--username root \</span><br><span class="line">--mapreduce-job-name  Platform_info_Hive2MySQL \</span><br><span class="line">--columns &quot;platform,cnt,d&quot;   \</span><br><span class="line">--input-fields-terminated-by &apos;\001&apos; \</span><br><span class="line">--table platform_stat \</span><br><span class="line">--export-dir /hadoop/project/platform_stat/day=$&#123;time&#125; </span><br><span class="line"></span><br><span class="line">error=$?</span><br><span class="line">if [ $error == 0 ] ; then</span><br><span class="line"> echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line"> echo &quot;$&#123;time&#125; work suceess&quot;</span><br><span class="line"> echo &quot;-------------------------------------------------------------------------------------------&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure></div>
<h2 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h2><p>下一个博客Azkaban讲解</p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Hadoop压缩-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/01/Hadoop%E5%8E%8B%E7%BC%A9-double-happy/">Hadoop压缩---double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/01/Hadoop%E5%8E%8B%E7%BC%A9-double-happy/" class="article-date">
  <time datetime="2018-01-01T11:54:32.000Z" itemprop="datePublished">2018-01-01</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="调优点："><a href="#调优点：" class="headerlink" title="调优点："></a>调优点：</h2><p>为什么要使用压缩呢？</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.节省空间 （数据在hdfs上以3副本存储 如果采用压缩 占用空间会少一些）</span><br><span class="line">2.时间：网络io 和 磁盘io 会减少 </span><br><span class="line">  （mapreduce过程中 map端输出采用压缩和不采用压缩效果很明显）</span><br><span class="line">  2.1 map端到reduce端会经过shuffle 如果map端采用压缩那么 map端数据传到reduce端过程中</span><br><span class="line">     数据压缩后体积会变小，那么经过网络传输的数据会变少 减少网络io</span><br><span class="line">     因为要经过网络传输，需要从磁盘读到内存 磁盘上的数据压缩后 读取到内存的数据体积</span><br><span class="line">     也会变小  所以也减少磁盘io</span><br><span class="line">  这样传输的时间也会减少很多，所以有必要进行压缩。</span><br></pre></td></tr></table></figure></div>
<p>但是注意的是如果采用压缩，对机器的cpu的要求高，所以压缩的使用场景</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.存储数据的空间不够</span><br><span class="line">2.机器的core要足够</span><br></pre></td></tr></table></figure></div>
<p>如果core不够还采用压缩，那么还是别采用压缩啦。</p>
<h2 id="压缩的技术"><a href="#压缩的技术" class="headerlink" title="压缩的技术"></a>压缩的技术</h2><p>有损压缩(lossy compression) :  适用于图片和视频  允许丢失几帧<br>无损压缩(lossless compression):原始数据解压缩数据是没有丢失的</p>
<p>对称和非对称：就是压缩和解压的时间相同叫对称 ，反义。</p>
<h2 id="压缩的使用场景结合mapreduce"><a href="#压缩的使用场景结合mapreduce" class="headerlink" title="压缩的使用场景结合mapreduce"></a>压缩的使用场景结合mapreduce</h2><p>数据压缩  map端输出可以用，reduce端输出也可以使用</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input   </span><br><span class="line">  因为这块 map读取数据的时候的inputformat默认会识别数据输入采用什么格式的压缩获取codec（</span><br><span class="line">     textinputformat源码里有）</span><br><span class="line">map out         配个参数就可以</span><br><span class="line">reduce out    配个参数就可以</span><br></pre></td></tr></table></figure></div>
<p>spark、flink同样的</p>
<h2 id="凡事都有两面性"><a href="#凡事都有两面性" class="headerlink" title="凡事都有两面性"></a>凡事都有两面性</h2><p>空间和时间  ok<br>cpu   耗费     cpu的利用率会高  而且整个作业的处理时长会略微长一些</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用的压缩：</span><br><span class="line">	有个解压缩过程 所以整个作业时间会略微长</span><br><span class="line"></span><br><span class="line">所以为了减少空间和网络磁盘io传输时间 cpu的耗费以及作业的时长会变长</span><br></pre></td></tr></table></figure></div>
<h2 id="常见的压缩格式"><a href="#常见的压缩格式" class="headerlink" title="常见的压缩格式"></a>常见的压缩格式</h2><p><img src="https://img-blog.csdnimg.cn/20190923100126703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>还有LZ4</p>
<h2 id="如何选择呢，这么多压缩的格式-压缩比和解压缩度"><a href="#如何选择呢，这么多压缩的格式-压缩比和解压缩度" class="headerlink" title="如何选择呢，这么多压缩的格式 压缩比和解压缩度"></a>如何选择呢，这么多压缩的格式 压缩比和解压缩度</h2><p><img src="https://img-blog.csdnimg.cn/20190923100600403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>相同配置的机器测试看看 </p>
<p>压缩比：压缩前和压缩后的比值</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">压缩比    Bzip2 30%   Gzip (两者之间)  ,snappy \lzo50%</span><br><span class="line">解压速度    反过来</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019092310115462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="压缩能否分片"><a href="#压缩能否分片" class="headerlink" title="压缩能否分片"></a>压缩能否分片</h2><p>hadoop作业是io密集型的，所以他的作业尽可能的采用压缩<br>spark、flink作业是pipline型的</p>
<p>注意：压缩又的是java写的，有的是native的，<br>所以你要在Hadoop里使用LZO(native的) 需要下载一些native的依赖</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Splitable：</span><br><span class="line">	一个文件相当于一个map task来处理，</span><br><span class="line">	1.假设一个5G的文件，不能使用分割的，也就意味着这个文件只能使用一个</span><br><span class="line">	maptask来处理，如果这个能分割，5G拆成10分 会采用10个maptask来处理</span><br><span class="line">	并行处理。5*1024/10 = 一个maptask处理的数据量。</span><br><span class="line"></span><br><span class="line">是否能够分割就决定了你的一个maptask处理的数据量有多少，</span><br><span class="line">如果能够分割就可以多个maptask并行处理</span><br></pre></td></tr></table></figure></div>

<h2 id="压缩是否支持分割"><a href="#压缩是否支持分割" class="headerlink" title="压缩是否支持分割"></a>压缩是否支持分割</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">分割：  注意是压缩过后的压缩文件是否支持分割的</span><br><span class="line">gzip    不可分割</span><br><span class="line">bzip2  可分割</span><br><span class="line">LZo    带索引可以分割（默认是不支持分割的）</span><br><span class="line">Snappy 不可分割</span><br></pre></td></tr></table></figure></div>
<p>是否能分割对使用哪个压缩有很大的影响意义</p>
<p><img src="https://img-blog.csdnimg.cn/20190923102928382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上图三个部分使用压缩：mapreduce的流程使用压缩的部分</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input：</span><br><span class="line">map out</span><br><span class="line">reduce out</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20190923103255281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">三个地方使用的压缩推荐：</span><br><span class="line">	input：</span><br><span class="line">		Bzip2(支持分割 读一个文件 支持分割会多个并行的maptask进行处理) 数据量特别大 如果不支持压缩</span><br><span class="line">		就会有一个maptask进行处理，性能很低。</span><br><span class="line">	</span><br><span class="line">	mapout：</span><br><span class="line">		shuffle过程要选择一个解压速度更快的压缩</span><br><span class="line">		因为每个maptask输出数据写到磁盘上之后经过网络io</span><br><span class="line">		没有必要采用压缩比高的，之后到reducetask这过程中是采用分片和不分片这块不重要了已经</span><br><span class="line">		因为maptask进来之前是一个大文件拆成多个maptask来处理</span><br><span class="line">		到reduce这个过程中 难道你还需要拆么？不需要，所以这块最重要的是解压速度</span><br><span class="line">	reduceout：</span><br><span class="line">		1.高的压缩比节省空间（使用于归档文件）</span><br><span class="line">		2.作为下一个map的输入呢？应该采用什么压缩方式，我会选择Bzip2或者LZO带索引的(支持分片)</span><br></pre></td></tr></table></figure></div>
<h2 id="MapReduce作业使用压缩实战"><a href="#MapReduce作业使用压缩实战" class="headerlink" title="MapReduce作业使用压缩实战"></a>MapReduce作业使用压缩实战</h2><p>在Hadoop的core-site.xml里配置压缩 ，mapreduce-site.xml配置你采用压缩的位置(map的输出和reduce的输出)</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">core-site.xml:</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;</span><br><span class="line">  org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">  org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">  org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">  org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">  com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">  com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mapreduce-site.xml:</span><br><span class="line">	 &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></div>
<p>测试：<br>    wc<br>    hdfs最后生成的结果是以bzip结尾的</p>
<p><img src="https://img-blog.csdnimg.cn/20190923112832168.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Hive的压缩使用"><a href="#Hive的压缩使用" class="headerlink" title="Hive的压缩使用"></a>Hive的压缩使用</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.创建表：</span><br><span class="line">CREATE EXTERNAL TABLE `ods_uid_pid_info_compression_test`(</span><br><span class="line">`uid` string, </span><br><span class="line">`pid` string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/home/double_happy/data/user_pid.txt&apos; overwrite into table ods_uid_pid_info_compression_test;</span><br><span class="line"></span><br><span class="line">2.去hdfs上查看这数据</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20190923113549154.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">使用压缩：</span><br><span class="line">	hive客户端里：</span><br><span class="line">	set hive.exec.compress.output=true;</span><br><span class="line">	set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20190923113944539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>查看hdfs上数据大小：<br><img src="https://img-blog.csdnimg.cn/20190923114026402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以对比一下 数据小了。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">	hive里设置压缩不建议直接在hive-site.xml里面配置，那是全局的，</span><br><span class="line">	建议还是在使用的时候用命令的方式</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SQL-01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/20/SQL-01/">SQL-01</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2017/12/20/SQL-01/" class="article-date">
  <time datetime="2017-12-20T11:53:13.000Z" itemprop="datePublished">2017-12-20</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="SQL-第一题：聚合函数形式的题"><a href="#SQL-第一题：聚合函数形式的题" class="headerlink" title="SQL-第一题：聚合函数形式的题"></a>SQL-第一题：聚合函数形式的题</h2><p>先讨论一下最基础的东西在hivesql里，比较晦涩，但是真的很好用哦。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">谈谈个人理解：</span><br><span class="line"> 在sql里两个核心的东西：（1）group ：核心是聚合（2）join ：的核心是要join哪些列 </span><br><span class="line"> 注意：group by 和 over在一个select里不能同时使用的哈</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们为什么要聚合呢？</span><br><span class="line">eg：一个表 表示三个班级的学生，有名称，分数，班级，如果不聚合 </span><br><span class="line">我能不能这样写呢 </span><br><span class="line">select name , score from student group by class  </span><br><span class="line">不能 为什么不能？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">首先我的name,score 在groupby的时候我不知道取哪一个，</span><br><span class="line">groupby class 就相当于在所有的列名里面取项，你不知道取哪个name，哪个score，</span><br><span class="line">所以取得项要么出现在groupby 后面，</span><br><span class="line">要么取得项是一个聚合列（eg：sum（score）代表每个班的所有得分数） ，</span><br><span class="line">这个聚合列就用到聚合函数udaf函数。</span><br></pre></td></tr></table></figure></div>
<p>案例1 ：【聚合函数形式的题】</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eg:求每一个班级得分最高的学生得姓名+分数+班级？</span><br><span class="line"> 解决方法：</span><br><span class="line"> 1.sql -GroupBy +Join</span><br><span class="line"> 2.RDD</span><br><span class="line"> 3.sql -开窗</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开窗函数：就是1对多 ，1就是partition by 谁 ,产生了多个数（产生了一个新列存这多个数） </span><br><span class="line">group by ：是多对1，把多个合并成一个</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1.group by + join</span><br><span class="line">1）子表a ： select class,max(score)  score from student group by class </span><br><span class="line">    （1）中不能有name 想完成需求只能子表与原表join形成一个大表，利用大表添加name</span><br><span class="line">2）</span><br><span class="line">    select b.name, a.class,a.score from student b </span><br><span class="line">join </span><br><span class="line">    (select class,max(score) score from student group by class ) a</span><br><span class="line">on </span><br><span class="line"> b.class =a.class and a.score = b.score</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2.开窗函数 </span><br><span class="line">开窗函数：结合上面的问题，我在表后面再加一列，加什么列呢，首先第一个我是按照班级先分组，</span><br><span class="line">班级里的分数从高到低做一个排序，把排序的名次 作为要加的列。</span><br><span class="line">这样就不用像上面那样group by，直接select 那个列就能取到值。这个就叫开窗函数。</span><br><span class="line">   select name,class,score,rank() over(partition by class order by score desc)  rank from student </span><br><span class="line">这个语句就是为了产生最后一列 。 over就叫开窗函数 ，这个over里面怎么开的窗呢？</span><br><span class="line">partition by 就是先分组（就是以什么开窗，相当于 再某一个class里面我用一次rank()，在另一个class里面 </span><br><span class="line">我在用一次rank（）， rank()的应用前提是 order by 某个东西,rank()之后，这样就产生了最后一列 名字叫rank </span><br><span class="line">这个语句执行之后产生了一个新的表，这个表多了一列叫rank。假如这个表叫aa</span><br><span class="line">我产生完表aa之后，直接就</span><br><span class="line"> select name,class,score from aa where rank =1   就完成需求</span><br></pre></td></tr></table></figure></div>
<p>开窗函数里不止只有Rank()函数可用 ，它有很多哈<br><img src="https://img-blog.csdnimg.cn/20190917102337546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTU1NzUxMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="SQL题01"><a href="#SQL题01" class="headerlink" title="SQL题01"></a>SQL题01</h2><p>先讲思路–&gt;再演示结果</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">SQL1： ods_domain_traffic_info这个表</span><br><span class="line">domain           time     traffic(T)</span><br><span class="line">gifshow.com   2019/01/01    5</span><br><span class="line">yy.com        2019/01/01    4</span><br><span class="line">huya.com      2019/01/01    1</span><br><span class="line">gifshow.com   2019/01/20    6</span><br><span class="line">gifshow.com   2019/02/01    8</span><br><span class="line">yy.com        2019/01/20    5</span><br><span class="line">gifshow.com   2019/02/02    7</span><br><span class="line">需求：统计每个用户的累计访问量   一个SQL搞定</span><br><span class="line">结果如下：</span><br><span class="line">domain          month     traffics   totals</span><br><span class="line">gifshow.com     2019-01      11         11</span><br><span class="line">gifshow.com     2019-02      15         26</span><br><span class="line">yy.com          2019-01       9         9</span><br><span class="line">huya.com        2019-01       1         1</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">思路：</span><br><span class="line">	结果要求的是 每个domain 每个月的 traffics  和 totals</span><br><span class="line">分两步：</span><br><span class="line">1.domain + month + traffics</span><br><span class="line">a.</span><br><span class="line">month : time 截取获得</span><br><span class="line">traffics : sum(traffic)</span><br><span class="line">b.</span><br><span class="line">group by(domain,month ) +sum(traffic) ===&gt; 拿到domain   month    traffics</span><br><span class="line"></span><br><span class="line">2.拿到domain   month    traffics 目的是 domain   month    traffics  totals</span><br><span class="line"></span><br><span class="line">a.新生成了一个列 totals  先想到 开窗函数 over()</span><br><span class="line">	partition by  谁呢？ order by 谁呢？</span><br><span class="line">	基于给的结果知道  partition by domain   order by  month     over()前面选择 sum(traffics) </span><br><span class="line">这样就ok了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">我写sql思路过程：</span><br><span class="line">1.每个domain 每个month的总量  ==》    domain   month  traffics</span><br><span class="line">tmp :</span><br><span class="line">	select </span><br><span class="line">		domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, </span><br><span class="line">		sum(traffic) as traffics</span><br><span class="line">	from ods_domain_traffic_info</span><br><span class="line">	group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">	</span><br><span class="line">2.目的是totals 生成了一个新列（1对多） 用开窗函数 ，以domain分组以month排序 之后用sum</span><br><span class="line">result:</span><br><span class="line">	select</span><br><span class="line">		domain,month,traffics,</span><br><span class="line">		sum(traffics)over(partition by domain order by month) as totals</span><br><span class="line">	from  tmp;</span><br><span class="line"></span><br><span class="line">整合：</span><br><span class="line">	select</span><br><span class="line">		domain,month,traffics,</span><br><span class="line">		sum(traffics)over(partition by domain order by month) as totals</span><br><span class="line">	from(</span><br><span class="line">	select </span><br><span class="line">		domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, </span><br><span class="line">		sum(traffic) as traffics</span><br><span class="line">	from ods_domain_traffic_info</span><br><span class="line">	group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">	) as tmp;</span><br><span class="line"></span><br><span class="line">注意哈：我写博客为了好看 使用了 tab建 ，如何你想测试的话 把sql中的tab 地方去掉哈 。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">结果演示：</span><br><span class="line">1. domain +time + traffic ---&gt; domain + month + traffics</span><br><span class="line">每个domain 每个month的总量  ==》    domain   month  traffics</span><br><span class="line">tmp :</span><br><span class="line">	select </span><br><span class="line">		domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, </span><br><span class="line">		sum(traffic) as traffics</span><br><span class="line">	from ods_domain_traffic_info</span><br><span class="line">	group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_domain_traffic_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7);</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917140000_1dab3570-5d32-449f-ab8f-4c896be57622): select</span><br><span class="line">domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">from ods_domain_traffic_info</span><br><span class="line">group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:domain, type:string, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:traffics, type:bigint, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917140000_1dab3570-5d32-449f-ab8f-4c896be57622); Time taken: 0.498 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917140000_1dab3570-5d32-449f-ab8f-4c896be57622): select</span><br><span class="line">domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">from ods_domain_traffic_info</span><br><span class="line">group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">INFO  : Query ID = double_happy_20190917140000_1dab3570-5d32-449f-ab8f-4c896be57622</span><br><span class="line">INFO  : Total jobs = 1</span><br><span class="line">INFO  : Launching Job 1 out of 1</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0001, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0001/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0001</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:00:23,046 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:00:28,411 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.44 sec</span><br><span class="line">INFO  : 2019-09-17 14:00:34,894 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.51 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 3 seconds 510 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0001</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.51 sec   HDFS Read: 9195 HDFS Write: 82 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 3 seconds 510 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917140000_1dab3570-5d32-449f-ab8f-4c896be57622); Time taken: 23.04 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+--------------+----------+-----------+--+</span><br><span class="line">|    domain    |  month   | traffics  |</span><br><span class="line">+--------------+----------+-----------+--+</span><br><span class="line">| gifshow.com  | 2019-01  | 11        |</span><br><span class="line">| gifshow.com  | 2019-02  | 15        |</span><br><span class="line">| huya.com     | 2019-01  | 1         |</span><br><span class="line">| yy.com       | 2019-01  | 9         |</span><br><span class="line">+--------------+----------+-----------+--+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.目的是totals 生成了一个新列（1对多） 用开窗函数 ，以domain分组以month排序 之后用sum</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; domain,month,traffics,</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; sum(traffics)over(partition by domain order by month) as totals</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_domain_traffic_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; ) as tmp;</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917140202_f7fde489-9ae9-4ab5-85fd-591382b70891): select</span><br><span class="line">domain,month,traffics,</span><br><span class="line">sum(traffics)over(partition by domain order by month) as totals</span><br><span class="line">from(</span><br><span class="line">select</span><br><span class="line">domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">from ods_domain_traffic_info</span><br><span class="line">group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">) as tmp</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:domain, type:string, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:traffics, type:bigint, comment:null), FieldSchema(name:totals, type:bigint, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917140202_f7fde489-9ae9-4ab5-85fd-591382b70891); Time taken: 0.156 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917140202_f7fde489-9ae9-4ab5-85fd-591382b70891): select</span><br><span class="line">domain,month,traffics,</span><br><span class="line">sum(traffics)over(partition by domain order by month) as totals</span><br><span class="line">from(</span><br><span class="line">select</span><br><span class="line">domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">from ods_domain_traffic_info</span><br><span class="line">group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">) as tmp</span><br><span class="line">INFO  : Query ID = double_happy_20190917140202_f7fde489-9ae9-4ab5-85fd-591382b70891</span><br><span class="line">INFO  : Total jobs = 1</span><br><span class="line">INFO  : Launching Job 1 out of 1</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0002, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0002/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0002</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:02:38,123 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:02:44,384 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.64 sec</span><br><span class="line">INFO  : 2019-09-17 14:02:50,678 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.26 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 4 seconds 260 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0002</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.26 sec   HDFS Read: 11518 HDFS Write: 92 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 4 seconds 260 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917140202_f7fde489-9ae9-4ab5-85fd-591382b70891); Time taken: 21.812 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+--------------+----------+-----------+---------+--+</span><br><span class="line">|    domain    |  month   | traffics  | totals  |</span><br><span class="line">+--------------+----------+-----------+---------+--+</span><br><span class="line">| gifshow.com  | 2019-01  | 11        | 11      |</span><br><span class="line">| gifshow.com  | 2019-02  | 15        | 26      |</span><br><span class="line">| huya.com     | 2019-01  | 1         | 1       |</span><br><span class="line">| yy.com       | 2019-01  | 9         | 9       |</span><br><span class="line">+--------------+----------+-----------+---------+--+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">也可以这样的 ：不生成新的列：</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; domain,month,</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; sum(traffics)over(partition by domain order by month) as traffics</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_domain_traffic_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; ) as tmp;</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917140505_20c693e7-e3af-4233-966a-94cdfc50388d): select</span><br><span class="line">domain,month,</span><br><span class="line">sum(traffics)over(partition by domain order by month) as traffics</span><br><span class="line">from(</span><br><span class="line">select</span><br><span class="line">domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">from ods_domain_traffic_info</span><br><span class="line">group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">) as tmp</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:domain, type:string, comment:null), FieldSchema(name:month, type:string, comment:null), FieldSchema(name:traffics, type:bigint, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917140505_20c693e7-e3af-4233-966a-94cdfc50388d); Time taken: 0.05 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917140505_20c693e7-e3af-4233-966a-94cdfc50388d): select</span><br><span class="line">domain,month,</span><br><span class="line">sum(traffics)over(partition by domain order by month) as traffics</span><br><span class="line">from(</span><br><span class="line">select</span><br><span class="line">domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7) as month, sum(traffic) as traffics</span><br><span class="line">from ods_domain_traffic_info</span><br><span class="line">group by domain,substr(regexp_replace(time,&quot;/&quot;,&quot;-&quot;),1,7)</span><br><span class="line">) as tmp</span><br><span class="line">INFO  : Query ID = double_happy_20190917140505_20c693e7-e3af-4233-966a-94cdfc50388d</span><br><span class="line">INFO  : Total jobs = 1</span><br><span class="line">INFO  : Launching Job 1 out of 1</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0003, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0003/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0003</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:05:17,501 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:05:22,722 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec</span><br><span class="line">INFO  : 2019-09-17 14:05:29,989 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.97 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 3 seconds 970 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0003</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.97 sec   HDFS Read: 11443 HDFS Write: 82 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 3 seconds 970 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917140505_20c693e7-e3af-4233-966a-94cdfc50388d); Time taken: 21.456 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+--------------+----------+-----------+--+</span><br><span class="line">|    domain    |  month   | traffics  |</span><br><span class="line">+--------------+----------+-----------+--+</span><br><span class="line">| gifshow.com  | 2019-01  | 11        |</span><br><span class="line">| gifshow.com  | 2019-02  | 26        |</span><br><span class="line">| huya.com     | 2019-01  | 1         |</span><br><span class="line">| yy.com       | 2019-01  | 9         |</span><br><span class="line">+--------------+----------+-----------+--+</span><br></pre></td></tr></table></figure></div>


<h2 id="SQL2"><a href="#SQL2" class="headerlink" title="SQL2"></a>SQL2</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SQL2:</span><br><span class="line">uid pid</span><br><span class="line">user1 a</span><br><span class="line">user2 b</span><br><span class="line">user1 c</span><br><span class="line">user2 c</span><br><span class="line">user3 c</span><br><span class="line">user3 c</span><br><span class="line">1）uv  ==&gt; uid cnt  应该是pid 有多少uid 访问 统计 uid个数  , 或者反过来？？</span><br><span class="line">2）统计每个产品top3的用户信息 ==&gt;  pid  uid  cnt</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">思路：</span><br><span class="line">（1）这块有歧义 那么两个都做一下</span><br><span class="line">	pid 有多少uid ？ 每个pid 有多少个 uid  ==》 pid + uid </span><br><span class="line">1. group by pid +count(distinct(uid))   要去重的 </span><br><span class="line">    uid访问pid的个数？  每个uid  + pid</span><br><span class="line"> a. group by uid  + count(distinct(pid))   要对pid去重</span><br><span class="line"></span><br><span class="line">这题比较简单 就是以谁分组 count 去重后的谁</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line">结果展示： 因为我就造了3个pid  abc 100个user</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select uid ,count(distinct(pid)) as cnt</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_uid_pid_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by uid;</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917141414_3c87306d-3ebd-49b1-9371-8fdba5629b97): select uid ,count(distinct(pid)) as cnt</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by uid</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:uid, type:string, comment:null), FieldSchema(name:cnt, type:bigint, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917141414_3c87306d-3ebd-49b1-9371-8fdba5629b97); Time taken: 0.055 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917141414_3c87306d-3ebd-49b1-9371-8fdba5629b97): select uid ,count(distinct(pid)) as cnt</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by uid</span><br><span class="line">INFO  : Query ID = double_happy_20190917141414_3c87306d-3ebd-49b1-9371-8fdba5629b97</span><br><span class="line">INFO  : Total jobs = 1</span><br><span class="line">INFO  : Launching Job 1 out of 1</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0004, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0004/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0004</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:14:52,471 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:14:57,670 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.05 sec</span><br><span class="line">INFO  : 2019-09-17 14:15:03,926 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.69 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 2 seconds 690 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0004</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.69 sec   HDFS Read: 17780 HDFS Write: 846 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 2 seconds 690 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917141414_3c87306d-3ebd-49b1-9371-8fdba5629b97); Time taken: 19.383 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+----------+------+--+</span><br><span class="line">|   uid    | cnt  |</span><br><span class="line">+----------+------+--+</span><br><span class="line">| user0    | 1    |</span><br><span class="line">| user1    | 3    |</span><br><span class="line">| user10   | 3    |</span><br><span class="line">| user100  | 3    |</span><br><span class="line">| user11   | 3    |</span><br><span class="line">| user12   | 3    |</span><br><span class="line">| user13   | 3    |</span><br><span class="line">| user14   | 3    |</span><br><span class="line">| user15   | 3    |</span><br><span class="line">| user16   | 3    |</span><br><span class="line">| user17   | 3    |</span><br><span class="line">| user18   | 1    |</span><br><span class="line">| user19   | 2    |</span><br><span class="line">| user2    | 3    |</span><br><span class="line">| user20   | 3    |</span><br><span class="line">| user21   | 3    |</span><br><span class="line">| user22   | 3    |</span><br><span class="line">| user24   | 3    |</span><br><span class="line">| user25   | 3    |</span><br><span class="line">| user26   | 3    |</span><br><span class="line">| user27   | 3    |</span><br><span class="line">| user28   | 3    |</span><br><span class="line">| user29   | 3    |</span><br><span class="line">| user3    | 3    |</span><br><span class="line">| user30   | 3    |</span><br><span class="line">| user31   | 2    |</span><br><span class="line">| user32   | 3    |</span><br><span class="line">| user33   | 3    |</span><br><span class="line">| user34   | 3    |</span><br><span class="line">| user36   | 2    |</span><br><span class="line">| user37   | 3    |</span><br><span class="line">| user38   | 3    |</span><br><span class="line">| user39   | 1    |</span><br><span class="line">| user4    | 3    |</span><br><span class="line">| user41   | 3    |</span><br><span class="line">| user42   | 1    |</span><br><span class="line">| user43   | 1    |</span><br><span class="line">| user44   | 3    |</span><br><span class="line">| user45   | 3    |</span><br><span class="line">| user46   | 3    |</span><br><span class="line">| user47   | 2    |</span><br><span class="line">| user48   | 3    |</span><br><span class="line">| user49   | 2    |</span><br><span class="line">| user5    | 3    |</span><br><span class="line">| user50   | 3    |</span><br><span class="line">| user51   | 3    |</span><br><span class="line">| user52   | 3    |</span><br><span class="line">| user54   | 3    |</span><br><span class="line">| user55   | 3    |</span><br><span class="line">| user57   | 1    |</span><br><span class="line">| user58   | 3    |</span><br><span class="line">| user59   | 2    |</span><br><span class="line">| user6    | 3    |</span><br><span class="line">| user60   | 1    |</span><br><span class="line">| user61   | 3    |</span><br><span class="line">| user62   | 3    |</span><br><span class="line">| user63   | 3    |</span><br><span class="line">| user64   | 3    |</span><br><span class="line">| user65   | 2    |</span><br><span class="line">| user66   | 3    |</span><br><span class="line">| user67   | 3    |</span><br><span class="line">| user68   | 1    |</span><br><span class="line">| user69   | 2    |</span><br><span class="line">| user7    | 3    |</span><br><span class="line">| user70   | 3    |</span><br><span class="line">| user71   | 3    |</span><br><span class="line">| user72   | 1    |</span><br><span class="line">| user73   | 3    |</span><br><span class="line">| user74   | 3    |</span><br><span class="line">| user75   | 3    |</span><br><span class="line">| user76   | 2    |</span><br><span class="line">| user77   | 3    |</span><br><span class="line">| user78   | 3    |</span><br><span class="line">| user79   | 3    |</span><br><span class="line">| user8    | 3    |</span><br><span class="line">| user80   | 3    |</span><br><span class="line">| user81   | 3    |</span><br><span class="line">| user82   | 1    |</span><br><span class="line">| user83   | 3    |</span><br><span class="line">| user84   | 1    |</span><br><span class="line">| user85   | 3    |</span><br><span class="line">| user86   | 3    |</span><br><span class="line">| user87   | 3    |</span><br><span class="line">| user88   | 3    |</span><br><span class="line">| user9    | 3    |</span><br><span class="line">| user90   | 3    |</span><br><span class="line">| user91   | 3    |</span><br><span class="line">| user92   | 3    |</span><br><span class="line">| user93   | 2    |</span><br><span class="line">| user94   | 3    |</span><br><span class="line">| user95   | 3    |</span><br><span class="line">| user96   | 3    |</span><br><span class="line">| user97   | 3    |</span><br><span class="line">| user98   | 3    |</span><br><span class="line">| user99   | 3    |</span><br><span class="line">+----------+------+--+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">（2）统计每个产品top3的用户信息 ==&gt;  pid  uid  cnt</span><br><span class="line"></span><br><span class="line">思路：</span><br><span class="line">1.pid  uid  cnt  top3</span><br><span class="line">意思是 每个pid  每个uid 的  访问次数 并 取出 top3</span><br><span class="line">分两步：</span><br><span class="line"></span><br><span class="line">step1：每个pid  每个uid 的  访问次数</span><br><span class="line">step2：基于step1 取出 top3</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">step1：每个pid  每个uid 的  访问次数</span><br><span class="line">group by（pid + uid)  + count（uid）    (group by 是有去重的哈 不难理解吧)</span><br><span class="line"></span><br><span class="line">tmp:</span><br><span class="line">	select pid,uid,count(uid) as count </span><br><span class="line">	from ods_uid_pid_info</span><br><span class="line">	group by pid,uid</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">step2: 基于step1    pid,uid,count   取出 top3 </span><br><span class="line">目的 Top3 =》 基于1 要以pid进行分组以count 进行排序  生成新的列 rank （用开窗 1 对 多）</span><br><span class="line"></span><br><span class="line">这块用到开窗 rank() 或者 row_number(） + over</span><br><span class="line">paritition by 谁 ？order by 谁？</span><br><span class="line">要求的是每个产品top3的用户信息</span><br><span class="line">所以是 paritition by pid order by count    (注意哈 这个count 是step1 得到的 每个pid 每个uid 的 count)</span><br><span class="line"></span><br><span class="line">result_tmp:</span><br><span class="line">	select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">	from tmp;  //有并列的</span><br><span class="line"></span><br><span class="line">result_tmp:</span><br><span class="line">	select pid,uid,count, row_number()over(partition by pid order by count desc) as rank</span><br><span class="line">	from tmp;   //没有并列的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">step 3：基于2 进行where rank  &lt;= 3</span><br><span class="line"></span><br><span class="line">result: 两种</span><br><span class="line">	select pid,uid,count</span><br><span class="line">	from result_tmp</span><br><span class="line">	where  rank&lt;=3;</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br></pre></td><td class="code"><pre><span class="line">step1 演示;   每个pid  每个uid 的  访问次数</span><br><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select pid,uid,count(uid) as count </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_uid_pid_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by pid,uid ;</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917144747_4703dd87-bc51-4175-8551-b4ffcc78ed11): select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:pid, type:string, comment:null), FieldSchema(name:uid, type:string, comment:null), FieldSchema(name:count, type:bigint, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917144747_4703dd87-bc51-4175-8551-b4ffcc78ed11); Time taken: 0.046 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917144747_4703dd87-bc51-4175-8551-b4ffcc78ed11): select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">INFO  : Query ID = double_happy_20190917144747_4703dd87-bc51-4175-8551-b4ffcc78ed11</span><br><span class="line">INFO  : Total jobs = 1</span><br><span class="line">INFO  : Launching Job 1 out of 1</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0007, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0007/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0007</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:47:44,611 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:47:49,855 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.07 sec</span><br><span class="line">INFO  : 2019-09-17 14:47:56,080 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.46 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 2 seconds 460 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0007</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.46 sec   HDFS Read: 17957 HDFS Write: 2767 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 2 seconds 460 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917144747_4703dd87-bc51-4175-8551-b4ffcc78ed11); Time taken: 19.001 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+------+----------+--------+--+</span><br><span class="line">| pid  |   uid    | count  |</span><br><span class="line">+------+----------+--------+--+</span><br><span class="line">| a    | user1    | 4      |</span><br><span class="line">| a    | user10   | 2      |</span><br><span class="line">| a    | user100  | 5      |</span><br><span class="line">| a    | user11   | 8      |</span><br><span class="line">| a    | user12   | 4      |</span><br><span class="line">| a    | user13   | 3      |</span><br><span class="line">| a    | user14   | 6      |</span><br><span class="line">| a    | user15   | 11     |</span><br><span class="line">| a    | user16   | 6      |</span><br><span class="line">| a    | user17   | 5      |</span><br><span class="line">| a    | user19   | 2      |</span><br><span class="line">| a    | user2    | 2      |</span><br><span class="line">| a    | user20   | 3      |</span><br><span class="line">| a    | user21   | 1      |</span><br><span class="line">| a    | user22   | 5      |</span><br><span class="line">| a    | user24   | 7      |</span><br><span class="line">| a    | user25   | 4      |</span><br><span class="line">| a    | user26   | 6      |</span><br><span class="line">| a    | user27   | 2      |</span><br><span class="line">| a    | user28   | 1      |</span><br><span class="line">| a    | user29   | 4      |</span><br><span class="line">| a    | user3    | 3      |</span><br><span class="line">| a    | user30   | 4      |</span><br><span class="line">| a    | user31   | 5      |</span><br><span class="line">| a    | user32   | 4      |</span><br><span class="line">| a    | user33   | 2      |</span><br><span class="line">| a    | user34   | 1      |</span><br><span class="line">| a    | user36   | 1      |</span><br><span class="line">| a    | user37   | 7      |</span><br><span class="line">| a    | user38   | 5      |</span><br><span class="line">| a    | user4    | 3      |</span><br><span class="line">| a    | user41   | 1      |</span><br><span class="line">| a    | user43   | 1      |</span><br><span class="line">| a    | user44   | 2      |</span><br><span class="line">| a    | user45   | 3      |</span><br><span class="line">| a    | user46   | 2      |</span><br><span class="line">| a    | user47   | 2      |</span><br><span class="line">| a    | user48   | 2      |</span><br><span class="line">| a    | user49   | 3      |</span><br><span class="line">| a    | user5    | 10     |</span><br><span class="line">| a    | user50   | 3      |</span><br><span class="line">| a    | user51   | 4      |</span><br><span class="line">| a    | user52   | 1      |</span><br><span class="line">| a    | user54   | 9      |</span><br><span class="line">| a    | user55   | 6      |</span><br><span class="line">| a    | user58   | 1      |</span><br><span class="line">| a    | user59   | 3      |</span><br><span class="line">| a    | user6    | 3      |</span><br><span class="line">| a    | user61   | 2      |</span><br><span class="line">| a    | user62   | 11     |</span><br><span class="line">| a    | user63   | 3      |</span><br><span class="line">| a    | user64   | 4      |</span><br><span class="line">| a    | user65   | 2      |</span><br><span class="line">| a    | user66   | 3      |</span><br><span class="line">| a    | user67   | 2      |</span><br><span class="line">| a    | user69   | 2      |</span><br><span class="line">| a    | user7    | 4      |</span><br><span class="line">| a    | user70   | 3      |</span><br><span class="line">| a    | user71   | 6      |</span><br><span class="line">| a    | user72   | 2      |</span><br><span class="line">| a    | user73   | 6      |</span><br><span class="line">| a    | user74   | 2      |</span><br><span class="line">| a    | user75   | 2      |</span><br><span class="line">| a    | user76   | 2      |</span><br><span class="line">| a    | user77   | 3      |</span><br><span class="line">| a    | user78   | 7      |</span><br><span class="line">| a    | user79   | 7      |</span><br><span class="line">| a    | user8    | 1      |</span><br><span class="line">| a    | user80   | 2      |</span><br><span class="line">| a    | user81   | 6      |</span><br><span class="line">| a    | user82   | 1      |</span><br><span class="line">| a    | user83   | 5      |</span><br><span class="line">| a    | user85   | 3      |</span><br><span class="line">| a    | user86   | 5      |</span><br><span class="line">| a    | user87   | 8      |</span><br><span class="line">| a    | user88   | 5      |</span><br><span class="line">| a    | user9    | 1      |</span><br><span class="line">| a    | user90   | 2      |</span><br><span class="line">| a    | user91   | 4      |</span><br><span class="line">| a    | user92   | 2      |</span><br><span class="line">| a    | user93   | 1      |</span><br><span class="line">| a    | user94   | 6      |</span><br><span class="line">| a    | user95   | 2      |</span><br><span class="line">| a    | user96   | 6      |</span><br><span class="line">| a    | user97   | 6      |</span><br><span class="line">| a    | user98   | 3      |</span><br><span class="line">| a    | user99   | 4      |</span><br><span class="line">| b    | user0    | 2      |</span><br><span class="line">| b    | user1    | 7      |</span><br><span class="line">| b    | user10   | 1      |</span><br><span class="line">| b    | user100  | 4      |</span><br><span class="line">| b    | user11   | 6      |</span><br><span class="line">| b    | user12   | 3      |</span><br><span class="line">| b    | user13   | 4      |</span><br><span class="line">| b    | user14   | 4      |</span><br><span class="line">| b    | user15   | 9      |</span><br><span class="line">| b    | user16   | 6      |</span><br><span class="line">| b    | user17   | 4      |</span><br><span class="line">| b    | user19   | 6      |</span><br><span class="line">| b    | user2    | 5      |</span><br><span class="line">+------+----------+--------+--+</span><br><span class="line">| pid  |   uid    | count  |</span><br><span class="line">+------+----------+--------+--+</span><br><span class="line">| b    | user20   | 1      |</span><br><span class="line">| b    | user21   | 8      |</span><br><span class="line">| b    | user22   | 8      |</span><br><span class="line">| b    | user24   | 2      |</span><br><span class="line">| b    | user25   | 1      |</span><br><span class="line">| b    | user26   | 7      |</span><br><span class="line">| b    | user27   | 2      |</span><br><span class="line">| b    | user28   | 2      |</span><br><span class="line">| b    | user29   | 4      |</span><br><span class="line">| b    | user3    | 8      |</span><br><span class="line">| b    | user30   | 3      |</span><br><span class="line">| b    | user32   | 3      |</span><br><span class="line">| b    | user33   | 2      |</span><br><span class="line">| b    | user34   | 4      |</span><br><span class="line">| b    | user36   | 1      |</span><br><span class="line">| b    | user37   | 10     |</span><br><span class="line">| b    | user38   | 3      |</span><br><span class="line">| b    | user4    | 5      |</span><br><span class="line">| b    | user41   | 1      |</span><br><span class="line">| b    | user44   | 1      |</span><br><span class="line">| b    | user45   | 1      |</span><br><span class="line">| b    | user46   | 2      |</span><br><span class="line">| b    | user47   | 1      |</span><br><span class="line">| b    | user48   | 7      |</span><br><span class="line">| b    | user49   | 2      |</span><br><span class="line">| b    | user5    | 8      |</span><br><span class="line">| b    | user50   | 6      |</span><br><span class="line">| b    | user51   | 7      |</span><br><span class="line">| b    | user52   | 3      |</span><br><span class="line">| b    | user54   | 2      |</span><br><span class="line">| b    | user55   | 1      |</span><br><span class="line">| b    | user58   | 5      |</span><br><span class="line">| b    | user59   | 1      |</span><br><span class="line">| b    | user6    | 1      |</span><br><span class="line">| b    | user60   | 4      |</span><br><span class="line">| b    | user61   | 10     |</span><br><span class="line">| b    | user62   | 4      |</span><br><span class="line">| b    | user63   | 4      |</span><br><span class="line">| b    | user64   | 1      |</span><br><span class="line">| b    | user65   | 1      |</span><br><span class="line">| b    | user66   | 7      |</span><br><span class="line">| b    | user67   | 1      |</span><br><span class="line">| b    | user68   | 1      |</span><br><span class="line">| b    | user69   | 8      |</span><br><span class="line">| b    | user7    | 8      |</span><br><span class="line">| b    | user70   | 3      |</span><br><span class="line">| b    | user71   | 4      |</span><br><span class="line">| b    | user73   | 5      |</span><br><span class="line">| b    | user74   | 1      |</span><br><span class="line">| b    | user75   | 3      |</span><br><span class="line">| b    | user77   | 8      |</span><br><span class="line">| b    | user78   | 2      |</span><br><span class="line">| b    | user79   | 9      |</span><br><span class="line">| b    | user8    | 1      |</span><br><span class="line">| b    | user80   | 6      |</span><br><span class="line">| b    | user81   | 6      |</span><br><span class="line">| b    | user83   | 3      |</span><br><span class="line">| b    | user84   | 4      |</span><br><span class="line">| b    | user85   | 6      |</span><br><span class="line">| b    | user86   | 5      |</span><br><span class="line">| b    | user87   | 6      |</span><br><span class="line">| b    | user88   | 4      |</span><br><span class="line">| b    | user9    | 1      |</span><br><span class="line">| b    | user90   | 4      |</span><br><span class="line">| b    | user91   | 2      |</span><br><span class="line">| b    | user92   | 5      |</span><br><span class="line">| b    | user94   | 3      |</span><br><span class="line">| b    | user95   | 3      |</span><br><span class="line">| b    | user96   | 4      |</span><br><span class="line">| b    | user97   | 7      |</span><br><span class="line">| b    | user98   | 3      |</span><br><span class="line">| b    | user99   | 3      |</span><br><span class="line">| c    | user1    | 5      |</span><br><span class="line">| c    | user10   | 7      |</span><br><span class="line">| c    | user100  | 2      |</span><br><span class="line">| c    | user11   | 4      |</span><br><span class="line">| c    | user12   | 5      |</span><br><span class="line">| c    | user13   | 3      |</span><br><span class="line">| c    | user14   | 2      |</span><br><span class="line">| c    | user15   | 6      |</span><br><span class="line">| c    | user16   | 3      |</span><br><span class="line">| c    | user17   | 3      |</span><br><span class="line">| c    | user18   | 1      |</span><br><span class="line">| c    | user2    | 2      |</span><br><span class="line">| c    | user20   | 2      |</span><br><span class="line">| c    | user21   | 4      |</span><br><span class="line">| c    | user22   | 4      |</span><br><span class="line">| c    | user24   | 4      |</span><br><span class="line">| c    | user25   | 4      |</span><br><span class="line">| c    | user26   | 6      |</span><br><span class="line">| c    | user27   | 4      |</span><br><span class="line">| c    | user28   | 1      |</span><br><span class="line">| c    | user29   | 7      |</span><br><span class="line">| c    | user3    | 3      |</span><br><span class="line">| c    | user30   | 1      |</span><br><span class="line">| c    | user31   | 1      |</span><br><span class="line">| c    | user32   | 3      |</span><br><span class="line">| c    | user33   | 1      |</span><br><span class="line">| c    | user34   | 2      |</span><br><span class="line">| c    | user37   | 6      |</span><br><span class="line">+------+----------+--------+--+</span><br><span class="line">| pid  |   uid    | count  |</span><br><span class="line">+------+----------+--------+--+</span><br><span class="line">| c    | user38   | 6      |</span><br><span class="line">| c    | user39   | 2      |</span><br><span class="line">| c    | user4    | 9      |</span><br><span class="line">| c    | user41   | 2      |</span><br><span class="line">| c    | user42   | 2      |</span><br><span class="line">| c    | user44   | 1      |</span><br><span class="line">| c    | user45   | 3      |</span><br><span class="line">| c    | user46   | 3      |</span><br><span class="line">| c    | user48   | 6      |</span><br><span class="line">| c    | user5    | 9      |</span><br><span class="line">| c    | user50   | 6      |</span><br><span class="line">| c    | user51   | 8      |</span><br><span class="line">| c    | user52   | 4      |</span><br><span class="line">| c    | user54   | 3      |</span><br><span class="line">| c    | user55   | 2      |</span><br><span class="line">| c    | user57   | 1      |</span><br><span class="line">| c    | user58   | 2      |</span><br><span class="line">| c    | user6    | 1      |</span><br><span class="line">| c    | user61   | 6      |</span><br><span class="line">| c    | user62   | 4      |</span><br><span class="line">| c    | user63   | 3      |</span><br><span class="line">| c    | user64   | 2      |</span><br><span class="line">| c    | user66   | 13     |</span><br><span class="line">| c    | user67   | 4      |</span><br><span class="line">| c    | user7    | 10     |</span><br><span class="line">| c    | user70   | 5      |</span><br><span class="line">| c    | user71   | 6      |</span><br><span class="line">| c    | user73   | 7      |</span><br><span class="line">| c    | user74   | 4      |</span><br><span class="line">| c    | user75   | 4      |</span><br><span class="line">| c    | user76   | 1      |</span><br><span class="line">| c    | user77   | 6      |</span><br><span class="line">| c    | user78   | 3      |</span><br><span class="line">| c    | user79   | 7      |</span><br><span class="line">| c    | user8    | 2      |</span><br><span class="line">| c    | user80   | 2      |</span><br><span class="line">| c    | user81   | 11     |</span><br><span class="line">| c    | user83   | 2      |</span><br><span class="line">| c    | user85   | 2      |</span><br><span class="line">| c    | user86   | 5      |</span><br><span class="line">| c    | user87   | 4      |</span><br><span class="line">| c    | user88   | 4      |</span><br><span class="line">| c    | user9    | 1      |</span><br><span class="line">| c    | user90   | 4      |</span><br><span class="line">| c    | user91   | 2      |</span><br><span class="line">| c    | user92   | 4      |</span><br><span class="line">| c    | user93   | 1      |</span><br><span class="line">| c    | user94   | 12     |</span><br><span class="line">| c    | user95   | 3      |</span><br><span class="line">| c    | user96   | 4      |</span><br><span class="line">| c    | user97   | 6      |</span><br><span class="line">| c    | user98   | 3      |</span><br><span class="line">| c    | user99   | 5      |</span><br><span class="line">+------+----------+--------+--+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br></pre></td><td class="code"><pre><span class="line">step2:目的 Top3 =》 基于1 要以pid进行分组以count 进行排序  生成新的列 rank</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select pid,uid,count(uid) as count </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_uid_pid_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by pid,uid </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; )as tmp;  </span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917144949_8fd22282-a9be-4e95-aa2a-b98121f6354d): select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">)as tmp</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:pid, type:string, comment:null), FieldSchema(name:uid, type:string, comment:null), FieldSchema(name:count, type:bigint, comment:null), FieldSchema(name:rank, type:int, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917144949_8fd22282-a9be-4e95-aa2a-b98121f6354d); Time taken: 0.054 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917144949_8fd22282-a9be-4e95-aa2a-b98121f6354d): select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">)as tmp</span><br><span class="line">INFO  : Query ID = double_happy_20190917144949_8fd22282-a9be-4e95-aa2a-b98121f6354d</span><br><span class="line">INFO  : Total jobs = 2</span><br><span class="line">INFO  : Launching Job 1 out of 2</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0008, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0008/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0008</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:49:58,539 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:50:03,879 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.05 sec</span><br><span class="line">INFO  : 2019-09-17 14:50:10,134 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.98 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 2 seconds 980 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0008</span><br><span class="line">INFO  : Launching Job 2 out of 2</span><br><span class="line">INFO  : Starting task [Stage-2:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0009, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0009/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0009</span><br><span class="line">INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:50:17,507 Stage-2 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:50:23,732 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec</span><br><span class="line">INFO  : 2019-09-17 14:50:29,976 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.39 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 3 seconds 390 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0009</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.98 sec   HDFS Read: 17068 HDFS Write: 6962 SUCCESS</span><br><span class="line">INFO  : Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.39 sec   HDFS Read: 14419 HDFS Write: 3494 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 6 seconds 370 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917144949_8fd22282-a9be-4e95-aa2a-b98121f6354d); Time taken: 39.409 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+------+----------+--------+-------+--+</span><br><span class="line">| pid  |   uid    | count  | rank  |</span><br><span class="line">+------+----------+--------+-------+--+</span><br><span class="line">| a    | user15   | 11     | 1     |</span><br><span class="line">| a    | user62   | 11     | 1     |</span><br><span class="line">| a    | user5    | 10     | 3     |</span><br><span class="line">| a    | user54   | 9      | 4     |</span><br><span class="line">| a    | user87   | 8      | 5     |</span><br><span class="line">| a    | user11   | 8      | 5     |</span><br><span class="line">| a    | user37   | 7      | 7     |</span><br><span class="line">| a    | user24   | 7      | 7     |</span><br><span class="line">| a    | user78   | 7      | 7     |</span><br><span class="line">| a    | user79   | 7      | 7     |</span><br><span class="line">| a    | user55   | 6      | 11    |</span><br><span class="line">| a    | user96   | 6      | 11    |</span><br><span class="line">| a    | user94   | 6      | 11    |</span><br><span class="line">| a    | user16   | 6      | 11    |</span><br><span class="line">| a    | user81   | 6      | 11    |</span><br><span class="line">| a    | user26   | 6      | 11    |</span><br><span class="line">| a    | user97   | 6      | 11    |</span><br><span class="line">| a    | user71   | 6      | 11    |</span><br><span class="line">| a    | user14   | 6      | 11    |</span><br><span class="line">| a    | user73   | 6      | 11    |</span><br><span class="line">| a    | user86   | 5      | 21    |</span><br><span class="line">| a    | user38   | 5      | 21    |</span><br><span class="line">| a    | user88   | 5      | 21    |</span><br><span class="line">| a    | user22   | 5      | 21    |</span><br><span class="line">| a    | user83   | 5      | 21    |</span><br><span class="line">| a    | user31   | 5      | 21    |</span><br><span class="line">| a    | user100  | 5      | 21    |</span><br><span class="line">| a    | user17   | 5      | 21    |</span><br><span class="line">| a    | user12   | 4      | 29    |</span><br><span class="line">| a    | user7    | 4      | 29    |</span><br><span class="line">| a    | user25   | 4      | 29    |</span><br><span class="line">| a    | user29   | 4      | 29    |</span><br><span class="line">| a    | user30   | 4      | 29    |</span><br><span class="line">| a    | user32   | 4      | 29    |</span><br><span class="line">| a    | user99   | 4      | 29    |</span><br><span class="line">| a    | user91   | 4      | 29    |</span><br><span class="line">| a    | user51   | 4      | 29    |</span><br><span class="line">| a    | user64   | 4      | 29    |</span><br><span class="line">| a    | user1    | 4      | 29    |</span><br><span class="line">| a    | user4    | 3      | 40    |</span><br><span class="line">| a    | user50   | 3      | 40    |</span><br><span class="line">| a    | user66   | 3      | 40    |</span><br><span class="line">| a    | user3    | 3      | 40    |</span><br><span class="line">| a    | user85   | 3      | 40    |</span><br><span class="line">| a    | user77   | 3      | 40    |</span><br><span class="line">| a    | user98   | 3      | 40    |</span><br><span class="line">| a    | user59   | 3      | 40    |</span><br><span class="line">| a    | user6    | 3      | 40    |</span><br><span class="line">| a    | user20   | 3      | 40    |</span><br><span class="line">| a    | user70   | 3      | 40    |</span><br><span class="line">| a    | user63   | 3      | 40    |</span><br><span class="line">| a    | user45   | 3      | 40    |</span><br><span class="line">| a    | user13   | 3      | 40    |</span><br><span class="line">| a    | user49   | 3      | 40    |</span><br><span class="line">| a    | user65   | 2      | 55    |</span><br><span class="line">| a    | user19   | 2      | 55    |</span><br><span class="line">| a    | user2    | 2      | 55    |</span><br><span class="line">| a    | user27   | 2      | 55    |</span><br><span class="line">| a    | user33   | 2      | 55    |</span><br><span class="line">| a    | user44   | 2      | 55    |</span><br><span class="line">| a    | user46   | 2      | 55    |</span><br><span class="line">| a    | user47   | 2      | 55    |</span><br><span class="line">| a    | user48   | 2      | 55    |</span><br><span class="line">| a    | user61   | 2      | 55    |</span><br><span class="line">| a    | user67   | 2      | 55    |</span><br><span class="line">| a    | user69   | 2      | 55    |</span><br><span class="line">| a    | user72   | 2      | 55    |</span><br><span class="line">| a    | user74   | 2      | 55    |</span><br><span class="line">| a    | user75   | 2      | 55    |</span><br><span class="line">| a    | user76   | 2      | 55    |</span><br><span class="line">| a    | user80   | 2      | 55    |</span><br><span class="line">| a    | user90   | 2      | 55    |</span><br><span class="line">| a    | user92   | 2      | 55    |</span><br><span class="line">| a    | user95   | 2      | 55    |</span><br><span class="line">| a    | user10   | 2      | 55    |</span><br><span class="line">| a    | user52   | 1      | 76    |</span><br><span class="line">| a    | user9    | 1      | 76    |</span><br><span class="line">| a    | user41   | 1      | 76    |</span><br><span class="line">| a    | user93   | 1      | 76    |</span><br><span class="line">| a    | user36   | 1      | 76    |</span><br><span class="line">| a    | user34   | 1      | 76    |</span><br><span class="line">| a    | user28   | 1      | 76    |</span><br><span class="line">| a    | user21   | 1      | 76    |</span><br><span class="line">| a    | user43   | 1      | 76    |</span><br><span class="line">| a    | user82   | 1      | 76    |</span><br><span class="line">| a    | user8    | 1      | 76    |</span><br><span class="line">| a    | user58   | 1      | 76    |</span><br><span class="line">| b    | user37   | 10     | 1     |</span><br><span class="line">| b    | user61   | 10     | 1     |</span><br><span class="line">| b    | user15   | 9      | 3     |</span><br><span class="line">| b    | user79   | 9      | 3     |</span><br><span class="line">| b    | user7    | 8      | 5     |</span><br><span class="line">| b    | user21   | 8      | 5     |</span><br><span class="line">| b    | user22   | 8      | 5     |</span><br><span class="line">| b    | user69   | 8      | 5     |</span><br><span class="line">| b    | user5    | 8      | 5     |</span><br><span class="line">| b    | user3    | 8      | 5     |</span><br><span class="line">| b    | user77   | 8      | 5     |</span><br><span class="line">| b    | user51   | 7      | 12    |</span><br><span class="line">| b    | user48   | 7      | 12    |</span><br><span class="line">+------+----------+--------+-------+--+</span><br><span class="line">| pid  |   uid    | count  | rank  |</span><br><span class="line">+------+----------+--------+-------+--+</span><br><span class="line">| b    | user26   | 7      | 12    |</span><br><span class="line">| b    | user97   | 7      | 12    |</span><br><span class="line">| b    | user66   | 7      | 12    |</span><br><span class="line">| b    | user1    | 7      | 12    |</span><br><span class="line">| b    | user50   | 6      | 18    |</span><br><span class="line">| b    | user87   | 6      | 18    |</span><br><span class="line">| b    | user85   | 6      | 18    |</span><br><span class="line">| b    | user81   | 6      | 18    |</span><br><span class="line">| b    | user80   | 6      | 18    |</span><br><span class="line">| b    | user19   | 6      | 18    |</span><br><span class="line">| b    | user16   | 6      | 18    |</span><br><span class="line">| b    | user11   | 6      | 18    |</span><br><span class="line">| b    | user2    | 5      | 26    |</span><br><span class="line">| b    | user92   | 5      | 26    |</span><br><span class="line">| b    | user58   | 5      | 26    |</span><br><span class="line">| b    | user73   | 5      | 26    |</span><br><span class="line">| b    | user4    | 5      | 26    |</span><br><span class="line">| b    | user86   | 5      | 26    |</span><br><span class="line">| b    | user88   | 4      | 32    |</span><br><span class="line">| b    | user71   | 4      | 32    |</span><br><span class="line">| b    | user29   | 4      | 32    |</span><br><span class="line">| b    | user84   | 4      | 32    |</span><br><span class="line">| b    | user13   | 4      | 32    |</span><br><span class="line">| b    | user17   | 4      | 32    |</span><br><span class="line">| b    | user100  | 4      | 32    |</span><br><span class="line">| b    | user34   | 4      | 32    |</span><br><span class="line">| b    | user14   | 4      | 32    |</span><br><span class="line">| b    | user96   | 4      | 32    |</span><br><span class="line">| b    | user90   | 4      | 32    |</span><br><span class="line">| b    | user63   | 4      | 32    |</span><br><span class="line">| b    | user62   | 4      | 32    |</span><br><span class="line">| b    | user60   | 4      | 32    |</span><br><span class="line">| b    | user12   | 3      | 46    |</span><br><span class="line">| b    | user99   | 3      | 46    |</span><br><span class="line">| b    | user94   | 3      | 46    |</span><br><span class="line">| b    | user95   | 3      | 46    |</span><br><span class="line">| b    | user98   | 3      | 46    |</span><br><span class="line">| b    | user75   | 3      | 46    |</span><br><span class="line">| b    | user83   | 3      | 46    |</span><br><span class="line">| b    | user52   | 3      | 46    |</span><br><span class="line">| b    | user32   | 3      | 46    |</span><br><span class="line">| b    | user30   | 3      | 46    |</span><br><span class="line">| b    | user70   | 3      | 46    |</span><br><span class="line">| b    | user38   | 3      | 46    |</span><br><span class="line">| b    | user54   | 2      | 58    |</span><br><span class="line">| b    | user28   | 2      | 58    |</span><br><span class="line">| b    | user27   | 2      | 58    |</span><br><span class="line">| b    | user49   | 2      | 58    |</span><br><span class="line">| b    | user46   | 2      | 58    |</span><br><span class="line">| b    | user24   | 2      | 58    |</span><br><span class="line">| b    | user33   | 2      | 58    |</span><br><span class="line">| b    | user0    | 2      | 58    |</span><br><span class="line">| b    | user91   | 2      | 58    |</span><br><span class="line">| b    | user78   | 2      | 58    |</span><br><span class="line">| b    | user67   | 1      | 68    |</span><br><span class="line">| b    | user65   | 1      | 68    |</span><br><span class="line">| b    | user64   | 1      | 68    |</span><br><span class="line">| b    | user6    | 1      | 68    |</span><br><span class="line">| b    | user59   | 1      | 68    |</span><br><span class="line">| b    | user55   | 1      | 68    |</span><br><span class="line">| b    | user74   | 1      | 68    |</span><br><span class="line">| b    | user8    | 1      | 68    |</span><br><span class="line">| b    | user47   | 1      | 68    |</span><br><span class="line">| b    | user45   | 1      | 68    |</span><br><span class="line">| b    | user44   | 1      | 68    |</span><br><span class="line">| b    | user41   | 1      | 68    |</span><br><span class="line">| b    | user36   | 1      | 68    |</span><br><span class="line">| b    | user25   | 1      | 68    |</span><br><span class="line">| b    | user9    | 1      | 68    |</span><br><span class="line">| b    | user20   | 1      | 68    |</span><br><span class="line">| b    | user10   | 1      | 68    |</span><br><span class="line">| b    | user68   | 1      | 68    |</span><br><span class="line">| c    | user66   | 13     | 1     |</span><br><span class="line">| c    | user94   | 12     | 2     |</span><br><span class="line">| c    | user81   | 11     | 3     |</span><br><span class="line">| c    | user7    | 10     | 4     |</span><br><span class="line">| c    | user5    | 9      | 5     |</span><br><span class="line">| c    | user4    | 9      | 5     |</span><br><span class="line">| c    | user51   | 8      | 7     |</span><br><span class="line">| c    | user79   | 7      | 8     |</span><br><span class="line">| c    | user10   | 7      | 8     |</span><br><span class="line">| c    | user73   | 7      | 8     |</span><br><span class="line">| c    | user29   | 7      | 8     |</span><br><span class="line">| c    | user38   | 6      | 12    |</span><br><span class="line">| c    | user37   | 6      | 12    |</span><br><span class="line">| c    | user97   | 6      | 12    |</span><br><span class="line">| c    | user15   | 6      | 12    |</span><br><span class="line">| c    | user77   | 6      | 12    |</span><br><span class="line">| c    | user61   | 6      | 12    |</span><br><span class="line">| c    | user50   | 6      | 12    |</span><br><span class="line">| c    | user26   | 6      | 12    |</span><br><span class="line">| c    | user48   | 6      | 12    |</span><br><span class="line">| c    | user71   | 6      | 12    |</span><br><span class="line">| c    | user99   | 5      | 22    |</span><br><span class="line">| c    | user12   | 5      | 22    |</span><br><span class="line">| c    | user1    | 5      | 22    |</span><br><span class="line">| c    | user70   | 5      | 22    |</span><br><span class="line">| c    | user86   | 5      | 22    |</span><br><span class="line">| c    | user75   | 4      | 27    |</span><br><span class="line">| c    | user87   | 4      | 27    |</span><br><span class="line">+------+----------+--------+-------+--+</span><br><span class="line">| pid  |   uid    | count  | rank  |</span><br><span class="line">+------+----------+--------+-------+--+</span><br><span class="line">| c    | user74   | 4      | 27    |</span><br><span class="line">| c    | user67   | 4      | 27    |</span><br><span class="line">| c    | user21   | 4      | 27    |</span><br><span class="line">| c    | user62   | 4      | 27    |</span><br><span class="line">| c    | user88   | 4      | 27    |</span><br><span class="line">| c    | user96   | 4      | 27    |</span><br><span class="line">| c    | user92   | 4      | 27    |</span><br><span class="line">| c    | user90   | 4      | 27    |</span><br><span class="line">| c    | user27   | 4      | 27    |</span><br><span class="line">| c    | user25   | 4      | 27    |</span><br><span class="line">| c    | user24   | 4      | 27    |</span><br><span class="line">| c    | user22   | 4      | 27    |</span><br><span class="line">| c    | user52   | 4      | 27    |</span><br><span class="line">| c    | user11   | 4      | 27    |</span><br><span class="line">| c    | user63   | 3      | 43    |</span><br><span class="line">| c    | user45   | 3      | 43    |</span><br><span class="line">| c    | user95   | 3      | 43    |</span><br><span class="line">| c    | user46   | 3      | 43    |</span><br><span class="line">| c    | user32   | 3      | 43    |</span><br><span class="line">| c    | user54   | 3      | 43    |</span><br><span class="line">| c    | user3    | 3      | 43    |</span><br><span class="line">| c    | user98   | 3      | 43    |</span><br><span class="line">| c    | user17   | 3      | 43    |</span><br><span class="line">| c    | user16   | 3      | 43    |</span><br><span class="line">| c    | user78   | 3      | 43    |</span><br><span class="line">| c    | user13   | 3      | 43    |</span><br><span class="line">| c    | user83   | 2      | 55    |</span><br><span class="line">| c    | user85   | 2      | 55    |</span><br><span class="line">| c    | user58   | 2      | 55    |</span><br><span class="line">| c    | user55   | 2      | 55    |</span><br><span class="line">| c    | user42   | 2      | 55    |</span><br><span class="line">| c    | user100  | 2      | 55    |</span><br><span class="line">| c    | user91   | 2      | 55    |</span><br><span class="line">| c    | user14   | 2      | 55    |</span><br><span class="line">| c    | user8    | 2      | 55    |</span><br><span class="line">| c    | user80   | 2      | 55    |</span><br><span class="line">| c    | user39   | 2      | 55    |</span><br><span class="line">| c    | user64   | 2      | 55    |</span><br><span class="line">| c    | user20   | 2      | 55    |</span><br><span class="line">| c    | user41   | 2      | 55    |</span><br><span class="line">| c    | user2    | 2      | 55    |</span><br><span class="line">| c    | user34   | 2      | 55    |</span><br><span class="line">| c    | user18   | 1      | 71    |</span><br><span class="line">| c    | user30   | 1      | 71    |</span><br><span class="line">| c    | user28   | 1      | 71    |</span><br><span class="line">| c    | user44   | 1      | 71    |</span><br><span class="line">| c    | user57   | 1      | 71    |</span><br><span class="line">| c    | user6    | 1      | 71    |</span><br><span class="line">| c    | user76   | 1      | 71    |</span><br><span class="line">| c    | user9    | 1      | 71    |</span><br><span class="line">| c    | user93   | 1      | 71    |</span><br><span class="line">| c    | user31   | 1      | 71    |</span><br><span class="line">| c    | user33   | 1      | 71    |</span><br><span class="line">+------+----------+--------+-------+--+</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">step3   基于2 进行where rank  &lt;= 3</span><br><span class="line"></span><br><span class="line">整合：</span><br><span class="line">result: 两种</span><br><span class="line">	select pid,uid,count</span><br><span class="line">	from(</span><br><span class="line">	select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">	from(</span><br><span class="line">	select pid,uid,count(uid) as count </span><br><span class="line">	from ods_uid_pid_info</span><br><span class="line">	group by pid,uid </span><br><span class="line">	)as tmp</span><br><span class="line">	)as result_tmp</span><br><span class="line">	where  rank&lt;=3;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">结果：</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select pid,uid,count</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select pid,uid,count(uid) as count </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_uid_pid_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by pid,uid </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; )as tmp</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; )as result_tmp</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; where  rank&lt;=3;</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917143131_3e5d7235-f034-4022-bea0-b86d6464a437): select pid,uid,count</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">)as tmp</span><br><span class="line">)as result_tmp</span><br><span class="line">where  rank&lt;=3</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:pid, type:string, comment:null), FieldSchema(name:uid, type:string, comment:null), FieldSchema(name:count, type:bigint, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917143131_3e5d7235-f034-4022-bea0-b86d6464a437); Time taken: 0.096 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917143131_3e5d7235-f034-4022-bea0-b86d6464a437): select pid,uid,count</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">)as tmp</span><br><span class="line">)as result_tmp</span><br><span class="line">where  rank&lt;=3</span><br><span class="line">INFO  : Query ID = double_happy_20190917143131_3e5d7235-f034-4022-bea0-b86d6464a437</span><br><span class="line">INFO  : Total jobs = 2</span><br><span class="line">INFO  : Launching Job 1 out of 2</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0005, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0005/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0005</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:31:30,465 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:31:34,683 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.17 sec</span><br><span class="line">INFO  : 2019-09-17 14:31:40,960 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.51 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 2 seconds 510 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0005</span><br><span class="line">INFO  : Launching Job 2 out of 2</span><br><span class="line">INFO  : Starting task [Stage-2:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0006, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0006/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0006</span><br><span class="line">INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:31:49,210 Stage-2 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:31:54,497 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.17 sec</span><br><span class="line">INFO  : 2019-09-17 14:32:00,785 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.48 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 3 seconds 480 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0006</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.51 sec   HDFS Read: 17079 HDFS Write: 6962 SUCCESS</span><br><span class="line">INFO  : Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.48 sec   HDFS Read: 14767 HDFS Write: 117 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 5 seconds 990 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917143131_3e5d7235-f034-4022-bea0-b86d6464a437); Time taken: 37.896 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+------+---------+--------+--+</span><br><span class="line">| pid  |   uid   | count  |</span><br><span class="line">+------+---------+--------+--+</span><br><span class="line">| a    | user15  | 11     |</span><br><span class="line">| a    | user62  | 11     |</span><br><span class="line">| a    | user5   | 10     |</span><br><span class="line">| b    | user37  | 10     |</span><br><span class="line">| b    | user61  | 10     |</span><br><span class="line">| b    | user15  | 9      |</span><br><span class="line">| b    | user79  | 9      |</span><br><span class="line">| c    | user66  | 13     |</span><br><span class="line">| c    | user94  | 12     |</span><br><span class="line">| c    | user81  | 11     |</span><br><span class="line">+------+---------+--------+--+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">带上排名 看的更清楚：</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2://hadoop101:10000&gt; select pid,uid,count,rank</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from(</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; select pid,uid,count(uid) as count </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; from ods_uid_pid_info</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; group by pid,uid </span><br><span class="line">. . . . . . . . . . . . . . . .&gt; )as tmp</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; )as result_tmp</span><br><span class="line">. . . . . . . . . . . . . . . .&gt; where  rank&lt;=3;</span><br><span class="line">INFO  : Compiling command(queryId=double_happy_20190917145252_c7075e67-87d3-4c1d-9d14-0944dfcdbdca): select pid,uid,count,rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">)as tmp</span><br><span class="line">)as result_tmp</span><br><span class="line">where  rank&lt;=3</span><br><span class="line">INFO  : Semantic Analysis Completed</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:pid, type:string, comment:null), FieldSchema(name:uid, type:string, comment:null), FieldSchema(name:count, type:bigint, comment:null), FieldSchema(name:rank, type:int, comment:null)], properties:null)</span><br><span class="line">INFO  : Completed compiling command(queryId=double_happy_20190917145252_c7075e67-87d3-4c1d-9d14-0944dfcdbdca); Time taken: 0.047 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing command(queryId=double_happy_20190917145252_c7075e67-87d3-4c1d-9d14-0944dfcdbdca): select pid,uid,count,rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count, rank()over(partition by pid order by count desc) as rank</span><br><span class="line">from(</span><br><span class="line">select pid,uid,count(uid) as count</span><br><span class="line">from ods_uid_pid_info</span><br><span class="line">group by pid,uid</span><br><span class="line">)as tmp</span><br><span class="line">)as result_tmp</span><br><span class="line">where  rank&lt;=3</span><br><span class="line">INFO  : Query ID = double_happy_20190917145252_c7075e67-87d3-4c1d-9d14-0944dfcdbdca</span><br><span class="line">INFO  : Total jobs = 2</span><br><span class="line">INFO  : Launching Job 1 out of 2</span><br><span class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0010, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0010/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0010</span><br><span class="line">INFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:52:46,500 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:52:50,688 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec</span><br><span class="line">INFO  : 2019-09-17 14:52:55,872 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.8 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 2 seconds 800 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0010</span><br><span class="line">INFO  : Launching Job 2 out of 2</span><br><span class="line">INFO  : Starting task [Stage-2:MAPRED] in serial mode</span><br><span class="line">INFO  : Number of reduce tasks not specified. Estimated from input data size: 1</span><br><span class="line">INFO  : In order to change the average load for a reducer (in bytes):</span><br><span class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">INFO  : In order to limit the maximum number of reducers:</span><br><span class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">INFO  : In order to set a constant number of reducers:</span><br><span class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">INFO  : Starting Job = job_1568699800773_0011, Tracking URL = http://hadoop101:8088/proxy/application_1568699800773_0011/</span><br><span class="line">INFO  : Kill Command = /home/double_happy/app/hadoop/bin/hadoop job  -kill job_1568699800773_0011</span><br><span class="line">INFO  : Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1</span><br><span class="line">INFO  : 2019-09-17 14:53:03,511 Stage-2 map = 0%,  reduce = 0%</span><br><span class="line">INFO  : 2019-09-17 14:53:09,752 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.89 sec</span><br><span class="line">INFO  : 2019-09-17 14:53:15,983 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.4 sec</span><br><span class="line">INFO  : MapReduce Total cumulative CPU time: 3 seconds 400 msec</span><br><span class="line">INFO  : Ended Job = job_1568699800773_0011</span><br><span class="line">INFO  : MapReduce Jobs Launched: </span><br><span class="line">INFO  : Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.8 sec   HDFS Read: 17080 HDFS Write: 6962 SUCCESS</span><br><span class="line">INFO  : Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.4 sec   HDFS Read: 14832 HDFS Write: 137 SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU Time Spent: 6 seconds 200 msec</span><br><span class="line">INFO  : Completed executing command(queryId=double_happy_20190917145252_c7075e67-87d3-4c1d-9d14-0944dfcdbdca); Time taken: 36.913 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">+------+---------+--------+-------+--+</span><br><span class="line">| pid  |   uid   | count  | rank  |</span><br><span class="line">+------+---------+--------+-------+--+</span><br><span class="line">| a    | user15  | 11     | 1     |</span><br><span class="line">| a    | user62  | 11     | 1     |</span><br><span class="line">| a    | user5   | 10     | 3     |</span><br><span class="line">| b    | user37  | 10     | 1     |</span><br><span class="line">| b    | user61  | 10     | 1     |</span><br><span class="line">| b    | user15  | 9      | 3     |</span><br><span class="line">| b    | user79  | 9      | 3     |</span><br><span class="line">| c    | user66  | 13     | 1     |</span><br><span class="line">| c    | user94  | 12     | 2     |</span><br><span class="line">| c    | user81  | 11     | 3     |</span><br><span class="line">+------+---------+--------+-------+--+</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Scala06-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/17/Scala06-double-happy/">Scala06--double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2017/12/17/Scala06-double-happy/" class="article-date">
  <time datetime="2017-12-17T11:52:21.000Z" itemprop="datePublished">2017-12-17</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="隐式转换"><a href="#隐式转换" class="headerlink" title="隐式转换"></a>隐式转换</h2><p>存在的目的：增强</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala里有三种：</span><br><span class="line">	隐式参数</span><br><span class="line">	隐式类型转换</span><br><span class="line">	隐式类</span><br></pre></td></tr></table></figure></div>
<p>隐式类型转换：****</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">eg：</span><br><span class="line">	A类型 ==》B类型 B对A已有的东西 进行增强（是感知不到的）</span><br><span class="line">scala中 File这个类原声的并没有类似与count，read方法   但是</span><br><span class="line">我们是可以通过隐式转换来增强File中并没有提供的方法</span><br></pre></td></tr></table></figure></div>
<p>这个东西是双刃剑 用不好 代码流程你可能都看不明白</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">需求：如何为一个已存在的类添加一个新方法？</span><br><span class="line"></span><br><span class="line">Java：使用代理</span><br><span class="line">scala：使用Implicit</span><br><span class="line"></span><br><span class="line">1.定义隐式转换函数</span><br><span class="line"></span><br><span class="line">implict def man2superman（man:Man）:Surperman = new Superman(man.name)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">代码：</span><br><span class="line">object ImplicitApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val bfx = new Surperman(&quot;bfx&quot;)</span><br><span class="line"></span><br><span class="line">    bfx.fly()</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      *隐式类型转换</span><br><span class="line">      * 1。定义隐式转换函数</span><br><span class="line">      *</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">    //1。</span><br><span class="line">    implicit def man2surperman(man:Man):Surperman= &#123;</span><br><span class="line">      new Surperman(man.name)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val double_happy = new Man(&quot;double_happy&quot;)</span><br><span class="line">    double_happy.fly()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class Man(val name:String)</span><br><span class="line"></span><br><span class="line">  class Surperman(val name : String)&#123;</span><br><span class="line"></span><br><span class="line">    def fly(): Unit =&#123;</span><br><span class="line">      println(s&quot;$name can fly...&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求二：scala中 File这个类原声的并没有类似与count，read方法   但是</span><br><span class="line">我们是可以通过隐式转换来增强File中并没有提供的方法</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">代码：</span><br><span class="line">	object ImplicitApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val bfx = new Surperman(&quot;bfx&quot;)</span><br><span class="line"></span><br><span class="line">    bfx.fly()</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      *隐式类型转换</span><br><span class="line">      * 1。定义隐式转换函数</span><br><span class="line">      *</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">    //1。</span><br><span class="line">    implicit def man2surperman(man:Man):Surperman= &#123;</span><br><span class="line">      new Surperman(man.name)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val double_happy = new Man(&quot;double_happy&quot;)</span><br><span class="line">    double_happy.fly()</span><br><span class="line"></span><br><span class="line">    //1.2 File 增强</span><br><span class="line">    implicit def file2RichFile(file: File)=new RichFile(file)</span><br><span class="line"></span><br><span class="line">    val file = new File(&quot;/Users/double_happy/zz/G7-03/工程/scala-flink/doc/implicit/file.txt&quot;)</span><br><span class="line"></span><br><span class="line">    println(file.read())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  class Man(val name:String)</span><br><span class="line"></span><br><span class="line">  class Surperman(val name : String)&#123;</span><br><span class="line"></span><br><span class="line">    def fly(): Unit =&#123;</span><br><span class="line">      println(s&quot;$name can fly...&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  //2.增强File</span><br><span class="line"></span><br><span class="line">  class RichFile(file:File)&#123;</span><br><span class="line"></span><br><span class="line">    def read() =&#123;</span><br><span class="line"></span><br><span class="line">      //文件的路径+文件名字</span><br><span class="line">       Source.fromFile(file.getPath).mkString</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>这样写 代码里全是implicit 比较乱 最好把他们抽取出来放到一个Obeject里面</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">隐式类型转换   在spark-core RDD里面有很多，</span><br><span class="line">其中，RDD obeject里面就是把大部分的implicit放到这里面的</span><br><span class="line">eg：</span><br><span class="line">object RDD &#123;</span><br><span class="line"></span><br><span class="line">  private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS =</span><br><span class="line">    &quot;spark.checkpoint.checkpointAllMarkedAncestors&quot;</span><br><span class="line"></span><br><span class="line">  // The following implicit functions were in SparkContext before 1.3 and users had to</span><br><span class="line">  // `import SparkContext._` to enable them. Now we move them here to make the compiler find</span><br><span class="line">  // them automatically. However, we still keep the old functions in SparkContext for backward</span><br><span class="line">  // compatibility and forward to the following functions directly.</span><br><span class="line"></span><br><span class="line">  implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])</span><br><span class="line">    (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = &#123;</span><br><span class="line">    new PairRDDFunctions(rdd)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = &#123;</span><br><span class="line">    new AsyncRDDActions(rdd)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p>隐式参数转换</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">隐式参数</span><br><span class="line">	方法/函数的参数可以使用implicit修饰</span><br><span class="line">	效果就是：</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">eg：</span><br><span class="line">  //Int 类型就具有 add方法了 是不是很神奇</span><br><span class="line">  implicit class Cal(x :Int)&#123;</span><br><span class="line"></span><br><span class="line">    def add(a:Int) = a+x</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  //File类增强</span><br><span class="line">  implicit class FileEnhance(file: File)&#123;</span><br><span class="line"></span><br><span class="line">    def read2() = Source.fromFile(file.getPath).mkString</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 隐式类：</span><br><span class="line">      *</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val file2 = new File(&quot;/Users/double_happy/zz/G7-03/工程/scala-flink/doc/implicit/file.txt&quot;)</span><br><span class="line">    </span><br><span class="line">    println(file2.read2())</span><br><span class="line"></span><br><span class="line">    println(4.add(2))</span><br></pre></td></tr></table></figure></div>

<h2 id="Scala泛型"><a href="#Scala泛型" class="headerlink" title="Scala泛型"></a>Scala泛型</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * scala的泛型 ： 类型的约束</span><br><span class="line">  */</span><br><span class="line">object GeneticApp &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val mm1 = new MM[Int,CupEnum,Int](90,CupEnum.A,175)</span><br><span class="line">    val mm2 = new MM[Int,CupEnum,Int](10,CupEnum.F,150)</span><br><span class="line">    val mm3 = new MM[Int,CupEnum,Int](80,CupEnum.B,165)</span><br><span class="line"></span><br><span class="line">    println(mm1)</span><br><span class="line">    println(mm2)</span><br><span class="line">    println(mm3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">abstract class Msg[T](content:T)</span><br><span class="line"></span><br><span class="line">class WeChatMsg(content:String) extends Msg(content)</span><br><span class="line"></span><br><span class="line">class DigitMsg[Int](content:Int) extends Msg(content)</span><br><span class="line"></span><br><span class="line">class MM [A,B,C](val faceValue:A,val cap:B,val height:C)&#123;</span><br><span class="line"></span><br><span class="line">  override def toString: String = faceValue +&quot;\t&quot;+cap+&quot;\t&quot;+height</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//scala中枚举的使用  固定写法</span><br><span class="line"></span><br><span class="line">object CupEnum extends Enumeration &#123;</span><br><span class="line"></span><br><span class="line">  type  CupEnum = Value</span><br><span class="line"></span><br><span class="line">  val A,B,C,D,E,F = Value</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<h2 id="Scala中的排序"><a href="#Scala中的排序" class="headerlink" title="Scala中的排序"></a>Scala中的排序</h2><p>对比java</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">两种：</span><br><span class="line">1.sort里的 Comparator</span><br><span class="line">2.实现 Comparable</span><br><span class="line"></span><br><span class="line">public class MM  implements Comparable&lt;MM&gt;&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        MM mm1 = new MM(&quot;mm1&quot;, 32);</span><br><span class="line">        MM mm2 = new MM(&quot;mm2&quot;, 34);</span><br><span class="line">        MM mm3 = new MM(&quot;mm3&quot;, 31);</span><br><span class="line"></span><br><span class="line">        List&lt;MM&gt; mms = new ArrayList&lt;&gt;();</span><br><span class="line">        mms.add(mm1);</span><br><span class="line">        mms.add(mm2);</span><br><span class="line">        mms.add(mm3);</span><br><span class="line">        // 1. 第一种方式 排序cpm01(mms);</span><br><span class="line">        //2。第二种方式 排序</span><br><span class="line"></span><br><span class="line">        cmp02(mms);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void cmp02(List&lt;MM&gt; mms) &#123;</span><br><span class="line">        Collections.sort(mms);</span><br><span class="line">        for (MM mm : mms) &#123;</span><br><span class="line">            System.out.println(mm);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void cpm01(List&lt;MM&gt; mms) &#123;</span><br><span class="line">        //排序</span><br><span class="line">        Collections.sort(mms, new Comparator&lt;MM&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public int compare(MM o1, MM o2) &#123;</span><br><span class="line">                return o1.cup - o2.cup;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        for (MM mm : mms) &#123;</span><br><span class="line">            System.out.println(mm);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line"></span><br><span class="line">    private int cup;</span><br><span class="line"></span><br><span class="line">    public MM() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public MM(String name, int cup) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">        this.cup = cup;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getName() &#123;</span><br><span class="line">        return name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setName(String name) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int getCup() &#123;</span><br><span class="line">        return cup;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setCup(int cup) &#123;</span><br><span class="line">        this.cup = cup;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;MM&#123;&quot; +</span><br><span class="line">                &quot;name=&apos;&quot; + name + &apos;\&apos;&apos; +</span><br><span class="line">                &quot;, cup=&quot; + cup +</span><br><span class="line">                &apos;&#125;&apos;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public int compareTo(MM o) &#123;</span><br><span class="line">        return -(this.cup - o.cup);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala排序与java相对应的：</span><br><span class="line">	1. Ordering  ==&gt;comparator</span><br><span class="line">	2. Ordered ==&gt;comparable</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">java里的上下界</span><br><span class="line">	上界（upper bounds）&lt;T extends Test &gt; T 可以是Test的子类型   &lt;? extends Test&gt;</span><br><span class="line">	下界(lower bounds)   &lt; T super Test &gt; T可以是Test的父类型			&lt;? super Test &gt;</span><br><span class="line"></span><br><span class="line">Scala 里的上下界</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object UpperLowerBountsApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //3.Man 排序</span><br><span class="line"></span><br><span class="line">//    val man1 = new Man(&quot;double&quot;,24)</span><br><span class="line">//    val man2 = new Man(&quot;xiao fang&quot;,18)</span><br><span class="line">//</span><br><span class="line">//    println(new MaxValue(man1,man2).compare)</span><br><span class="line">//    println(new MaxValue2(man1,man2).compare)</span><br><span class="line"></span><br><span class="line">    //3.2</span><br><span class="line"></span><br><span class="line">    val man3 = new Man2(&quot;double&quot;,24)</span><br><span class="line">    val man4 = new Man2(&quot;xiao fang&quot;,18)</span><br><span class="line"></span><br><span class="line">   // println(new MaxValue2(man3,man4).compare)  //是不行的 Man2需要隐式转换 或者 实现compareble接口 或者 extends  Ordered[Man2]</span><br><span class="line"></span><br><span class="line">    implicit def man2ToOrderedMan2(man2: Man2) = new Ordered[Man2] &#123;</span><br><span class="line">      override def compare(that: Man2): Int = that.age - man2.age</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(new MaxValue2(man3,man4).compare)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //3.3 上下文界定</span><br><span class="line"></span><br><span class="line">    implicit val cpmtor = new Ordering[Man2] &#123;</span><br><span class="line">      override def compare(x: Man2, y: Man2): Int = -(x.age -y.age)</span><br><span class="line">    &#125;</span><br><span class="line">    println(new MaxValue3(man3,man4).compare())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //0。。</span><br><span class="line"></span><br><span class="line">    val maxInt = new MaxInt(3,6)</span><br><span class="line"></span><br><span class="line">    println(maxInt.compare())</span><br><span class="line"></span><br><span class="line">    //1。需要需要写泛型 如果实现compareble接口 就不用写</span><br><span class="line">    val maxValue = new MaxValue[Integer](6,10) //Int 没有实现compareble接口</span><br><span class="line"></span><br><span class="line">    println(maxValue.compare)</span><br><span class="line"></span><br><span class="line">    //2。使用视图界定 不用写泛型</span><br><span class="line">    val maxValue2 = new MaxValue2(13,15)</span><br><span class="line"></span><br><span class="line">    println(maxValue2.compare)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//求最大值  Int 类型的</span><br><span class="line">class  MaxInt(x :Int , y:Int)&#123;</span><br><span class="line"></span><br><span class="line">  def compare()=&#123;</span><br><span class="line">    if(x &gt; y) x else y</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class  MaxLong(x :Long , y:Long)&#123;</span><br><span class="line"></span><br><span class="line">  def compare()=&#123;</span><br><span class="line">    if(x &gt; y) x else y</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//引入 scala里的上界  这里是表示 T是Comparable[T]的子类型</span><br><span class="line">class  MaxValue[T &lt;: Comparable[T]] (x:T,y:T)&#123;</span><br><span class="line"></span><br><span class="line">  def compare = if(x.compareTo(y) &gt;0)  x else  y</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//视图界定 ： 底层是使用隐式转换的</span><br><span class="line">class  MaxValue2[T &lt;% Comparable[T]] (x:T,y:T)&#123;</span><br><span class="line"></span><br><span class="line">  def compare = if(x.compareTo(y) &gt;0)  x else  y</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//上下文界定</span><br><span class="line">class  MaxValue3[T : Ordering](x : T,y:T)(implicit cpmtor:Ordering[T])&#123;</span><br><span class="line"></span><br><span class="line">  def compare() =if(cpmtor.compare(x,y) &gt; 0) x else y</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Man(val name :String , val age : Int) extends  Ordered[Man]&#123;</span><br><span class="line"></span><br><span class="line">  override def compare(that: Man): Int = that.age - age</span><br><span class="line"></span><br><span class="line">  override def toString: String = name + &quot;\t&quot; + age</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Man2(val name :String , val age : Int) &#123;</span><br><span class="line"></span><br><span class="line">  override def toString: String = name + &quot;\t&quot; + age</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">总结下来 scala里排序：  那些界定 能看懂即可 </span><br><span class="line">	不管你使用什么界定 ，转换的类不用说 里面是一定有compare的</span><br><span class="line">	bean类 不管是隐式转换也好 还是继承或实现 ordered ，最终 bean类 和转换类 结合使用的时候 他们都存在</span><br><span class="line">	类似compare的东西 才不会报错！！！</span><br></pre></td></tr></table></figure></div>




<h2 id="逆变和协变"><a href="#逆变和协变" class="headerlink" title="逆变和协变"></a>逆变和协变</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">scala里 泛型类型是不可变的  本意  但是人为的让他可以</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * scala的泛型 ： 类型的约束</span><br><span class="line">  */</span><br><span class="line">object GeneticApp &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //1. 泛型</span><br><span class="line">    val mm1 = new MM[Int,CupEnum,Int](90,CupEnum.A,175)</span><br><span class="line">    val mm2 = new MM[Int,CupEnum,Int](10,CupEnum.F,150)</span><br><span class="line">    val mm3 = new MM[Int,CupEnum,Int](80,CupEnum.B,165)</span><br><span class="line"></span><br><span class="line">    println(mm1)</span><br><span class="line">    println(mm2)</span><br><span class="line">    println(mm3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //2.nb  xb</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 泛型类型是不可变的</span><br><span class="line">      *</span><br><span class="line">      *  [UserA] 能不能放 Child</span><br><span class="line">      *  eg：val test: Test[UserA] = new Test[Child]     //默认是不行的哈 泛型类型是不可变的</span><br><span class="line">      *</span><br><span class="line">      *  UserA ==》 Child  协变   补充增强    在Test里属性参数 加一个+ 号即可实现</span><br><span class="line">      *</span><br><span class="line">      *  UserA ==》Person 逆变   减少    在Test里属性参数 加一个- 号即可实现</span><br><span class="line">      *</span><br><span class="line">      */</span><br><span class="line">    val test: Test[UserA] = new Test[UserA]</span><br><span class="line">    val test1: Test[UserA] = new Test[Child]</span><br><span class="line">    println(test)</span><br><span class="line">    println(test1)</span><br><span class="line"></span><br><span class="line">//    val test3: Test[UserA] = new Test[Person]</span><br><span class="line">    println(test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //使用场景</span><br><span class="line">    val list = List(1,2,3,4,5)</span><br><span class="line">   // list.reduceLeft[UserA]()  点开看一下 就知道  这是一个下界 返回值类型 就是 UserA</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def test[T](t:T)=println(t)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Person</span><br><span class="line"></span><br><span class="line">class  UserA extends  Person</span><br><span class="line"></span><br><span class="line">class Child extends  UserA</span><br><span class="line"></span><br><span class="line">class Test[+UserA]</span><br><span class="line"></span><br><span class="line">abstract class Msg[T](content:T)</span><br><span class="line"></span><br><span class="line">class WeChatMsg(content:String) extends Msg(content)</span><br><span class="line"></span><br><span class="line">class DigitMsg[Int](content:Int) extends Msg(content)</span><br><span class="line"></span><br><span class="line">class MM [A,B,C](val faceValue:A,val cap:B,val height:C)&#123;</span><br><span class="line"></span><br><span class="line">  override def toString: String = faceValue +&quot;\t&quot;+cap+&quot;\t&quot;+height</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//scala中枚举的使用  固定写法</span><br><span class="line"></span><br><span class="line">object CupEnum extends Enumeration &#123;</span><br><span class="line"></span><br><span class="line">  type  CupEnum = Value</span><br><span class="line"></span><br><span class="line">  val A,B,C,D,E,F = Value</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<h2 id="Scala操作JDBC"><a href="#Scala操作JDBC" class="headerlink" title="Scala操作JDBC"></a>Scala操作JDBC</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.pom.xml  添加jdbc</span><br><span class="line">2.开发</span><br><span class="line">	最基础的方法</span><br><span class="line">	2.使用scalikejdbc</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">object ScalalikJDBCAPP &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def  insert()=&#123;</span><br><span class="line">    DB.autoCommit(&#123;</span><br><span class="line">      implicit session =&gt;&#123;</span><br><span class="line">        SQL(&quot;insert into tmp(topic,groupid,partititions,offset) values(?,?,?,?)&quot;)</span><br><span class="line">          .bind(&quot;happy&quot;,&quot;test-happy-group&quot;,3,8)    //插入具体的值</span><br><span class="line">          .update().apply()  //执行</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def update(): Unit = &#123;</span><br><span class="line">    DB.autoCommit(&#123;</span><br><span class="line">      implicit session =&gt;&#123;</span><br><span class="line">        SQL(&quot;update  tmp(topic,groupid,partititions,offset) set offset=? where topic=? and groupid=? and partititions=? and offset=? &quot;)</span><br><span class="line">          .bind(18,&quot;happy&quot;,&quot;test-happy-group&quot;,3)    //修改具体的值</span><br><span class="line">          .update().apply()  //执行</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def query(): Unit = &#123;</span><br><span class="line">    val queryresult = DB.readOnly(&#123;</span><br><span class="line">      implicit session =&gt; &#123;</span><br><span class="line">        SQL(&quot;select * from tmp &quot;).map(rs =&gt; &#123;</span><br><span class="line"></span><br><span class="line">          tmp(</span><br><span class="line">            rs.string(&quot;topic&quot;),</span><br><span class="line">            rs.string(&quot;groupid&quot;),</span><br><span class="line">            rs.int(&quot;partitions&quot;),</span><br><span class="line">            rs.long(&quot;offset&quot;)</span><br><span class="line">          )</span><br><span class="line">        &#125;).list().apply() //执行</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    queryresult.foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  case class tmp(topic:String,groupid:String,partitions:Int,offset:Long)</span><br><span class="line"></span><br><span class="line">  def delete(): Unit = &#123;</span><br><span class="line">    DB.autoCommit(&#123;</span><br><span class="line">      implicit session =&gt;&#123;</span><br><span class="line">        SQL(&quot;delete from tmp where partition=? &quot;)</span><br><span class="line">          .bind(3)    //删除具体的值</span><br><span class="line">          .update().apply()  //执行</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def transaction(): Unit = &#123;</span><br><span class="line">    DB.localTx(&#123;</span><br><span class="line">      implicit session =&gt;&#123;</span><br><span class="line">        SQL(&quot;delete from tmp where partition=? &quot;)</span><br><span class="line">          .bind(3)    //删除具体的值</span><br><span class="line">          .update().apply()  //执行</span><br><span class="line"></span><br><span class="line">        //1/0   测试用  加上即  3删除了2没有删除 如果事务保证 那么谁都不会删除 ***</span><br><span class="line"></span><br><span class="line">        SQL(&quot;delete from tmp where partition=? &quot;)</span><br><span class="line">          .bind(2)    //删除具体的值</span><br><span class="line">          .update().apply()  //执行</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    //1.解析配置文件</span><br><span class="line">    DBs.setupAll()</span><br><span class="line"></span><br><span class="line">    //插入</span><br><span class="line">    insert()</span><br><span class="line"></span><br><span class="line">    //修改</span><br><span class="line">    update()</span><br><span class="line"></span><br><span class="line">    //查询</span><br><span class="line">    query()</span><br><span class="line"></span><br><span class="line">    //删除</span><br><span class="line">    delete()</span><br><span class="line"></span><br><span class="line">    //事务</span><br><span class="line">    transaction()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Flume03-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/13/Flume03-double-happy/">Flume03--double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2017/12/13/Flume03-double-happy/" class="article-date">
  <time datetime="2017-12-13T11:51:37.000Z" itemprop="datePublished">2017-12-13</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="Flume核心组件"><a href="#Flume核心组件" class="headerlink" title="Flume核心组件"></a>Flume核心组件</h2><p>六大组件：Source、Interceptors、 Channel Selectors、channel、Sink、Sink Processors<br>事务  三大核心组件自定义开发 这些是必须要掌握的重点</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果整个流程的事务不能保证好的话 会产生两个问题？</span><br><span class="line">1.数据丢失   </span><br><span class="line">2.数据会重复</span><br><span class="line">为什么丢失？为什么重复？ 之后章节演示</span><br></pre></td></tr></table></figure></div>
<h2 id="多Agent"><a href="#多Agent" class="headerlink" title="多Agent"></a>多Agent</h2><p>Flume：Agent的技术选型 没有对与错只有合适不合适</p>
<p>案例1</p>
<p><img src="https://img-blog.csdnimg.cn/2019091023540096.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTU1NzUxMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">三个 agent 配置</span><br><span class="line">    待续。。。。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">	1.多个Agent进行传输 选择</span><br><span class="line">			nc：avro sink</span><br><span class="line">			taildir：avro sink</span><br><span class="line">			source: avro source</span><br><span class="line">	2.A1 和A2可能是不同业务的数据 格式也可能是不一样的 可以按这个图接 ，接到A3之后是要加一些标识的，明确知道日志是什么类型的，</span><br><span class="line">	通过event进来之后进过一系列拦截器链可以设置header信息来区别日志</span><br></pre></td></tr></table></figure></div>
<h2 id="优化上面的图"><a href="#优化上面的图" class="headerlink" title="优化上面的图"></a>优化上面的图</h2><p>A3数据从channel出来直接写道一个sink里去的，工作当中肯定是不行的，sink挂掉就gg了。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Sink Processors：</span><br><span class="line">	1.load balance</span><br><span class="line">	2.failover</span><br></pre></td></tr></table></figure></div>
<p><strong>Sink Processors：</strong><br><img src="https://img-blog.csdnimg.cn/2019091100054664.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTU1NzUxMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>load balance</strong><br>        一个sink发一些数据，另外sink发送一些数据（轮询 和随机）<img src="https://img-blog.csdnimg.cn/20190911000928562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTU1NzUxMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>案例2：</p>
<p><img src="https://img-blog.csdnimg.cn/20190911001420721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTU1NzUxMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">配置待续。。。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">	1.A0需要sink组</span><br></pre></td></tr></table></figure></div>

<p><strong>failover</strong><br><img src="https://img-blog.csdnimg.cn/2019091100164313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTU1NzUxMg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">	1.优先级高的sink发送数据 另一个sink是不发送数据的      值越大的优先级越大</span><br><span class="line">		优先级高的sink挂掉之后会走优先级低的sink</span><br></pre></td></tr></table></figure></div>


<h2 id="Flume优化的东西要注意的"><a href="#Flume优化的东西要注意的" class="headerlink" title="Flume优化的东西要注意的"></a>Flume优化的东西要注意的</h2><p>面试题：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">QA1：谈谈你在工作中针对Flume的调优有哪些</span><br><span class="line">	Source：</span><br><span class="line">		文件：TailDir   注意 默认是不支持递归的     </span><br><span class="line">		     </span><br><span class="line">		网络：avro</span><br><span class="line">	Channel：</span><br><span class="line">		Memory</span><br><span class="line">		File</span><br><span class="line">		capacity</span><br><span class="line">		transactionCapacity</span><br><span class="line">		source/sink: batchSize</span><br><span class="line">		</span><br><span class="line">	Sink：配置sink的个数多和少  吞吐量的问题 </span><br><span class="line">	不是越多越好 每一个sink都是jvm进程 </span><br><span class="line">	sink的batchsize决定出来多少数据</span><br><span class="line">	</span><br><span class="line">	Flume Agent架构 ：</span><br><span class="line">		failover和load balance  什么场景可用性最高 </span><br><span class="line">		一起用可以 但是对机器要求也高 ，退而求其次 ，但是failover是一定要配置的</span><br><span class="line"></span><br><span class="line">QA2：谈谈你们使用Flume过程中时如何监控的</span><br><span class="line">    主要是监控 channel    如果channle数据多了 说明数据挤压 sink说明出问题了</span><br><span class="line">    如果sink出现问题了，意味着source 疯狂的往channle里入数据，但是这时候没有sink消费channel数据 </span><br><span class="line">    那么 channle早晚会爆掉，随之 source也写不进去了。</span><br><span class="line">Ganglia，json-》es-&gt;kabanna展示 都可以做监控</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. TailDir   Source：</span><br><span class="line">	注意：</span><br><span class="line">	如果某一个目录下面或者某几个目录下面的数据文件非常非常多，</span><br><span class="line">	1.最好增加filegroups的个数，这样吞吐量肯定能上去 因为你处理的是不同的文件</span><br><span class="line">     2.对于source 来讲里面是有一个 batchsize的 （参考值 10000-50000）</span><br><span class="line">         Max number of lines to read and send to the channel at a time.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2. channel</span><br><span class="line">      选择memory 和file 看你能不能接受丢少量数据 mem达到99.99%才会丢</span><br><span class="line">      不要采用kafka channel 一个flume框架就可以了 模型当中你又引用一个kafka进来，出问题的可能性成指数增长 </span><br><span class="line">      </span><br><span class="line">      capacity（channel里能存多少个event）</span><br><span class="line">      		The maximum number of events stored in the channel</span><br><span class="line">	  transactionCapacity（souce往channle里塞多少数据 和 sink从channle里取多少数据  事务的时候）</span><br><span class="line">	  		The maximum number of events the channel will take from a source or give to a sink per transaction</span><br><span class="line"></span><br><span class="line">transactionCapacity  是大于capacity</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Flume02-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/12/11/Flume02-double-happy/">Flume02--double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2017/12/11/Flume02-double-happy/" class="article-date">
  <time datetime="2017-12-11T11:50:48.000Z" itemprop="datePublished">2017-12-11</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="Flume的agent配置模板"><a href="#Flume的agent配置模板" class="headerlink" title="Flume的agent配置模板"></a>Flume的agent配置模板</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">agent_name: 配置的agent的名称</span><br><span class="line">a1：就是agent的名称（名字随便起的哈）</span><br><span class="line"></span><br><span class="line"># Name the components on this agent</span><br><span class="line">&lt;agent_name&gt;.sources = &lt;source_name&gt;</span><br><span class="line">&lt;agent_name&gt;.sinks = &lt;sink_name&gt;</span><br><span class="line">&lt;agent_name&gt;.channels = &lt;channel_name&gt;</span><br><span class="line"></span><br><span class="line">&lt;agent_name&gt;.sources.&lt;source_name&gt;.type = xx</span><br><span class="line">&lt;agent_name&gt;.sinks.&lt;sink_name&gt;.type = yyy</span><br><span class="line">&lt;agent_name&gt;.channels.&lt;channel_name&gt;.type = zzz</span><br><span class="line"></span><br><span class="line">&lt;agent_name&gt;.sources.&lt;source_name&gt;.channels = &lt;channel_name&gt;</span><br><span class="line">&lt;agent_name&gt;.sinks.&lt;sink_name&gt;.channel = &lt;channel_name&gt;</span><br></pre></td></tr></table></figure></div>
<p>基于上一篇文章末尾引出一个，</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从指定的网络端口上采集日志到控制台输出</span><br></pre></td></tr></table></figure></div>
<p>那么Flume支持的source、channel、sink有哪些呢？</p>
<h2 id="2-Flume支持的source、channel、sink有哪些呢？"><a href="#2-Flume支持的source、channel、sink有哪些呢？" class="headerlink" title="2.Flume支持的source、channel、sink有哪些呢？"></a>2.Flume支持的source、channel、sink有哪些呢？</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">source：</span><br><span class="line">	avro（是rpc服务框架）</span><br><span class="line">	exec （是监控一个文件） ： tail -F  xx.log（大F和小f有区别）后面只能接一个文件(小f是动态的)</span><br><span class="line">	Spooling Directory: 能监控一个文件夹（文件夹下不能有子文件夹的）</span><br><span class="line">	Taildir（是既能监控文件又能监控文件夹）**</span><br><span class="line">	netcat</span><br><span class="line">sink：</span><br><span class="line">	HDFS</span><br><span class="line">	logger（写到控制台）</span><br><span class="line">	avro : 配合avro source使用</span><br><span class="line">	kafka	</span><br><span class="line">channel：只是数据的存储不涉及到数据的缓存</span><br><span class="line">	memory</span><br><span class="line">	file</span><br><span class="line">Agent：各种组合source、channel、sink之间的关系</span><br></pre></td></tr></table></figure></div>
<h2 id="3-案例分析"><a href="#3-案例分析" class="headerlink" title="3.案例分析"></a>3.案例分析</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">eg：为了学习   这个思路可以借鉴      生产上直接用 Taildir</span><br><span class="line">   把一个文件中新增的内容收集到HDFS上去</span><br><span class="line">	exec - memory - hdfs</span><br><span class="line">    一个文件夹</span><br><span class="line">	spooling - memory - hdfs</span><br><span class="line">    文件数据写入kafka</span><br><span class="line">	exec - memory - kafka</span><br></pre></td></tr></table></figure></div>
<h2 id="4-实战"><a href="#4-实战" class="headerlink" title="4.实战"></a>4.实战</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求1：采集指定文件的内容到HDFS</span><br><span class="line">技术选型：exec - memory - hdfs</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agent  待续。。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">需求1总结：</span><br><span class="line">batchSize :积累多少个event 刷到hdfs上去</span><br><span class="line">fileType：默认是secqueceFile   ,DataStream可以理解为文本（没有压缩的）</span><br><span class="line">writeFormat:默认是Writable</span><br><span class="line"></span><br><span class="line">hdfs上一个128m的文件和1kb文件都各自占用一个block，而一个block有一个元数据信息，</span><br><span class="line">元数据多了会占用namenode的内存，元数据过多会导致namenode挂掉 （所以注意 设置文件多大合适呢？）</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需求2：采集指定文件夹的内容到控制台</span><br><span class="line">选型：spooling - memory - logger</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agent  待续。。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">需求2总结：</span><br><span class="line">如果监控的文件夹下进来一个文件，那个文件处理完以后，会在那个文件后面加一个COMPLETED标识 ，</span><br><span class="line">但是为什么同一个文件进去两次flume就挂掉了呢 ， 在spooling监控的文件夹下的文件如果被处理过以后，</span><br><span class="line">再给这个文件内容追加写或者改就会报错，如果处理过后的文件的文件名在后面又被用到了，也会报错。</span><br><span class="line">也就是文件名不能有重复的</span><br><span class="line">如果agent里fileHeader参数设置为true，默认event 里的header的key就是文件路径+文件名</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
  </nav>


</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://sxwanggit126.github.io/" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>

  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>