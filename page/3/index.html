<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="DoubleHappy or Jepson">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;3&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main">
  
    <article id="post-Spark10-内存管理" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/26/Spark10-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/">Spark10--内存管理</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/26/Spark10-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" class="article-date">
  <time datetime="2018-01-26T12:05:30.000Z" itemprop="datePublished">2018-01-26</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ spark-submit --help</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn </span><br><span class="line">提交到yarn的时候 肯定是有一堆executor进程的对吧  那么 到底有几个呢？每个executor 多少core？每个executor 多少内存？</span><br><span class="line">这些是提交作业的时候都要配置的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> on yarn 模式</span><br><span class="line"> --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">--executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode）</span><br><span class="line">                              					</span><br><span class="line"> per executor到底使用几个cpu core呢？</span><br><span class="line"></span><br><span class="line"> --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line"></span><br><span class="line">之前我就遇到一个问题就是 </span><br><span class="line">我的数据量 没有我给的executor内存大 为什么程序跑不出来呢？还oom呢？</span><br><span class="line"></span><br><span class="line">说明对spark内存管理不了解</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Tuning Spark</a><br>优化章节<br><a href="http://spark.apache.org/docs/latest/tuning.html#memory-management-overview" target="_blank" rel="noopener">Memory Management Overview</a></p>
<p>Memory usage in Spark largely falls under one of two categories: <strong>execution and storage</strong>. <strong>Execution</strong> memory refers to that used for computation in <strong>shuffles, joins, sorts and aggregations</strong>, while <strong>storage</strong> memory refers to that used for <strong>caching and propagating internal data across the cluster.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">内存分为：</span><br><span class="line">    execution ： computation in shuffles, joins, sorts and aggregations  用于计算的</span><br><span class="line">	storage ：caching and propagating internal data   用于存储</span><br><span class="line"></span><br><span class="line">所以spark的executor内存是经过划分的 你给他1G 用于计算的达不到1G</span><br><span class="line">查看源码：</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">SparkEnv类：</span><br><span class="line"></span><br><span class="line">    val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</span><br><span class="line">    val memoryManager: MemoryManager =</span><br><span class="line">      if (useLegacyMemoryManager) &#123;</span><br><span class="line">        new StaticMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        UnifiedMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.memory.useLegacyMode 什么意思呢？去官网找一下</span><br><span class="line"></span><br><span class="line">决定你spark采用什么样的内存管理机制</span><br><span class="line">是否使用历史遗留版本  false</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark Configuration</a></p>
<p><img src="https://img-blog.csdnimg.cn/20191025155826318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>1.SparkEnv类 进去搜索memoryManager<br><img src="https://img-blog.csdnimg.cn/20191025161627818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>2.点进去StaticMemoryManager    </p>
<p><img src="https://img-blog.csdnimg.cn/20191025161814167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>3.点进去getMaxExecutionMemory 或者getMaxStorageMemory  点不进去 说明这个方法就在这个类里面<br>搜索getMaxExecutionMemory<br><img src="https://img-blog.csdnimg.cn/20191025161953166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StaticMemoryManager    历史遗留版本   静态内存管理</span><br><span class="line">UnifiedMemoryManager    统一内存管理</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">静态内存管理机制： 存储和执行是单独的</span><br><span class="line">  StaticMemoryManager &#123;</span><br><span class="line">	getMaxExecutionMemory&#123;</span><br><span class="line"></span><br><span class="line">    val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</span><br><span class="line">   ......</span><br><span class="line">	val memoryFraction = conf.getDouble(&quot;spark.shuffle.memoryFraction&quot;, 0.2) //默认0.2</span><br><span class="line">    val safetyFraction = conf.getDouble(&quot;spark.shuffle.safetyFraction&quot;, 0.8) //默认0.8</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong   //1000m*0.2*0.8 = 160m</span><br><span class="line"></span><br><span class="line">  你传进来1g 真正用来计算的Execution 才 160m </span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	getMaxStorageMemory&#123;</span><br><span class="line">    val memoryFraction = conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6)</span><br><span class="line">    val safetyFraction = conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9)</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong   //1000m*0.6*0.9 = 540m</span><br><span class="line"></span><br><span class="line">    你传进来1g 用来存储的的Storage 540m</span><br><span class="line">    如果你整个作业不需要 cache 不需要缓存 那么这个部分的内存就浪费掉了</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">systemMaxMemory 假设是传进来的内存 实际上比传进来的内存小一点</span><br><span class="line">所以你传进来的内存 是有个占比 有安全系数占比</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"> 统一内存管理:存储和执行内存是公用的 ==》会有相互借内存的</span><br><span class="line">UnifiedMemoryManager&#123;</span><br><span class="line">	val maxMemory = getMaxMemory(conf)&#123;</span><br><span class="line">		val systemMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</span><br><span class="line">		val reservedMemory = 300m</span><br><span class="line">		。。。。。</span><br><span class="line">		//1000m - 300m </span><br><span class="line">		val usableMemory = systemMemory - reservedMemory</span><br><span class="line">   		val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">    	(usableMemory * memoryFraction).toLong  //(1000m - 300m)*0.6 = 420m</span><br><span class="line"></span><br><span class="line">    	你真正能使用的内存 420m 这是存储端和执行端公有的 就这么多</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	//存储的占了0.5</span><br><span class="line">	onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">getMaxMemory  :Return the total amount of memory shared between execution and storage, in bytes.</span><br><span class="line">所以最终：</span><br><span class="line">Storage ： (1000m - 300m)*0.6*0.5 = 210m</span><br><span class="line">Execution ： 210m</span><br></pre></td></tr></table></figure></div>
<p>新版内存管理：<br>In Spark, execution and storage share a unified region (M). When <strong>no execution memory is used</strong>, <strong>storage can acquire all the available memory and vice versa.</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">no execution memory is used 那么storage 会获取所有资源</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Spark1.0版本：</span><br><span class="line">静态资源管理：</span><br><span class="line">	execution | storage        </span><br><span class="line">	1.如果storage用来做cache的很少 storage就剩余很多内存资源</span><br><span class="line">	那么execution 做sort、join、shuffle的如果 这部分的内存资源不够 只能 spill to disk 。</span><br><span class="line">	2.反过来</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">到了Spark1.6版本：</span><br><span class="line">统一内存管理：</span><br><span class="line">	名字都变了</span><br><span class="line">	execution|storage     默认各占50%</span><br><span class="line">	这块 execution的优先级高，storage如果cache的满了 把cache的数据spill to disk，</span><br><span class="line">	如果execution的不够用，那么他会去storage拿资源，极限情况下，只会给storage留一丢丢资源，不会让storage很没面子。但是 execution就是有借无还，谁让他优先级高呢。</span><br><span class="line">	还有一点就是 execution借完了之后 storage此时也需要cache 为什么execution不能还给storage内存呢？</span><br><span class="line">	如果此时execution 正在shuffle 还了内存 execution 会出现问题的 所以不能还</span><br></pre></td></tr></table></figure></div>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark009-spark-shell执行流程" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/26/Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/">Spark009--spark-shell执行流程</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/26/Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/" class="article-date">
  <time datetime="2018-01-26T12:04:44.000Z" itemprop="datePublished">2018-01-26</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="spark-shell脚本"><a href="#spark-shell脚本" class="headerlink" title="spark-shell脚本"></a>spark-shell脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-shell </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"># Shell script for starting the Spark Shell REPL</span><br><span class="line"></span><br><span class="line">cygwin=false     </span><br><span class="line">case &quot;$(uname)&quot; in     </span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"># Enter posix mode for bash</span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE=&quot;Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation&quot;</span><br><span class="line"></span><br><span class="line"># SPARK-4161: scala does not assume use of the java classpath,</span><br><span class="line"># so we need to add the &quot;-Dscala.usejavacp=true&quot; flag manually. We</span><br><span class="line"># do this specifically for the Spark shell because the scala REPL</span><br><span class="line"># has its own class loader, and any additional classpath specified</span><br><span class="line"># through spark.driver.extraClassPath is not automatically propagated.</span><br><span class="line">SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # Workaround for issue involving JLine and Cygwin</span><br><span class="line">    # (see http://sourceforge.net/p/jline/bugs/40/).</span><br><span class="line">    # If you&apos;re using the Mintty terminal emulator in Cygwin, may need to set the</span><br><span class="line">    # &quot;Backspace sends ^H&quot; setting in &quot;Keys&quot; section of the Mintty options</span><br><span class="line">    # (see https://github.com/sbt/sbt/issues/562).</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot;</span><br><span class="line">    &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in</span><br><span class="line"># binary distribution of Spark where Scala is not installed</span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=&quot;&quot;</span><br><span class="line"></span><br><span class="line"># restore stty settings (echo in particular)</span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=&quot;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ &quot;$saved_stty&quot; != &quot;&quot; ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># to reenable echo if we are interrupted before completing.</span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"># save terminal settings</span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"># clear on error so we don&apos;t later try to restore them</span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line"></span><br><span class="line"># record the exit status lest it be overwritten:</span><br><span class="line"># then reenable echo and propagate the code.</span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.  cygwin=false  //windows 电脑操作linux东西 要安装cygwin  </span><br><span class="line">2.  case &quot;$(uname)&quot; in       //uname 知道我们是什么操作系统</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025135328497.png" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ uname -r        //内核的版本</span><br><span class="line">3.10.0-514.26.2.el7.x86_64</span><br><span class="line">[double_happy@hadoop101 bin]$ uname -a      //打印所有的信息</span><br><span class="line">Linux hadoop101 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>
<p><strong>case in</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">      模式1)</span><br><span class="line">      		command</span><br><span class="line">      ;;</span><br><span class="line">      模式2)</span><br><span class="line">      		command</span><br><span class="line">      ;;</span><br><span class="line">      *)</span><br><span class="line">      	command</span><br><span class="line">      	;;</span><br><span class="line">      	esac</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat test.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">cygwin=false     </span><br><span class="line">case &quot;$(uname)&quot; in     </span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line">echo $cygwin</span><br><span class="line">[double_happy@hadoop101 script]$ sh test.sh </span><br><span class="line">false</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat case.sh </span><br><span class="line">read -p &quot;press key , then press return:&quot; KEY</span><br><span class="line">case $KEY in</span><br><span class="line">        [a-z]|[A-Z])</span><br><span class="line">echo &quot;this is a letter..&quot;;;</span><br><span class="line">        [0-9])</span><br><span class="line">echo &quot;this is a digit...&quot; ;;</span><br><span class="line">        *)</span><br><span class="line">echo &quot;other..&quot; ;;</span><br><span class="line">esac</span><br><span class="line">[double_happy@hadoop101 script]$ sh case.sh </span><br><span class="line">press key , then press return:1</span><br><span class="line">this is a digit...</span><br><span class="line">[double_happy@hadoop101 script]$ sh case.sh </span><br><span class="line">press key , then press return:a</span><br><span class="line">this is a letter..</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>
<p><strong>if -z</strong><br><a href="https://www.cnblogs.com/new-journey/p/11017659.html" target="_blank" rel="noopener">shell 参数</a></p>
<p>if [ -z “${SPARK_HOME}” ]; then<br>  source “$(dirname “$0”)”/find-spark-home<br>fi</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[ -z STRING ]  “STRING” 的长度为零则为真。  </span><br><span class="line">dirname 是什么？</span><br><span class="line">获取当前的路径</span><br><span class="line"></span><br><span class="line">find-spark-home是在bin目录下  这脚本里有 就是去export SPARK_HOME</span><br><span class="line"> source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home    目的就是找 SPARK_HOME</span><br><span class="line"> 所以你环境里配置了 SPARK_HOME if那个shell 就直接跳过去 就不用找SPARK_HOME</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat dirname.sh </span><br><span class="line">HOME=`cd $(dirname &quot;$0&quot;);pwd`</span><br><span class="line">echo $HOME</span><br><span class="line">[double_happy@hadoop101 script]$ sh dirname.sh </span><br><span class="line">/home/double_happy/script</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>

<p><strong>main</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">main &quot;$@&quot;  main方法里 ：</span><br><span class="line"></span><br><span class="line"> &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line"></span><br><span class="line">知道 spark-shell底层调用的是  spark-submit</span><br></pre></td></tr></table></figure></div>
<p><strong>$@</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat main.sh </span><br><span class="line">function main()&#123;</span><br><span class="line">        echo &quot;.....&quot;$@</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 script]$ sh main.sh abc </span><br><span class="line">.....abc</span><br><span class="line">[double_happy@hadoop101 script]$ sh main.sh abc 123 456</span><br><span class="line">.....abc 123 456</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line">$@ :就是把一堆输入的参数  带走</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat main.sh </span><br><span class="line">function main()&#123;</span><br><span class="line">        echo &quot;.....&quot;$@</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 script]$ mv main.sh spark-shell</span><br><span class="line">[double_happy@hadoop101 script]$ chmod +x spark-shell </span><br><span class="line">[double_happy@hadoop101 script]$ ./spark-shell --master yarn --jars mysql.driver.jar</span><br><span class="line">.....--master yarn --jars mysql.driver.jar</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">明白了吗 spark-shell的参数 就是这么传进去的</span><br></pre></td></tr></table></figure></div>
<h2 id="spark-submit脚本"><a href="#spark-submit脚本" class="headerlink" title="spark-submit脚本"></a>spark-submit脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-submit </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># disable randomized hash for string in Python 3.3+</span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br><span class="line"></span><br><span class="line">exec 是做什么的？</span><br><span class="line">就是一个执行的命令</span><br><span class="line">例如在当前shell中执行 exec ls  表示执行ls这条命令来替换当前的shell ，即为执行完后会退出当前shell。</span><br><span class="line"></span><br><span class="line">为了避免这个结果的影响，一般将exec命令放到一个shell脚本中，用主脚本调用这个脚本，调用处可以用bash  xx.sh(xx.sh为存放exec命令的脚本)，这样会为xx.sh建立一个子shell去执行，当执行exec后该子脚本进程就被替换成相应的exec的命令。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025145541791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>直接退出了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat exec.sh </span><br><span class="line">exec ls</span><br><span class="line">[double_happy@hadoop101 script]$ sh exec.sh </span><br><span class="line">azkaban-job  case.sh  dirname.sh  exec.sh  flume-agent  spark-shell  test.sh</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line">明白了吗？把exec 封装在一个shell 里 去调用别的脚本</span><br></pre></td></tr></table></figure></div>
<h2 id="spark-class脚本"><a href="#spark-class脚本" class="headerlink" title="spark-class脚本"></a>spark-class脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-class </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;&quot;/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"># Find the java binary</span><br><span class="line">if [ -n &quot;$&#123;JAVA_HOME&#125;&quot; ]; then</span><br><span class="line">  RUNNER=&quot;$&#123;JAVA_HOME&#125;/bin/java&quot;</span><br><span class="line">else</span><br><span class="line">  if [ &quot;$(command -v java)&quot; ]; then</span><br><span class="line">    RUNNER=&quot;java&quot;</span><br><span class="line">  else</span><br><span class="line">    echo &quot;JAVA_HOME is not set&quot; &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Find Spark jars.</span><br><span class="line">if [ -d &quot;$&#123;SPARK_HOME&#125;/jars&quot; ]; then</span><br><span class="line">  SPARK_JARS_DIR=&quot;$&#123;SPARK_HOME&#125;/jars&quot;</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR=&quot;$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d &quot;$SPARK_JARS_DIR&quot; ] &amp;&amp; [ -z &quot;$SPARK_TESTING$SPARK_SQL_TESTING&quot; ]; then</span><br><span class="line">  echo &quot;Failed to find Spark jars directory ($SPARK_JARS_DIR).&quot; 1&gt;&amp;2</span><br><span class="line">  echo &quot;You need to build Spark with the target \&quot;package\&quot; before running this program.&quot; 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH=&quot;$SPARK_JARS_DIR/*&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Add the launcher build dir to the classpath if requested.</span><br><span class="line">if [ -n &quot;$SPARK_PREPEND_CLASSES&quot; ]; then</span><br><span class="line">  LAUNCH_CLASSPATH=&quot;$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># For tests</span><br><span class="line">if [[ -n &quot;$SPARK_TESTING&quot; ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># The launcher library will print arguments separated by a NULL character, to allow arguments with</span><br><span class="line"># characters that would be otherwise interpreted by the shell. Read that in a while loop, populating</span><br><span class="line"># an array that will be used to exec the final command.</span><br><span class="line">#</span><br><span class="line"># The exit code of the launcher is appended to the output, so the parent shell removes it from the</span><br><span class="line"># command array and checks the value to see if the launcher succeeded.</span><br><span class="line">build_command() &#123;</span><br><span class="line">  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">  printf &quot;%d\0&quot; $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Turn off posix mode since it does not allow process substitution</span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d &apos;&apos; -r ARG; do</span><br><span class="line">  CMD+=(&quot;$ARG&quot;)</span><br><span class="line">done &lt; &lt;(build_command &quot;$@&quot;)</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"># Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes</span><br><span class="line"># the code that parses the output of the launcher to get confused. In those cases, check if the</span><br><span class="line"># exit code is an integer, and if it&apos;s not, handle it as a special error case.</span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo &quot;$&#123;CMD[@]&#125;&quot; | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=(&quot;$&#123;CMD[@]:0:$LAST&#125;&quot;)</span><br><span class="line">exec &quot;$&#123;CMD[@]&#125;&quot;</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">. &quot;$&#123;SPARK_HOME&#125;&quot;/bin/load-spark-env.sh  这是在做什么？</span><br><span class="line">就是把环境里的scala版本找到</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">build_command() &#123;</span><br><span class="line">  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">  printf &quot;%d\0&quot; $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RUNNER=java 去看一下就知道</span><br></pre></td></tr></table></figure></div>
<p>整个spark-shell流程：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">spark-shell &#123;</span><br><span class="line"></span><br><span class="line">&quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.repl.Main \</span><br><span class="line"> --name &quot;Spark shell&quot; \</span><br><span class="line"> &quot;$@&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">==&gt; spark-submit&#123;</span><br><span class="line">	exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class \</span><br><span class="line">	 org.apache.spark.deploy.SparkSubmit \</span><br><span class="line">	 &quot;$@&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">==&gt;spark-class&#123;</span><br><span class="line">	build_command() &#123;</span><br><span class="line">	  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">	  printf &quot;%d\0&quot; $?</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell最底层就是使用Jave来启动 org.apache.spark.launcher.Main类</span><br><span class="line">REPL  ==》交互式解释器</span><br><span class="line"></span><br><span class="line">看org.apache.spark.launcher.Main 源码 需要这个依赖</span><br><span class="line"></span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-launcher_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line"> &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">idea ：</span><br><span class="line">control + shift +n </span><br><span class="line">control + shift +f</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025152410447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark008-补充Spark007" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/25/Spark008-%E8%A1%A5%E5%85%85Spark007/">Spark008--补充Spark007</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/25/Spark008-%E8%A1%A5%E5%85%85Spark007/" class="article-date">
  <time datetime="2018-01-25T12:03:56.000Z" itemprop="datePublished">2018-01-25</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><img src="https://img-blog.csdnimg.cn/20191024155059801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上次的结果输出的文本是这样的，客户说我需要压缩的格式呢？<br>生产上出来的数据非常非常大 肯定是需要压缩的<br>eg：5分钟数据量达到10多个G</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class</span><br><span class="line">   * supporting the key and value types K and V in this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note We should make sure our tasks are idempotent when speculation is enabled, i.e. do</span><br><span class="line">   * not use output committer that writes data directly.</span><br><span class="line">   * There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad</span><br><span class="line">   * result of using direct output committer with speculation enabled.</span><br><span class="line">   */</span><br><span class="line">  def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      conf: JobConf = new JobConf(self.context.hadoopConfiguration),</span><br><span class="line">      codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">codec 就是指定压缩的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    /**</span><br><span class="line">      * Android</span><br><span class="line">      *   xxxx.log</span><br><span class="line">      *</span><br><span class="line">      * iOS</span><br><span class="line">      *   xxx.log</span><br><span class="line">      */</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],</span><br><span class="line">        classOf[RuozedataMultipleTextOutputFormat],</span><br><span class="line">        classOf[GzipCodec])      //这块加上压缩的格式</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024155642185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>学学底层的实现：</strong> 前面的文章好像有写 忘记了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">1.saveAsHadoopFile</span><br><span class="line"> def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      codec: Class[_ &lt;: CompressionCodec]): Unit = self.withScope &#123;</span><br><span class="line">    saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">      new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2.点进去</span><br><span class="line"> saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">      new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line"></span><br><span class="line">3.</span><br><span class="line">  def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      conf: JobConf = new JobConf(self.context.hadoopConfiguration),</span><br><span class="line">      codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit = self.withScope &#123;</span><br><span class="line">    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).</span><br><span class="line">    val hadoopConf = conf</span><br><span class="line">    hadoopConf.setOutputKeyClass(keyClass)</span><br><span class="line">    hadoopConf.setOutputValueClass(valueClass)</span><br><span class="line">    conf.setOutputFormat(outputFormatClass)</span><br><span class="line">    for (c &lt;- codec) &#123;</span><br><span class="line">      hadoopConf.setCompressMapOutput(true)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,</span><br><span class="line">        CompressionType.BLOCK.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Use configured output committer if already set</span><br><span class="line">    if (conf.getOutputCommitter == null) &#123;</span><br><span class="line">      hadoopConf.setOutputCommitter(classOf[FileOutputCommitter])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // When speculation is on and output committer class name contains &quot;Direct&quot;, we should warn</span><br><span class="line">    // users that they may loss data if they are using a direct output committer.</span><br><span class="line">    val speculationEnabled = self.conf.getBoolean(&quot;spark.speculation&quot;, false)</span><br><span class="line">    val outputCommitterClass = hadoopConf.get(&quot;mapred.output.committer.class&quot;, &quot;&quot;)</span><br><span class="line">    if (speculationEnabled &amp;&amp; outputCommitterClass.contains(&quot;Direct&quot;)) &#123;</span><br><span class="line">      val warningMessage =</span><br><span class="line">        s&quot;$outputCommitterClass may be an output committer that writes data directly to &quot; +</span><br><span class="line">          &quot;the final location. Because speculation is enabled, this output committer may &quot; +</span><br><span class="line">          &quot;cause data loss (see the case in SPARK-10063). If possible, please use an output &quot; +</span><br><span class="line">          &quot;committer that does not have this behavior (e.g. FileOutputCommitter).&quot;</span><br><span class="line">      logWarning(warningMessage)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FileOutputFormat.setOutputPath(hadoopConf,</span><br><span class="line">      SparkHadoopWriterUtils.createPathFromString(path, hadoopConf))</span><br><span class="line">    saveAsHadoopDataset(hadoopConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4. 3里面关键</span><br><span class="line">   for (c &lt;- codec) &#123;</span><br><span class="line">      hadoopConf.setCompressMapOutput(true)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,</span><br><span class="line">        CompressionType.BLOCK.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"> hadoopConf.setCompressMapOutput(true) 点进去看看</span><br><span class="line"></span><br><span class="line"> public void setCompressMapOutput(boolean compress) &#123;</span><br><span class="line">    setBoolean(JobContext.MAP_OUTPUT_COMPRESS, compress);</span><br><span class="line">  &#125;</span><br><span class="line">再点进去</span><br><span class="line"></span><br><span class="line">public static final String MAP_OUTPUT_COMPRESS = &quot;mapreduce.map.output.compress&quot;;</span><br><span class="line"></span><br><span class="line">mapreduce.map.output.compress 这个参数不就是设置map端的输出压缩</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec 这个参数 </span><br><span class="line">设置最终输出的压缩 前面的文章写过</span><br><span class="line"></span><br><span class="line">所以这底层的代码就是Mapreduce代码 **** 就是Spark给封装好的</span><br></pre></td></tr></table></figure></div>

<h2 id="补充知识点"><a href="#补充知识点" class="headerlink" title="补充知识点"></a>补充知识点</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      *  map vs mapPartitions(优先选择)  transformation</span><br><span class="line">      *</span><br><span class="line">      *  RDD 1w个元素  需要你把这个RDD的元素写入到MySQL （前提没有使用数据库连接池那种）</span><br><span class="line">      * 1w个元素 我要写1w次MySQL</span><br><span class="line">      * </span><br><span class="line">      *  RDD 1w个元素  10分区   10次</span><br><span class="line">      *</span><br><span class="line">map vs mapPartitions ：</span><br><span class="line">这两个写入db哪个好？   优先级角度选 mapPartitions 是没有问题的 能解决80%生产上的操作</span><br><span class="line">分情况的 应该说都不好  你应该想说 mapPartitions好 但是</span><br><span class="line">当你数据量很大 分区数很少 那么一个分区里的数据量很大 写的时候 可能oom </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      *</span><br><span class="line">      *  foreach vs foreachPartition   action</span><br><span class="line">      *</span><br><span class="line">      * rdd==&gt; transformations ==&gt; action</span><br><span class="line">      *  真正生产上把RDD的数据写入到DB，是使用foreachPartition</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03ToMySQL &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_)</span><br><span class="line">        .sortBy(-_._2)</span><br><span class="line">        .foreachPartition(partition =&gt;&#123;</span><br><span class="line">          var connection : Connection = null</span><br><span class="line">          var pstmt:PreparedStatement = null</span><br><span class="line">          try&#123;</span><br><span class="line">            connection = MySQLUtils.getConnection()</span><br><span class="line">            val sql = &quot;insert into topn(domain,url,cnt) values (?,?,?)&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            //真正的数据是分区里的元素</span><br><span class="line">            partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.execute()  </span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;catch &#123;</span><br><span class="line">            case  e:Exception =&gt; e.printStackTrace()</span><br><span class="line">          &#125;finally &#123;</span><br><span class="line">            MySQLUtils.closeResource(pstmt,connection)  //这块只关闭connection可以么? 可以的 java知识</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">object MySQLUtils &#123;</span><br><span class="line">  def getConnection():Connection = &#123;</span><br><span class="line">    Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    getConnection(&quot;hadoop101&quot;, &quot;3306&quot;, &quot;hive_dwd&quot;, &quot;root&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  def getConnection(host: String, port: String, database: String, user: String, password: String):Connection = &#123;</span><br><span class="line">    Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    DriverManager.getConnection(s&quot;jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/$&#123;database&#125;&quot;, user, password)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 关闭资源</span><br><span class="line">    *</span><br><span class="line">    * @param resources 可变数组</span><br><span class="line">    */</span><br><span class="line">  def closeResource(resources: AutoCloseable*): Unit = &#123;</span><br><span class="line">    for (resource &lt;- resources) &#123;</span><br><span class="line">      resource.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from topn;</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| domain          | url   | cnt  |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| www.baidu.com   | url2  |    2 |</span><br><span class="line">| www.baidu.com   | url5  |    5 |</span><br><span class="line">| www.baidu.com   | url1  |    1 |</span><br><span class="line">| www.baidu.com   | url4  |    4 |</span><br><span class="line">| www.baidu.com   | url3  |    3 |</span><br><span class="line">| www.twitter.com | url10 |   11 |</span><br><span class="line">| www.twitter.com | url6  |    1 |</span><br><span class="line">| www.twitter.com | url9  |    6 |</span><br><span class="line">| www.google.com  | url2  |    2 |</span><br><span class="line">| www.google.com  | url6  |    7 |</span><br><span class="line">| www.google.com  | url1  |    1 |</span><br><span class="line">| www.google.com  | url8  |    7 |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">能使用scalikejdbc 把数据写入MySQL更好哈   前面scala篇有讲</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">问题指出：</span><br><span class="line">  partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.execute()  </span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">如果你一个partition 一个元素执行一次  里有1w个元素呢？   性能不好</span><br><span class="line"></span><br><span class="line">肯定是要批处理的   一个批次给它搞一个事务 把自动提交给关掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03ToMySQL &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_)</span><br><span class="line">        .sortBy(-_._2)</span><br><span class="line">        .foreachPartition(partition =&gt;&#123;</span><br><span class="line">          var connection : Connection = null</span><br><span class="line">          var pstmt:PreparedStatement = null</span><br><span class="line">          try&#123;</span><br><span class="line">            connection = MySQLUtils.getConnection()</span><br><span class="line">            connection.setAutoCommit(false)</span><br><span class="line"></span><br><span class="line">     /*       //先把之前的数据删掉   </span><br><span class="line">            val sqlDelete = s&quot;delete from topn where access_time = $&#123;args(0)&#125;&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sqlDelete)</span><br><span class="line">            pstmt.execute()*/</span><br><span class="line"></span><br><span class="line">            //再插入数据</span><br><span class="line">            val sql = &quot;insert into topn(domain,url,cnt) values (?,?,?)&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            //真正的数据是分区里的元素</span><br><span class="line">            partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.addBatch()</span><br><span class="line">            &#125;)</span><br><span class="line">            pstmt.executeBatch()   //执行批次</span><br><span class="line">            connection.commit()  //提交事务</span><br><span class="line">          &#125;catch &#123;</span><br><span class="line">            case  e:Exception =&gt; e.printStackTrace()</span><br><span class="line">          &#125;finally &#123;</span><br><span class="line">            MySQLUtils.closeResource(pstmt,connection)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; truncate table topn;       //代码里加上删除之前的数据 可以解决 最好别truncate 只是学习时方便</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from topn; </span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| domain          | url   | cnt  |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| www.baidu.com   | url5  |    5 |</span><br><span class="line">| www.baidu.com   | url2  |    2 |</span><br><span class="line">| www.baidu.com   | url4  |    4 |</span><br><span class="line">| www.baidu.com   | url1  |    1 |</span><br><span class="line">| www.baidu.com   | url3  |    3 |</span><br><span class="line">| www.twitter.com | url6  |    1 |</span><br><span class="line">| www.twitter.com | url10 |   11 |</span><br><span class="line">| www.twitter.com | url9  |    6 |</span><br><span class="line">| www.google.com  | url2  |    2 |</span><br><span class="line">| www.google.com  | url6  |    7 |</span><br><span class="line">| www.google.com  | url1  |    1 |</span><br><span class="line">| www.google.com  | url8  |    7 |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><p>工作当中是再idea里开发的 在生产上是在Submitting Applications</p>
<p><img src="https://img-blog.csdnimg.cn/20191024192531620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.Spark-shell 底层调用的是Spark-submit</span><br><span class="line"></span><br><span class="line">Spark-submit怎么使用呢？</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications" target="_blank" rel="noopener">Submitting Applications</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">idea里面：</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">        val conf = new SparkConf()       //提交到集群上的时候 setAppName 和setMater全都去掉</span><br><span class="line">        val sc = new SparkContext(conf)</span><br><span class="line">        val input = sc.textFile(args(0))</span><br><span class="line">        val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot;,&quot;)</span><br><span class="line">          val site = splits(0)</span><br><span class="line">          val url = splits(1)</span><br><span class="line">          ((site, url), 1)</span><br><span class="line">        &#125;).reduceByKey(_+_).saveAsTextFile(args(1))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">打包提交到集群上去</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --help</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 bin]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[]  可选  &lt;&gt; 必选</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  \</span><br><span class="line">--name InterviewApp03 \</span><br><span class="line">--class com.ruozedata.spark.spark05.InterviewApp03 \</span><br><span class="line">--master local[2] \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">/data_spark/input/  /data_spark/output </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">抛出一个问题 我的路径前面没有添加 hdfs:xxx:8020/路径  为什么我的不用加 ？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024200131809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 lib]$ hadoop fs -text /data_spark/output/par*</span><br><span class="line">19/10/24 20:05:35 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">19/10/24 20:05:35 INFO compress.CodecPool: Got brand-new decompressor [.bz2]</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line">((www.baidu.com,url1),1)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line">((www.google.com,url1),1)</span><br><span class="line">((www.baidu.com,url3),3)</span><br><span class="line">((www.google.com,url2),2)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url6),1)</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url2),2)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">[double_happy@hadoop101 lib]$</span><br></pre></td></tr></table></figure></div>
<p>If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a Spark cluster. <strong>To do this, create an assembly jar (or “uber” jar) containing your code and its dependencies.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">你的代码 依赖hadoop spark 但是这些集群本身 就有 打包的时候 要的是瘦包 如果需要第三方的包</span><br><span class="line">可以使用  --jars  或者 package到application 里 </span><br><span class="line"></span><br><span class="line">个人不建议使用assembly 打包</span><br><span class="line"></span><br><span class="line">到ss 打包再讨论</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line"></span><br><span class="line">第三方包 加入这个命令</span><br></pre></td></tr></table></figure></div>
<h2 id="Loading-Configuration-from-a-File"><a href="#Loading-Configuration-from-a-File" class="headerlink" title="Loading Configuration from a File"></a>Loading Configuration from a File</h2><p>The <strong>spark-submit script</strong> can load default Spark configuration values from a properties file and pass them on to your application. By default, it will <strong>read options from conf/spark-defaults.conf in the Spark directory.</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">spark-submit 默认会加载 conf/spark-defaults.conf   in the Spark directory </span><br><span class="line"></span><br><span class="line">当然也可以人为指定 ：</span><br><span class="line"> --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"># spark.master                     spark://master:7077</span><br><span class="line"># spark.eventLog.enabled           true</span><br><span class="line"># spark.eventLog.dir               hdfs://namenode:8021/directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br><span class="line">[double_happy@hadoop101 conf]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">带来的好处是什么？</span><br><span class="line">默认情况下 你每次开启spark-shell  --master local[2]</span><br><span class="line"> 每次都要手动加参数 </span><br><span class="line"> 可以再 spark-defalut.xml 里加上即可。</span><br><span class="line"></span><br><span class="line">如果你的业务线非常多 你就多写几个spark-defalut.xml  通过 --files 传过去你想要的模式</span><br></pre></td></tr></table></figure></div>
<p>以上是最基本的操作</p>
<p>它默认走的是 spark-defalut.xml 那么底层的实现一定是走的默认参数的</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>上面跑的程序<br><img src="https://img-blog.csdnimg.cn/20191024203040986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">成功完之后 SC被干掉了 UI上面还能看到么？</span><br><span class="line">看不见了 如果半夜三点程序挂了 或者在调优的场景下  你程序跑完 UI就没了呀 该怎么解决呢？</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/monitoring.html#web-interfaces" target="_blank" rel="noopener">Monitoring</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;c&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;d&quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((d,1), (b,2), (a,2), (c,2))                  </span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024203854192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:<br>        A list of scheduler stages and tasks<br>        A summary of RDD sizes and memory usage<br>        Environmental information.<br>        Information about the running executors</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.页面上展示 有多少个job （就是代码里有多少个action）</span><br><span class="line">2.stages ---&gt;一个 job里面  有多少个stage    点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204201773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.一个stage有几个task呢？点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204355319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Information about the running executors：</span><br><span class="line">关于你运行的executors信息 </span><br><span class="line">1.你运行这个程序 设置了多少个executors 活的有多少 死的有多少</span><br><span class="line">2.对调优很重要 </span><br><span class="line">你肯定要看 ：</span><br><span class="line">   1.shuffle的数据量有多少 </span><br><span class="line">   2.经历多少算子</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204640805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这副图运行时间慢的原因是 ：<br>emmm 总有贪小便宜的人 去我云服务器上挖矿 悄踏玛真的很烦 因为我把端口全部开放了</p>
<p> If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc).</p>
<p>这句话挺重要的 因为之前在公司里 我喜欢用 yarn client模式 而我提交的任务比较多<br>达到特别多的时候 再提交任务 是排不上的哈 提交不上的 </p>
<p><strong>Note that this information is only available for the duration of the application by default</strong>. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Note that this information is only available for the duration of the application by default</span><br><span class="line"></span><br><span class="line">1.默认情况下 这个页面只能 在 application 生命周期内有效  所以运行完 sc.stop 之后</span><br><span class="line">你的UI界面就看不见了 </span><br><span class="line">2.spark.eventLog.enabled 这个参数设置为true  开启spark日志 当你再打开ui界面 可以看的到 运行完的application</span><br></pre></td></tr></table></figure></div>
<p>那么怎么构建 总结的UI呢？<br><img src="https://img-blog.csdnimg.cn/20191024224445859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.spark.eventLog.enabled true   开关打开</span><br><span class="line">2.spark.eventLog.dir hdfs://namenode/shared/spark-logs    </span><br><span class="line">记录spark运行过程中日志记录在这个目录  这是spark作业触发的</span><br><span class="line"></span><br><span class="line">3.使用start-history-server.sh 展示记录的spark日志  也需要一个目录  </span><br><span class="line">spark.history.fs.logDirectory 用这个参数 </span><br><span class="line">这个参数 是配置spark-env.sh里</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">这个跟配置压缩一样 打开开关 + 指定codec</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"> spark.master                     local[2]</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"> spark.eventLog.dir               hdfs://hadoop101:8020/spark_directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/monitoring.html#viewing-after-the-fact" target="_blank" rel="noopener">参数配置</a></p>
<p>This creates a web interface at http://<server-url>:18080 by default, listing <strong>incomplete</strong> and <strong>completed</strong> applications and attempts.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.completed 和 incomplete spark怎么区别呢 需要一个刷新时间 </span><br><span class="line">用这个参数 spark.history.fs.update.interval</span><br><span class="line"></span><br><span class="line">2.如果配置成功之后日志日积月累多了 数据量会很大 所以需要定期删除的</span><br><span class="line">spark.history.fs.cleaner.enabled     是否需要清理呢</span><br><span class="line">spark.history.fs.cleaner.interval         清理周期是多少</span><br><span class="line">spark.history.fs.cleaner.maxAge        一次清理几天的</span><br></pre></td></tr></table></figure></div>
<p>配置一下 ：<br><img src="https://img-blog.csdnimg.cn/20191024230341733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>启动：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.hdfs 上创建 log 文件夹</span><br><span class="line">2.启动./sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">测试查看是否成功</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ ./start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.deploy.history.HistoryServer-1-hadoop101.out</span><br><span class="line">[double_happy@hadoop101 sbin]$ tail -200f /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.deploy.history.HistoryServer-1-hadoop101.out</span><br><span class="line">Spark Command: /usr/java/java/bin/java -cp /home/double_happy/app/spark/conf/:/home/double_happy/app/spark/jars/*:/home/double_happy/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://hadoop101:8020/spark_directory -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">========================================</span><br><span class="line">19/10/24 23:06:35 INFO HistoryServer: Started daemon with process name: 6633@hadoop101</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for TERM</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for HUP</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for INT</span><br><span class="line">19/10/24 23:06:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/24 23:06:35 INFO FsHistoryProvider: History server ui acls disabled; users with admin permissions: ; groups with admin permissions</span><br><span class="line">19/10/24 23:06:37 INFO Utils: Successfully started service on port 18080.</span><br><span class="line">19/10/24 23:06:37 INFO HistoryServer: Bound HistoryServer to 0.0.0.0, and started at http://hadoop101:18080</span><br></pre></td></tr></table></figure></div>
<p>说明启动ok </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">测试：</span><br><span class="line">1.spark-shell 运行了一个东西</span><br><span class="line">2.spark-submit 提交了两次</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.</span><br><span class="line">scala&gt;  sc.parallelize(List(&quot;a&quot;,&quot;c&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;d&quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((d,1), (b,2), (a,2), (c,2))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.stop</span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">spark-submit  \</span><br><span class="line">&gt; --name InterviewApp03 \</span><br><span class="line">&gt; --class com.ruozedata.spark.spark05.InterviewApp03 \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; /data_spark/input/  /data_spark/output</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024231709105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Note that in all of these UIs, the tables are sortable by <strong>clicking their headers,</strong> making it easy to <strong>identify slow tasks, data skew, etc</strong>.</p>
<p>Note<br>1.<strong>The history server displays both completed and incomplete Spark jobs.</strong> If an application makes multiple attempts after failures, the failed attempts will be displayed, as well as any ongoing incomplete attempt or the final successful attempt.</p>
<p>2.Incomplete applications are only updated intermittently. The time between updates is defined by the interval between checks for changed files (spark.history.fs.update.interval). On larger clusters, the update interval may be set to large values. The way to view a running application is actually to view its own web UI.</p>
<p>3.Applications which exited without registering themselves as completed will be listed as incomplete —even though they are no longer running. This can happen if an application crashes.</p>
<p><strong>4.One way to signal the completion of a Spark job is to stop the Spark Context explicitly (sc.stop()), or in Python using the with SparkContext() as sc: construct to handle the Spark Context setup and tear down.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4.就是你代码里 sc.stop() 写了  程序完成后会显示在 completed 里面 如果不写会显示在incomplete </span><br><span class="line">所以 为了 好区分正在运行的作业还是 完成的作业 sc.stop() 要加上的</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191025111143777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191025111221278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.本地运行的作业 全部以 local开头的 </span><br><span class="line">2.ui上显示很多信息   点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102423185850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这个页面不就回来了么 程序已经运行完了   测试成功。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Download 之后就是个Json文件 也可以去HDFS上去看我配置的log目录 也可以下载的</span><br><span class="line">整个历史页面 就是靠 Json文件 来渲染的</span><br><span class="line"></span><br><span class="line">所以这个东西有了 spark调优就方便了很多</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024232531503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="start-history-server-sh"><a href="#start-history-server-sh" class="headerlink" title="start-history-server.sh"></a>start-history-server.sh</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ cat start-history-server.sh </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Starts the history server on the machine this script is executed on.</span><br><span class="line">#</span><br><span class="line"># Usage: start-history-server.sh</span><br><span class="line">#</span><br><span class="line"># Use the SPARK_HISTORY_OPTS environment variable to set history server configuration.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;/sbin/spark-config.sh&quot;</span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh&quot;</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;/sbin&quot;/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 &quot;$@&quot;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">去idea里找到 HistoryServer类：</span><br><span class="line">1.这个类一定有main方法</span><br><span class="line"></span><br><span class="line"> def main(argStrings: Array[String]): Unit = &#123;</span><br><span class="line">    Utils.initDaemon(log)</span><br><span class="line">    new HistoryServerArguments(conf, argStrings)</span><br><span class="line">    initSecurity()</span><br><span class="line">    val securityManager = createSecurityManager(conf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. new HistoryServerArguments(conf, argStrings)  点进去 </span><br><span class="line">里面的  解析</span><br><span class="line">// This mutates the SparkConf, so all accesses to it must be made after this line</span><br><span class="line">   Utils.loadDefaultSparkProperties(conf, propertiesFile)</span><br><span class="line"></span><br><span class="line">3.def loadDefaultSparkProperties(conf: SparkConf, filePath: String = null): String = &#123;</span><br><span class="line">    val path = Option(filePath).getOrElse(getDefaultPropertiesFile())</span><br><span class="line">    Option(path).foreach &#123; confFile =&gt;</span><br><span class="line">      getPropertiesFromFile(confFile).filter &#123; case (k, v) =&gt;</span><br><span class="line">        k.startsWith(&quot;spark.&quot;)</span><br><span class="line">      &#125;.foreach &#123; case (k, v) =&gt;</span><br><span class="line">        conf.setIfMissing(k, v)</span><br><span class="line">        sys.props.getOrElseUpdate(k, v)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    path</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">4.getDefaultPropertiesFile</span><br><span class="line"></span><br><span class="line">def getDefaultPropertiesFile(env: Map[String, String] = sys.env): String = &#123;</span><br><span class="line">    env.get(&quot;SPARK_CONF_DIR&quot;)</span><br><span class="line">      .orElse(env.get(&quot;SPARK_HOME&quot;).map &#123; t =&gt; s&quot;$t$&#123;File.separator&#125;conf&quot; &#125;)</span><br><span class="line">      .map &#123; t =&gt; new File(s&quot;$t$&#123;File.separator&#125;spark-defaults.conf&quot;)&#125;</span><br><span class="line">      .filter(_.isFile)</span><br><span class="line">      .map(_.getAbsolutePath)</span><br><span class="line">      .orNull</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">能知道 加载 spark-defaults.conf 文件 明白了吗 </span><br><span class="line">详细脚本实现 自己看源码</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024233636507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">源码里有写：</span><br><span class="line">spark.history.retainedApplications   ： 默认50个</span><br><span class="line"></span><br><span class="line">The number of applications to retain UI data for in the cache. If this cap is exceeded, then the oldest applications will be removed from the cache. If an application is not in the cache, it will have to be loaded from disk if it is accessed from the UI.</span><br><span class="line"></span><br><span class="line">1.The number of applications to retain UI data for in the cache</span><br><span class="line"> 是在内存中的</span><br><span class="line"> 这个参数 不是 ui上面只能展示50个意思哈   是内存里只放50个 超过了 removed from the cache </span><br><span class="line"> 2. 看解释 很清楚</span><br></pre></td></tr></table></figure></div>
<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p><img src="https://img-blog.csdnimg.cn/2019102510374444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Shared-Variables"><a href="#Shared-Variables" class="headerlink" title="Shared Variables"></a>Shared Variables</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables" target="_blank" rel="noopener">Shared Variables</a><br>Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. <strong>These variables are copied to each machine</strong>, and <strong>no updates</strong> to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: <strong>broadcast variables and accumulators.</strong></p>
<p>Accumulators are variables that are <strong>only “added”</strong> to through an associative and commutative operation and can therefore <strong>be efficiently supported in parallel.</strong><br>Spark natively supports accumulators of <strong>numeric types</strong>, and <strong>programmers can add support for new types.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accumulators：</span><br><span class="line">使用场景：ETl处理的时候 把正确的条数 和总的条数  统计出来 </span><br><span class="line">1.原生的支持数值类型 ，开发者开发可以支持别的类型</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val accum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res1: Long = 10</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102510514440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在的计数器都是AccumulatorV2 版本  官网上写了如何自定义累加器 生产上我没用用过</span><br></pre></td></tr></table></figure></div>
<p>案例：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    var cnts = 0</span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,7,8),3)</span><br><span class="line">    data.foreach(x =&gt; &#123;</span><br><span class="line">      cnts += 1</span><br><span class="line">      println(s&quot;cnts:$cnts&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line">    println(cnts+&quot;~~~~~~~&quot;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">cnts:1</span><br><span class="line">cnts:1</span><br><span class="line">cnts:2</span><br><span class="line">cnts:2</span><br><span class="line">cnts:3</span><br><span class="line">cnts:1</span><br><span class="line">cnts:2</span><br><span class="line">cnts:3</span><br><span class="line">0~~~~~~~</span><br><span class="line"></span><br><span class="line">这个结果什么意思？是想要的结果么？</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	上面这个 不是并行的 也就是 多个分区 是无法计算的 </span><br><span class="line">需要使用计数器 </span><br><span class="line">   计数器一定是在action算子之后使用   一定是要触发action的要不然拿不到结果 </span><br><span class="line">那么触发多个action可以么？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">broadcast variables ：</span><br><span class="line">类似的</span><br><span class="line">1.前面解析ip库的时候，使用的mapreduce的分布式缓存 </span><br><span class="line">2.在sql里的 </span><br><span class="line"></span><br><span class="line">a join b on a.id=b.id ==&gt; shuffle  (普通的join 必然有shuffle的)  </span><br><span class="line">会按照join的条件 作为key(就是id)，其他的值作为value </span><br><span class="line"> 经过shuffle到reduce端 把相同的key聚在一块 来做的</span><br><span class="line"></span><br><span class="line">大数据集和小数据集join ==&gt;采用 mapjoin</span><br><span class="line"> 小表放到缓存中，不会有真正的join发生，底层其实就是一个匹配  匹配上拿出来 匹配不上就滚蛋的</span><br><span class="line"> </span><br><span class="line"> 那么 上面的这些可以使用 broadcast variables 来实现</span><br></pre></td></tr></table></figure></div>
<p>Broadcast variables allow the programmer to keep a read-only variable <strong>cached on each machine rather than shipping a copy of it with tasks.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">eg:这个代码  </span><br><span class="line">val xx = new HashMap() // 10M</span><br><span class="line">rdd.map(x=&gt;&#123;</span><br><span class="line">    ....xx</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">如果有一万个task  每个task里 额外的  1w*10M</span><br><span class="line"></span><br><span class="line">Broadcast variables cached on each machine （理解为executor就可以）而不是 cp到tasks</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">object RDDOperationApp02 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    //广播的时候 要使用 kv的 最佳实践</span><br><span class="line">    val rdd1 = sc.parallelize(Array((&quot;23&quot;,&quot;smart&quot;),(&quot;9&quot;,&quot;愤怒的麻雀&quot;))).collectAsMap()</span><br><span class="line">    val rdd2 = sc.parallelize(Array((&quot;23&quot;,&quot;郑州&quot;),(&quot;9&quot;,&quot;蜀国&quot;),(&quot;14&quot;,&quot;魔都&quot;)))</span><br><span class="line">    val rdd1_bc = sc.broadcast(rdd1)</span><br><span class="line">    rdd2.map(x=&gt;(x._1,x)).mapPartitions(x =&gt; &#123;</span><br><span class="line">      val bc_value = rdd1_bc.value</span><br><span class="line">      for((k,v)&lt;- x if(bc_value.contains(k)))</span><br><span class="line">        yield (k, bc_value.get(k).getOrElse(&quot;&quot;), v._2)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(9,愤怒的麻雀,蜀国)</span><br><span class="line">(23,smart,郑州)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Scala中的yield的主要作用是记住每次迭代中的有关值，并逐一存入到一个数组中。</span><br><span class="line">要将结果存放到数组的变量或表达式必须放在yield&#123;&#125;里最后位置</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(Array((&quot;23&quot;,&quot;smart&quot;),(&quot;9&quot;,&quot;愤怒的麻雀&quot;))).collectAsMap()</span><br><span class="line">rdd1: scala.collection.Map[String,String] = Map(23 -&gt; smart, 9 -&gt; 愤怒的麻雀)</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(Array((&quot;23&quot;,&quot;郑州&quot;),(&quot;9&quot;,&quot;蜀国&quot;),(&quot;14&quot;,&quot;魔都&quot;)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1_bc = sc.broadcast(rdd1)</span><br><span class="line">rdd1_bc: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,String]] = Broadcast(3)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.map(x=&gt;(x._1,x)).mapPartitions(x =&gt; &#123;</span><br><span class="line">     |   val bc_value = rdd1_bc.value</span><br><span class="line">     |   for((k,v)&lt;- x if(bc_value.contains(k)))</span><br><span class="line">     |     yield (k, bc_value.get(k).getOrElse(&quot;&quot;), v._2)</span><br><span class="line">     | &#125;).foreach(println)</span><br><span class="line">(23,smart,郑州)</span><br><span class="line">(9,愤怒的麻雀,蜀国)</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">查看页面 是没有shuffle的 没有join的 就是mapjoin</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025114937388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>sparkcore之后sparksql 以及sparkstreaming 、sss、spark调优  的重要的文章是进行私密的 我写博客的目的是为了做笔记 为了学习</p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark007-综合案例" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/24/Spark007-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/">Spark007--综合案例</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/24/Spark007-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2018-01-24T12:03:00.000Z" itemprop="datePublished">2018-01-24</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.数据如下</span><br><span class="line">    a,1,3</span><br><span class="line">    a,2,4</span><br><span class="line">    b,1,1</span><br><span class="line">根据第一列统计出</span><br><span class="line">    a,3,7</span><br><span class="line">    b,1,1</span><br><span class="line">用RDD实现</span><br><span class="line"></span><br><span class="line">分析：</span><br><span class="line">1）使用逗号对数据进行拆分 (a,&lt;1,3&gt;)  a=_.1  &lt;1,3&gt;=_.2</span><br><span class="line">2）reduceByKey((a,b)=&gt;a+b)   =&gt;  _.2._1 + _.2._2</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark04</span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object InterviewApp01 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.parallelize(List(</span><br><span class="line">      List(&quot;a&quot;,1,3),</span><br><span class="line">      List(&quot;a&quot;,2,4),</span><br><span class="line">      List(&quot;b&quot;,1,1)</span><br><span class="line">    ))</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val key = x(0).toString</span><br><span class="line">      val v1 = x(1).toString.toInt</span><br><span class="line">      val v2 = x(2).toString.toInt</span><br><span class="line">      (key, (v1,v2))</span><br><span class="line">    &#125;).reduceByKey((x,y)=&gt;&#123;</span><br><span class="line">      (x._1 + y._1, x._2+y._2)</span><br><span class="line">    &#125;).map(x=&gt;List(x._1, x._2._1,x._2._2)).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">List(a, 3, 7)</span><br><span class="line">List(b, 1, 1)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2.广告投放 收费标准：</span><br><span class="line">	看到就收费</span><br><span class="line">	点击 才收费</span><br><span class="line">eg：</span><br><span class="line">&quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;    </span><br><span class="line"> uid,导航,1,0       （1表示看到了 0表示没有点进去）</span><br><span class="line"></span><br><span class="line">需求1：人和“一个东西”的展示量以及点击量</span><br><span class="line">eg：1000000 一起看 2 1</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">使用reduceBykey和groupBykey都实现一下：</span><br><span class="line"></span><br><span class="line">package com.ruozedata.spark.spark04</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object InterviewApp02 &#123;</span><br><span class="line">    def main(args: Array[String]): Unit = &#123;</span><br><span class="line">      val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">      val input = sc.parallelize(List(</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,1,0 1表示看到了 0表示没有点进去</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">        * 需求：人和“一个东西”的展示量以及点击量</span><br><span class="line">        * 1）组合key：人和所谓的一个东西</span><br><span class="line">        *</span><br><span class="line">        * 1000000 一起看 2 1</span><br><span class="line">        */</span><br><span class="line">      val processRDD = input.flatMap(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;,&quot;)</span><br><span class="line">        val id = splits(0).toInt</span><br><span class="line">        val word = splits(1)</span><br><span class="line">        val show = splits(2).toInt</span><br><span class="line">        val clicks = splits(3).toInt</span><br><span class="line"></span><br><span class="line">        val words = word.split(&quot;\\|&quot;)</span><br><span class="line">        words.map(x =&gt; ((id, x), (show, clicks)))</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">        * 在每个task/partition按照key先进行一个本地的聚合mapSideCombine: Boolean = true</span><br><span class="line">        * 预聚合之后，在每个task之上对于相同key的数据只有一条</span><br><span class="line">        *</span><br><span class="line">        *</span><br><span class="line">        * 调优 前 vs 后</span><br><span class="line">        * 是否能按照调优之前和调优之后作业的执行时间来对比?</span><br><span class="line">        * 时间之外还有其他的：读进来多少数据，shuffle出去多少数据，shuffle读写花费多少时间</span><br><span class="line">        * </span><br><span class="line">        * 所以优先选择reduceByKey </span><br><span class="line">        * 	1.shuffle数据少</span><br><span class="line">        * 去4040页面查看就知道了</span><br><span class="line">        */</span><br><span class="line">      processRDD.reduceByKey((x,y)=&gt;(x._1+y._1, x._2+y._2)).printInfo()</span><br><span class="line"></span><br><span class="line">      // 数据全部进行shuffle操作</span><br><span class="line">      processRDD</span><br><span class="line">        .groupByKey().mapValues(x=&gt;&#123;</span><br><span class="line">        val totalShows = x.map(_._1).sum</span><br><span class="line">        val totalClicks = x.map(_._2).sum</span><br><span class="line">        (totalShows, totalClicks)</span><br><span class="line">      &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">((1000000,军旅),(10,5))</span><br><span class="line">((1000000,一起看),(10,5))</span><br><span class="line">((1000000,士兵突击),(10,5))</span><br><span class="line">((1000001,电视剧),(5,5))</span><br><span class="line">((1000001,我的团长我的团),(5,5))</span><br><span class="line">((1000001,一起看),(5,5))</span><br><span class="line">((1000001,军旅),(5,5))</span><br><span class="line">((1000000,电视剧),(10,5))</span><br><span class="line">-------------------------</span><br><span class="line">((1000000,一起看),(10,5))</span><br><span class="line">((1000000,军旅),(10,5))</span><br><span class="line">((1000001,电视剧),(5,5))</span><br><span class="line">((1000000,士兵突击),(10,5))</span><br><span class="line">((1000001,军旅),(5,5))</span><br><span class="line">((1000001,我的团长我的团),(5,5))</span><br><span class="line">((1000001,一起看),(5,5))</span><br><span class="line">((1000000,电视剧),(10,5))</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191023170856895.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023171046603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>一定要注意数据结构</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp02 &#123;</span><br><span class="line">    def main(args: Array[String]): Unit = &#123;</span><br><span class="line">      val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">      val input = sc.parallelize(List(</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,1,0 1表示看到了 0表示没有点进去</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">        * 需求：人和“一个东西”的展示量以及点击量</span><br><span class="line">        * 1）组合key：人和所谓的一个东西</span><br><span class="line">        *</span><br><span class="line">        * 1000000 一起看 2 1</span><br><span class="line">        * flatMap 注意 为什么不用map *** </span><br><span class="line">        */</span><br><span class="line">      val processRDD = input.flatMap(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;,&quot;)</span><br><span class="line">        val id = splits(0).toInt</span><br><span class="line">        val word = splits(1)</span><br><span class="line">        val show = splits(2).toInt</span><br><span class="line">        val clicks = splits(3).toInt</span><br><span class="line"></span><br><span class="line">        val words = word.split(&quot;\\|&quot;)</span><br><span class="line">        words.map(x =&gt; ((id, x), (show, clicks)))</span><br><span class="line">      &#125;)</span><br><span class="line">      // 数据全部进行shuffle操作</span><br><span class="line">      processRDD</span><br><span class="line">        .groupByKey().printInfo()</span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((1000000,一起看),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000000,军旅),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000001,电视剧),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line">((1000000,士兵突击),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000001,军旅),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line">((1000001,我的团长我的团),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line">((1000000,电视剧),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000001,一起看),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">groupByKey 没有预聚合 还记得wc那张图么 上一篇的。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">3.  分组排序/组内排序</span><br><span class="line">  求每个域名访问量最大的url的Top N</span><br><span class="line"></span><br><span class="line">数据格式：</span><br><span class="line">www.baidu.com,url1</span><br><span class="line">www.baidu.com,url2</span><br><span class="line">www.baidu.com,url2</span><br><span class="line">www.baidu.com,url3</span><br><span class="line">www.baidu.com,url3</span><br><span class="line">www.baidu.com,url3</span><br><span class="line">www.baidu.com,url4</span><br></pre></td></tr></table></figure></div>
<p>我们一步步来：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    processRDD.printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((www.google.com,url6),1)</span><br><span class="line">((www.baidu.com,url1),1)</span><br><span class="line">((www.baidu.com,url2),1)</span><br><span class="line">((www.google.com,url6),1)</span><br><span class="line">((www.baidu.com,url2),1)</span><br><span class="line">((www.google.com,url2),1)</span><br><span class="line">。。。。。</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>分组 ： 如何分组？ 不分组行不行？</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">        processRDD.reduceByKey(_+_)</span><br><span class="line">          .groupBy(_._1._1)</span><br><span class="line">          .printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(www.baidu.com,CompactBuffer(</span><br><span class="line">((www.baidu.com,url1),1), </span><br><span class="line">((www.baidu.com,url3),3),</span><br><span class="line"> ((www.baidu.com,url5),5),</span><br><span class="line">  ((www.baidu.com,url2),2),</span><br><span class="line">   ((www.baidu.com,url4),4)))</span><br><span class="line">   </span><br><span class="line">(www.twitter.com,CompactBuffer(((www.twitter.com,url9),6), ((www.twitter.com,url10),11), ((www.twitter.com,url6),1)))</span><br><span class="line">(www.google.com,CompactBuffer(((www.google.com,url6),7), ((www.google.com,url8),7), ((www.google.com,url1),1), ((www.google.com,url2),2)))</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">       processRDD.reduceByKey(_+_)</span><br><span class="line">          .groupBy(_._1._1)</span><br><span class="line">            .mapValues(x =&gt; &#123;</span><br><span class="line">              x.toList.sortBy(-_._2)  // toList是一个很大的安全隐患    </span><br><span class="line">                .map(x =&gt; (x._1._2, x._2)).take(TOPN)</span><br><span class="line">            &#125;).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(www.baidu.com,List((url5,5), (url4,4)))</span><br><span class="line">(www.twitter.com,List((url10,11), (url9,6)))</span><br><span class="line">(www.google.com,List((url6,7), (url8,7)))</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>  x.toList.sortBy(-<em>.</em>2)  // toList是一个很大的安全隐患，为什么这么说呢？<br>  x来了一亿条数据 list就炸掉了 所以这样 虽然能出结果 但是不能用<br>  如何规避掉呢 这块就使用rdd算子 不用scala的高级函数</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">如何解决呢？</span><br><span class="line">1.方法：</span><br><span class="line">	分而治之的思路</span><br><span class="line">		类似mapreduce 思想  一个文件 拆成多个inputsplits 每个split单独处理 之后reduce聚合</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    // 分而治之的思路</span><br><span class="line">        val sites = Array(&quot;www.baidu.com&quot;,&quot;www.google.com&quot;,&quot;www.twitter.com&quot;)</span><br><span class="line">        for(site &lt;- sites) &#123;</span><br><span class="line">          processRDD.filter(_._1._1 == site)</span><br><span class="line">            .reduceByKey(_+_).sortBy(-_._2)</span><br><span class="line">            .take(TOPN).foreach(println)</span><br><span class="line">        &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line"></span><br><span class="line">有什么问题？</span><br><span class="line">1.会产生好多job  去ui上看 </span><br><span class="line">2.val sites = Array(&quot;www.baidu.com&quot;,&quot;www.google.com&quot;,&quot;www.twitter.com&quot;) 不要这么写</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">方法2; 优化方法1</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    // 分而治之的思路</span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_).sortBy(-_._2) .take(TOPN).foreach(println)</span><br><span class="line">    &#125;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line"></span><br><span class="line">问题：</span><br><span class="line"> val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">生产上能直接collect么？不能 这代码怎么改进呢？</span><br></pre></td></tr></table></figure></div>

<h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">reduceByKey走的是 defaultPartitioner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = &#123;</span><br><span class="line">    val rdds = (Seq(rdd) ++ others)</span><br><span class="line">    val hasPartitioner = rdds.filter(_.partitioner.exists(_.numPartitions &gt; 0))</span><br><span class="line">    if (hasPartitioner.nonEmpty) &#123;</span><br><span class="line">      hasPartitioner.maxBy(_.partitions.length).partitioner.get</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      if (rdd.context.conf.contains(&quot;spark.default.parallelism&quot;)) &#123;</span><br><span class="line">        new HashPartitioner(rdd.context.defaultParallelism)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        new HashPartitioner(rdds.map(_.partitions.length).max)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">RDD的5大特性中有一条是 Partitioner</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024101322660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</span><br><span class="line">      : RDD[(K, V)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    val part = new RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">    new ShuffledRDD[K, V, V](self, part)</span><br><span class="line">      .setKeyOrdering(if (ascending) ordering else ordering.reverse)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">RangePartitioner：</span><br><span class="line"></span><br><span class="line">object PartitionerApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">  </span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,30,100,300,400,500),3)</span><br><span class="line">  //Kafka分区策略</span><br><span class="line">    data.zipWithIndex().sortByKey()</span><br><span class="line">      .mapPartitionsWithIndex((index, partition)=&gt;&#123;</span><br><span class="line">        partition.map(x=&gt;s&quot;分区是$index, 元素是$&#123;x._1&#125;&quot;)</span><br><span class="line">      &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">分区是0, 元素是1</span><br><span class="line">分区是0, 元素是2</span><br><span class="line">分区是0, 元素是3</span><br><span class="line">分区是0, 元素是4</span><br><span class="line">分区是1, 元素是5</span><br><span class="line">分区是1, 元素是6</span><br><span class="line">分区是1, 元素是30</span><br><span class="line">分区是1, 元素是100</span><br><span class="line">分区是2, 元素是300</span><br><span class="line">分区是2, 元素是400</span><br><span class="line">分区是2, 元素是500</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def groupByKey(): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">defaultPartitioner --》HashPartitioner</span><br><span class="line"></span><br><span class="line">object PartitionerApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">   </span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,30,100,300,400,500),3)</span><br><span class="line">    data.zipWithIndex().groupByKey()</span><br><span class="line">      .mapPartitionsWithIndex((index, partition)=&gt;&#123;</span><br><span class="line">        partition.map(x=&gt;s&quot;分区是$index, 元素是$&#123;x._1&#125;&quot;)</span><br><span class="line">      &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是;</span><br><span class="line">分区是0, 元素是300</span><br><span class="line">分区是0, 元素是30</span><br><span class="line">分区是0, 元素是6</span><br><span class="line">分区是0, 元素是3</span><br><span class="line">分区是1, 元素是100</span><br><span class="line">分区是1, 元素是4</span><br><span class="line">分区是1, 元素是1</span><br><span class="line">分区是1, 元素是400</span><br><span class="line">分区是2, 元素是500</span><br><span class="line">分区是2, 元素是5</span><br><span class="line">分区是2, 元素是2</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<h2 id="多路径输出-定制化业务"><a href="#多路径输出-定制化业务" class="headerlink" title="多路径输出 ***定制化业务"></a>多路径输出 ***定制化业务</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">日志格式：</span><br><span class="line">uid1	Andriod	v3	215.197.96.120	4780	2019-09-11 08:37:33	9232	app19-20-18	香港			2019	09	11</span><br><span class="line">uid4	Symbain	71.10.97	168.170.39.193	189	2019-09-11 01:44:36	9232	app19-20-18	湖北	武汉	联通	2019	09	11</span><br><span class="line">uid2	linux	v2	168.170.39.193	189	2019-09-11 03:08:19	9232	app15-1-14-11	湖北	武汉	联通	2019	09	11</span><br><span class="line">uid3	linux	v2	168.170.39.193	189	2019-09-11 03:51:14	9232	app15-1-14-11	湖北	武汉	联通	2019	09	11</span><br><span class="line">uid6	mac	71.10.97	171.125.131.128	4780	2019-09-11 06:54:29	9232	app19-20-18	山西	忻州	联通	2019	09	11</span><br><span class="line">uid10	mac	71.10.97	171.125.131.128	4780	2019-09-11 07:09:27	189	app15-1-14-11	山西	忻州	联通	2019	09	11</span><br><span class="line"></span><br><span class="line">eg：给你一个完整的日志 做成 客户定制版  （可以去各大云平台cdn上查看产品）</span><br><span class="line">这个功能很重要 定制是收钱的（这个功能 公司一年收益是很多的 ）</span><br><span class="line"></span><br><span class="line">       1.假设按照不同的 品牌进行落盘</span><br><span class="line">       Andriod的数据都落在 Andriod的文件夹下</span><br><span class="line">       Symbain的数据都落在 Symbain的文件夹下</span><br><span class="line">      2. 输出的日志 客户想要什么字段的日志就输出什么字段的日志 （而不是把全部的日志都输出）</span><br></pre></td></tr></table></figure></div>

<p> <strong>1.假设按照不同的 品牌进行落盘</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输出的时候相当于根据某一个字段进行输出</span><br></pre></td></tr></table></figure></div>
<p>我们一步一步的来：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    /**</span><br><span class="line">      * Android</span><br><span class="line">      *   xxxx.log</span><br><span class="line">      *</span><br><span class="line">      * iOS</span><br><span class="line">      *   xxx.log</span><br><span class="line">      */</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(8))</span><br><span class="line">      .saveAsTextFile(output)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这个结果肯定是不对的 都放到一个目录下的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102412035911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">所有日志到在一个文件夹下：</span><br><span class="line">part-00000：mac、linux、Symbain  的 数据</span><br><span class="line">part-00001：Andriod </span><br><span class="line">part-00002 ：空</span><br><span class="line">part-00003：空</span><br><span class="line">part-00004：windows</span><br><span class="line">	</span><br><span class="line">	不仅仅是日志到在一个文件夹下 而且有的同一个文件下有别的品牌的数据</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1.我们是要把某一个字段作为输出文件夹名 </span><br><span class="line">这就是多目录输出 </span><br><span class="line">2.mapreduce里面有这个类  MultipleTextOutputFormat</span><br><span class="line">所以我们自己实现一个类继承这个类就可以</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * This class extends the MultipleOutputFormat, allowing to write the output</span><br><span class="line"> * data to different output files in Text output format.</span><br><span class="line"> */</span><br><span class="line">@InterfaceAudience.Public</span><br><span class="line">@InterfaceStability.Stable</span><br><span class="line">public class MultipleTextOutputFormat&lt;K, V&gt;</span><br><span class="line">    extends MultipleOutputFormat&lt;K, V&gt; &#123;</span><br><span class="line"></span><br><span class="line">  private TextOutputFormat&lt;K, V&gt; theTextOutputFormat = null;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  protected RecordWriter&lt;K, V&gt; getBaseRecordWriter(FileSystem fs, JobConf job,</span><br><span class="line">      String name, Progressable arg3) throws IOException &#123;</span><br><span class="line">    if (theTextOutputFormat == null) &#123;</span><br><span class="line">      theTextOutputFormat = new TextOutputFormat&lt;K, V&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">    return theTextOutputFormat.getRecordWriter(fs, job, name, arg3);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class MyDataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    补充里面的实现方法</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024121310993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>没有自己想要的方法，继续看MultipleTextOutputFormat的父类<br><img src="https://img-blog.csdnimg.cn/20191024121544286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> class MyDataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;   </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line"> 1.</span><br><span class="line">   s&quot;$key/$name&quot;   </span><br><span class="line">   key和name是什么东西呢？一会debug看一下</span><br><span class="line">2.这个东西写好了之后怎么用呢？</span><br><span class="line"> 第一个测试代码基础上 就不能使用saveAsTextFile</span><br><span class="line"> 因为你要定向输出到某一个类里面去 需要设置FileOutputFormat</span><br><span class="line"></span><br><span class="line">这和在mapreduce里是一样的 </span><br><span class="line">这块就要使用 saveAsHadoopFile</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024122713780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class</span><br><span class="line"> * supporting the key and value types K and V in this RDD. Compress with the supplied codec.</span><br><span class="line"> */</span><br><span class="line">def saveAsHadoopFile(</span><br><span class="line">    path: String,</span><br><span class="line">    keyClass: Class[_],</span><br><span class="line">    valueClass: Class[_],</span><br><span class="line">    outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">    codec: Class[_ &lt;: CompressionCodec]): Unit = self.withScope &#123;</span><br><span class="line">  saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">    new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],classOf[RuozedataMultipleTextOutputFormat])</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">查看结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024123422488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191024123557180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>怎么才能去掉呢？<br><img src="https://img-blog.csdnimg.cn/2019102412383967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">知道了kv代表了什么。那么如何去掉文件里的多出那一列key值呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102412411829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">默认generateActualKey 返回是最终输出的key  所以我们自定义的类里 重写这个方法 就ok了</span><br><span class="line">  /**</span><br><span class="line">   * Generate the actual key from the given key/value. The default behavior is that</span><br><span class="line">   * the actual key is equal to the given key</span><br><span class="line">   * </span><br><span class="line">   * @param key</span><br><span class="line">   *          the key of the output data</span><br><span class="line">   * @param value</span><br><span class="line">   *          the value of the output data</span><br><span class="line">   * @return the actual key derived from the given key/value</span><br><span class="line">   */</span><br><span class="line">  protected K generateActualKey(K key, V value) &#123;</span><br><span class="line">    return key;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">你mapreduce代码里 不输出值 用什么？使用NullWritable.get()   不可能使用 “” 或者 null 你可以测试一下使用它们输出是什么。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],classOf[RuozedataMultipleTextOutputFormat])</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">查看结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024124752801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>基本功能实现完成 </p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark06-依赖关系" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/24/Spark06-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/">Spark06--依赖关系</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/24/Spark06-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/" class="article-date">
  <time datetime="2018-01-24T12:02:17.000Z" itemprop="datePublished">2018-01-24</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rdd ==&gt; transformation s ==&gt; action</span><br><span class="line">就是rdd经过一系列的转换 最后触发action</span><br><span class="line">eg：</span><br><span class="line">textFile(path) ==&gt; map ==&gt; filter ==&gt; ...  ==&gt; collect</span><br><span class="line">每一步转换都会形成一个rdd </span><br><span class="line">RDDA   RDDB   RDDC</span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">一个rdd 三个分区 经过一个map之后 分区不会发生变化的 再filter 分区也是三个</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.你对rdd做一个map操作  其实是对rdd内部的所有数据做map操作  ----RDD篇</span><br><span class="line">2.窄依赖操作 默认不会造成分区的个数发生变化</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023093733887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对于这个场景他们之间是有一个<strong>依赖关系</strong>的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1.假如说 RDDB 分区里 6，8 这元素在计算的时候挂了</span><br><span class="line">那么spark再重新计算的时候 它只需要重新计算这一个分区就可以了</span><br><span class="line">2.这个分区里的数据怎么来的呢？</span><br><span class="line">直接从上一个rddA分区 里拿过来 计算就可以 其他分区不会做处理  所以这里面存在依赖关系的</span><br><span class="line">3.6和8这个元素的这个分区 到底从RDDA的哪一个分区过来的 </span><br><span class="line">这个是必然知道的 再spark里叫Lineage</span><br><span class="line">4.Lineage ： 一个rdd 是如何从父RDD计算的来的</span><br><span class="line">5.RDD里的五大特性的其中一个特性 是可以得到依赖关系的 </span><br><span class="line"></span><br><span class="line">eg：因为你每次transformation的时候会把这个依赖关系记录下来的   这样就知道父rdd是谁</span><br><span class="line">就是自己数据坏了 去爸爸那计算恢复 总有源头可以计算恢复 </span><br><span class="line">这个机制</span><br><span class="line">就是Spark性能高的一个非常重要的原因</span><br><span class="line"></span><br><span class="line">6. 性能 + 容错 （容错也体现在 数据坏了 重新算一下就ok）</span><br><span class="line">7. 整个过程就是一个计算链</span><br><span class="line">8. 如果转换非常多 </span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">	这一个链路 100个转换 算到第99个数据坏了 ，如果要重头算 也是挺麻烦的一件事 </span><br><span class="line">	core里面 提供 checkpoint（根本用不到 了解即可）</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023095648257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>(1)idea中debug是可以看到依赖关系的<br><img src="https://img-blog.csdnimg.cn/20191023101103284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>所以整个过程中 你的RDD是怎么来的 spark是知道的</p>
<p>(2) spark-shell中</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.map(_*2)</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; val c = b.filter(_ &gt; 6)</span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: Array[Int] = Array(8, 10)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023101642501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023101727428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">那么这个过程中到底产生多少个RDD呢？</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: Array[Int] = Array(8, 10)</span><br><span class="line"></span><br><span class="line">scala&gt; c.toDebugString</span><br><span class="line">res1: String =</span><br><span class="line">(2) MapPartitionsRDD[2] at filter at &lt;console&gt;:25 []</span><br><span class="line"> |  MapPartitionsRDD[1] at map at &lt;console&gt;:25 []</span><br><span class="line"> |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">parallelize  --》ParallelCollectionRDD</span><br><span class="line">map  --》MapPartitionsRDD</span><br><span class="line">filter  ---》MapPartitionsRDD</span><br><span class="line"></span><br><span class="line">那么这几个东西哪里来的呢？看源码</span><br><span class="line"></span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">parallelize 返回的是一个RDD 而真正的类型是 ParallelCollectionRDD 其他同理</span><br></pre></td></tr></table></figure></div>
<h2 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile() 这一个过程产生多少个RDD呢？</span><br><span class="line"></span><br><span class="line">scala&gt; sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).collect()</span><br><span class="line">res2: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">结果出来了 到页面上看一下。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102310260984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">这个过程产生了多少rdd呢？</span><br><span class="line">scala&gt; sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).toDebugString</span><br><span class="line">res3: String =</span><br><span class="line">(2) ShuffledRDD[12] at reduceByKey at &lt;console&gt;:25 []</span><br><span class="line"> +-(2) MapPartitionsRDD[11] at map at &lt;console&gt;:25 []</span><br><span class="line">    |  MapPartitionsRDD[10] at flatMap at &lt;console&gt;:25 []</span><br><span class="line">    |  file:///home/double_happy/data/double_happy.txt MapPartitionsRDD[9] at textFile at &lt;console&gt;:25 []</span><br><span class="line">    |  file:///home/double_happy/data/double_happy.txt HadoopRDD[8] at textFile at &lt;console&gt;:25 []</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">textFile ：  HadoopRDD +  MapPartitionsRDD</span><br><span class="line">flatMap  ： MapPartitionsRDD</span><br><span class="line">map  ： MapPartitionsRDD</span><br><span class="line">reduceByKey  ： ShuffledRDD</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">textFile  过程：</span><br><span class="line"></span><br><span class="line">1.textFile</span><br><span class="line"> def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2.hadoopFile</span><br><span class="line"> def hadoopFile[K, V](</span><br><span class="line">      path: String,</span><br><span class="line">      inputFormatClass: Class[_ &lt;: InputFormat[K, V]],</span><br><span class="line">      keyClass: Class[K],</span><br><span class="line">      valueClass: Class[V],</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">hadoopFile 我没用拷贝全 但是足够了 返回值是一个 kv类型的   真正的返回的是HadoopRDD</span><br><span class="line"></span><br><span class="line">3.hadoopFile 就是 mapreduce里的 读取文本文件的mapper过程</span><br><span class="line"></span><br><span class="line">通过：mapper</span><br><span class="line">TextInputFormat</span><br><span class="line">mapper: LongWritable（每行数据的偏移量）  Text(每行数据的内容)</span><br><span class="line">那么 RDD[(K, V) 就是 （偏移量，Text）</span><br><span class="line"></span><br><span class="line">4.</span><br><span class="line">    hadoopFile(path, classOf[TextInputFormat],</span><br><span class="line">     classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions)</span><br><span class="line">      .map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">      </span><br><span class="line">这个HadoopRDD 之后的map操作 所以会产生MapPartitionsRDD</span><br><span class="line">map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">就是把读取的内容拿出来</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	rdd里的分区数据对应hdfs上一个文件有多少个block</span><br><span class="line"></span><br><span class="line">所以HadoopRDD底层实现可以去看一下 </span><br><span class="line"></span><br><span class="line"> override def getPartitions: Array[Partition] = &#123;</span><br><span class="line">    val jobConf = getJobConf()</span><br><span class="line">    // add the credentials here as this can be called before SparkContext initialized</span><br><span class="line">    SparkHadoopUtil.get.addCredentials(jobConf)</span><br><span class="line">    val inputFormat = getInputFormat(jobConf)</span><br><span class="line">    val inputSplits = inputFormat.getSplits(jobConf, minPartitions)</span><br><span class="line">    val array = new Array[Partition](inputSplits.size)</span><br><span class="line">    for (i &lt;- 0 until inputSplits.size) &#123;</span><br><span class="line">      array(i) = new HadoopPartition(id, i, inputSplits(i))</span><br><span class="line">    &#125;</span><br><span class="line">    array</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">getInputFormat(jobConf).getSplits(jobConf, minPartitions) 明白了吗</span><br></pre></td></tr></table></figure></div>
<h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">窄依赖</span><br><span class="line">       一个父RDD的partition至多被子RDD的partition使用一次</span><br><span class="line">       OneToOneDependency</span><br><span class="line">       都在一个stage中完成</span><br><span class="line">宽依赖   &lt;= 会产生shuffle 会有新的stage</span><br><span class="line">       一个父RDD的partition会被子RDD的partition使用多次</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191023111033655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105631974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105652721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105850276.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023110813166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果经过宽依赖之后的RDD的某一个分区数据挂掉</span><br><span class="line">需要去父RDD重新计算 会把父亲所有分区都会算一下才行 </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.所有分区都要重算</span><br><span class="line">从容错的角度来说，在开发过程，能使用窄依赖就使用窄依赖 emm这就话 不全对</span><br><span class="line">在某些情况下 会把窄依赖改成宽依赖 来实现。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023111336288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="解析wc过程"><a href="#解析wc过程" class="headerlink" title="解析wc过程"></a>解析wc过程</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;)</span><br><span class="line"> val words = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line"> val pair = words.map((_,1))</span><br><span class="line"> val result = pair.reduceByKey(_+_)</span><br><span class="line">   result.collect()</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023114118106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">为什么会多一个conbine操作呢？</span><br><span class="line">reduceBykey算子底层封装好的</span><br><span class="line"></span><br><span class="line">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> def combineByKeyWithClassTag[C](</span><br><span class="line">      createCombiner: V =&gt; C,</span><br><span class="line">      mergeValue: (C, V) =&gt; C,</span><br><span class="line">      mergeCombiners: (C, C) =&gt; C,</span><br><span class="line">      partitioner: Partitioner,</span><br><span class="line">      mapSideCombine: Boolean = true,</span><br><span class="line">      serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != null, &quot;mergeCombiners must be defined&quot;) // required as of Spark 0.9.0</span><br><span class="line">    if (keyClass.isArray) &#123;</span><br><span class="line">      if (mapSideCombine) &#123;</span><br><span class="line">        throw new SparkException(&quot;Cannot use map-side combining with array keys.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      if (partitioner.isInstanceOf[HashPartitioner]) &#123;</span><br><span class="line">        throw new SparkException(&quot;HashPartitioner cannot partition array keys.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    val aggregator = new Aggregator[K, V, C](</span><br><span class="line">      self.context.clean(createCombiner),</span><br><span class="line">      self.context.clean(mergeValue),</span><br><span class="line">      self.context.clean(mergeCombiners))</span><br><span class="line">    if (self.partitioner == Some(partitioner)) &#123;</span><br><span class="line">      self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        val context = TaskContext.get()</span><br><span class="line">        new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">      &#125;, preservesPartitioning = true)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new ShuffledRDD[K, V, C](self, partitioner)</span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">combineByKeyWithClassTag中的 mapSideCombine: Boolean = true</span><br><span class="line"></span><br><span class="line">map端输出 设置Combine为true </span><br><span class="line"></span><br><span class="line">reduceBykey这个算子有Combine，那么groupBykey算子有么？</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    // groupByKey shouldn&apos;t use map side combine because map side combine does not</span><br><span class="line">    // reduce the amount of data shuffled and requires all map side data be inserted</span><br><span class="line">    // into a hash table, leading to more objects in the old gen.</span><br><span class="line">    val createCombiner = (v: V) =&gt; CompactBuffer(v)</span><br><span class="line">    val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v</span><br><span class="line">    val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2</span><br><span class="line">    val bufs = combineByKeyWithClassTag[CompactBuffer[V]](</span><br><span class="line">      createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span><br><span class="line">    bufs.asInstanceOf[RDD[(K, Iterable[V])]]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mapSideCombine = false </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">reduceByKey</span><br><span class="line">	有map端输出预聚合功能的</span><br><span class="line">groupBykey</span><br><span class="line">	全数据shuffle的 ，没有预聚合</span><br></pre></td></tr></table></figure></div>
<h2 id="shuffle-operations"><a href="#shuffle-operations" class="headerlink" title="shuffle operations"></a>shuffle operations</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">shuffle operations</a></p>
<p>The shuffle is Spark’s mechanism for <strong>re-distributing data</strong> so that it’s grouped differently across partitions. This typically involves <strong>copying data across executors and machines</strong>, making the shuffle a complex and costly operation.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re-distributing data：</span><br><span class="line">数据重新分区 --就是shuffle过程  我画的wc那个图</span><br></pre></td></tr></table></figure></div>
<p>Operations which can cause a shuffle include repartition operations like repartition and coalesce, ‘ByKey operations (except for counting) like groupByKey and reduceByKey, and join operations like cogroup and join.</p>
<p>上面这句话是不严谨的 之后测试证实。</p>
<p><strong>The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O.</strong> </p>
<p>这块官网好好读读</p>
<h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">    val lines = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;)</span><br><span class="line">    val words = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">    val pair = words.map((_,1))</span><br><span class="line">    </span><br><span class="line">   val result2 = pair.groupByKey()</span><br><span class="line">    val result1 = pair.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">假设pair之后还有其他的业务逻辑</span><br><span class="line">这里是：</span><br><span class="line">	groupByKey</span><br><span class="line">	reduceByKey</span><br><span class="line"></span><br><span class="line">到pair为止 大家都是公用的 这块就有必要使用cache机制 </span><br><span class="line"></span><br><span class="line">如果不做这个操作和做了 有什么区别呢？</span><br></pre></td></tr></table></figure></div>
<p>（1）没有做cache测试</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[16] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res4: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br><span class="line">查看4040页面</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023145325933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再执行一遍 re.collect 页面还是这样的 </p>
<p>（2）做cache</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res0: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res1: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.cache</span><br><span class="line">res2: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line">查看页面</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023145545411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再执行一边</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102314564550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>会发现执行了两次 就根本不是一个东西了 ，做了cache已经把我们的东西持久化到默认的存储级别里去了，下次就会去缓存里读取数据了</strong><br><img src="https://img-blog.csdnimg.cn/20191023145840158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.不做cache 如果你对同一个操作执行多次 下一次会从头开始执行</span><br><span class="line">2.如果做了cache （lazy 的操作并不会触发）</span><br><span class="line">3.cache后 默认的存储级别后 为什么数据量会变大了呢？</span><br><span class="line">	之后再说。</span><br></pre></td></tr></table></figure></div>
<h2 id="persist和cache的区别"><a href="#persist和cache的区别" class="headerlink" title="persist和cache的区别"></a>persist和cache的区别</h2><p>You can mark an RDD to be persisted using the persist() or cache() methods on it. <strong>The first time it is computed in an action,</strong> it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.<br> If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the <strong>RDD.unpersist() method.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The first time it is computed in an action </span><br><span class="line">就是说 sparkcore里的</span><br><span class="line">persist、cache 执行是在遇到action算子 才触发</span><br><span class="line"></span><br><span class="line">在迭代次数比较多的场景下 使用</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res0: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res1: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.cache</span><br><span class="line">res2: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.unpersist()</span><br><span class="line">res5: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023150726195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cache 、 persist   是 lazy的 </span><br><span class="line">2.unpersist  是 eager的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def cache(): this.type = persist()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</span><br><span class="line"></span><br><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,    // _deserialized 不序列化</span><br><span class="line">    private var _replication: Int = 1)</span><br><span class="line">  extends Externalizable</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.cache</span><br><span class="line">    ==&gt; persist</span><br><span class="line">        ==&gt; persist(MEMORY_ONLY)</span><br><span class="line">2.cache、persist 默认都走的是MEMORY_ONLY</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023151458914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>换一种存储级别<br><img src="https://img-blog.csdnimg.cn/20191023152323222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>数据是不是小了</strong> 序列化的会节省空间</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; re.collect</span><br><span class="line">res7: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.unpersist()</span><br><span class="line">res8: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line"></span><br><span class="line">scala&gt; re.persist(StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">res9: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res10: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<p><strong>Which Storage Level to Choose?</strong><br><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#which-storage-level-to-choose" target="_blank" rel="noopener">Which Storage Level to Choose?</a></p>
<p>一定要做序列化么？这和压缩是一个道理<br><strong>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency.</strong></p>
<h2 id="coalesce和repartition"><a href="#coalesce和repartition" class="headerlink" title="coalesce和repartition"></a>coalesce和repartition</h2><p>repartition</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">   * Return a new RDD that has exactly numPartitions partitions.</span><br><span class="line">   *</span><br><span class="line">   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses</span><br><span class="line">   * a shuffle to redistribute data.</span><br><span class="line">   *</span><br><span class="line">   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,</span><br><span class="line">   * which can avoid performing a shuffle.</span><br><span class="line">   */</span><br><span class="line">  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = true)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Can increase or decrease the level of parallelism in this RDD</span><br><span class="line"> shuffle = true</span><br><span class="line">就是</span><br><span class="line">repartition 无论增大还是减少 分区数 它都走shuffle</span><br><span class="line"></span><br><span class="line">增大分区数 我们使用repartition  </span><br><span class="line">repartition 底层调用coalesce</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark03</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object CoalesceAndRepartitionApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val data = sc.parallelize(List(1 to 9: _*),3)</span><br><span class="line"></span><br><span class="line">    data.mapPartitionsWithIndex((index,partition) =&gt; &#123;</span><br><span class="line">      partition.map(x=&gt;s&quot;分区是$index,元素是$x&quot;)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    val repartitionRDD = data.repartition(4)</span><br><span class="line">    repartitionRDD.mapPartitionsWithIndex((index,partition) =&gt; &#123;</span><br><span class="line">      partition.map(x=&gt;s&quot;分区是$index,元素是$x&quot;)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">分区是1,元素是4</span><br><span class="line">分区是0,元素是1</span><br><span class="line">分区是1,元素是5</span><br><span class="line">分区是1,元素是6</span><br><span class="line">分区是0,元素是2</span><br><span class="line">分区是0,元素是3</span><br><span class="line">分区是2,元素是7</span><br><span class="line">分区是2,元素是8</span><br><span class="line">分区是2,元素是9</span><br><span class="line">-------------------------</span><br><span class="line">分区是1,元素是3</span><br><span class="line">分区是0,元素是2</span><br><span class="line">分区是1,元素是6</span><br><span class="line">分区是0,元素是5</span><br><span class="line">分区是1,元素是9</span><br><span class="line">分区是0,元素是8</span><br><span class="line">分区是3,元素是1</span><br><span class="line">分区是3,元素是4</span><br><span class="line">分区是3,元素是7</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1 to 9: _*),3)</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; data.repartition(4)</span><br><span class="line">res11: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at repartition at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; data.repartition(4).collect</span><br><span class="line">res12: Array[Int] = Array(2, 7, 3, 4, 8, 5, 9, 1, 6)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023154639524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.能够说明 repartition是走shuffle的</span><br></pre></td></tr></table></figure></div>

<p><strong>coalesce</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Return a new RDD that is reduced into `numPartitions` partitions.</span><br><span class="line">   *</span><br><span class="line">   * This results in a narrow dependency, e.g. if you go from 1000 partitions</span><br><span class="line">   * to 100 partitions, there will not be a shuffle, instead each of the 100</span><br><span class="line">   * new partitions will claim 10 of the current partitions. If a larger number</span><br><span class="line">   * of partitions is requested, it will stay at the current number of partitions.</span><br><span class="line">   *</span><br><span class="line">   * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1,</span><br><span class="line">   * this may result in your computation taking place on fewer nodes than</span><br><span class="line">   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span><br><span class="line">   * you can pass shuffle = true. This will add a shuffle step, but means the</span><br><span class="line">   * current upstream partitions will be executed in parallel (per whatever</span><br><span class="line">   * the current partitioning is).</span><br><span class="line">   *</span><br><span class="line">   * @note With shuffle = true, you can actually coalesce to a larger number</span><br><span class="line">   * of partitions. This is useful if you have a small number of partitions,</span><br><span class="line">   * say 100, potentially with a few partitions being abnormally large. Calling</span><br><span class="line">   * coalesce(1000, shuffle = true) will result in 1000 partitions with the</span><br><span class="line">   * data distributed using a hash partitioner. The optional partition coalescer</span><br><span class="line">   * passed in must be serializable.</span><br><span class="line">   */</span><br><span class="line">  def coalesce(numPartitions: Int, shuffle: Boolean = false,</span><br><span class="line">               partitionCoalescer: Option[PartitionCoalescer] = Option.empty)</span><br><span class="line">              (implicit ord: Ordering[T] = null)</span><br><span class="line">      : RDD[T] = withScope &#123;</span><br><span class="line">    require(numPartitions &gt; 0, s&quot;Number of partitions ($numPartitions) must be positive.&quot;)</span><br><span class="line">    if (shuffle) &#123;</span><br><span class="line">      /** Distributes elements evenly across output partitions, starting from a random partition. */</span><br><span class="line">      val distributePartition = (index: Int, items: Iterator[T]) =&gt; &#123;</span><br><span class="line">        var position = (new Random(index)).nextInt(numPartitions)</span><br><span class="line">        items.map &#123; t =&gt;</span><br><span class="line">          // Note that the hash code of the key will just be the key itself. The HashPartitioner</span><br><span class="line">          // will mod it with the number of total partitions.</span><br><span class="line">          position = position + 1</span><br><span class="line">          (position, t)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; : Iterator[(Int, T)]</span><br><span class="line"></span><br><span class="line">      // include a shuffle step so that our upstream tasks are still distributed</span><br><span class="line">      new CoalescedRDD(</span><br><span class="line">        new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        new HashPartitioner(numPartitions)),</span><br><span class="line">        numPartitions,</span><br><span class="line">        partitionCoalescer).values</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new CoalescedRDD(this, numPartitions, partitionCoalescer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Return a new RDD that is reduced into `numPartitions` partitions.</span><br><span class="line">1.coalesce 用来 reduced numPartitions </span><br><span class="line"></span><br><span class="line">shuffle: Boolean = false,</span><br><span class="line"></span><br><span class="line">2。coalesce 默认是不走shuffle的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; data.partitions.size</span><br><span class="line">res15: Int = 3</span><br><span class="line"></span><br><span class="line">scala&gt; data.coalesce(4).partitions.size</span><br><span class="line">res16: Int = 3</span><br><span class="line"></span><br><span class="line">scala&gt; data.coalesce(2).partitions.size</span><br><span class="line">res17: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt;data.coalesce(4,true).partitions.size    //这样就走shuffle啦</span><br><span class="line">res18: Int = 4</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">repartition调用的是coalesce算子，shuffle默认为true    会产生新的 stage</span><br><span class="line">coalesce  shuffle默认为false    传shuffle为true，就和repartition一样</span><br></pre></td></tr></table></figure></div>
<p>Operations which can cause a shuffle include repartition operations like repartition and coalesce, </p>
<p>所以这句话 coalesce 默认是不会产生shuffle的 官网这话不严谨。</p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark005-核心架构" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/23/Spark005-%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84/">Spark005--核心架构</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/23/Spark005-%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84/" class="article-date">
  <time datetime="2018-01-23T12:01:31.000Z" itemprop="datePublished">2018-01-23</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">spark核心架构官网</a></p>
<h2 id="核心术语Glossary"><a href="#核心术语Glossary" class="headerlink" title="核心术语Glossary"></a>核心术语Glossary</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">核心术语</span><br><span class="line">    Application *****</span><br><span class="line">        a driver program</span><br><span class="line">        executors on the cluster.</span><br><span class="line">    Application jar</span><br><span class="line">    Driver program  *****</span><br><span class="line">        main  </span><br><span class="line">        sc </span><br><span class="line">    Cluster manager</span><br><span class="line">    Deploy mode</span><br><span class="line">        YARN: RM NM(container)</span><br><span class="line">            cluster: Driver是跑在container</span><br><span class="line">            client：Driver就运行在你提交机器的本地</span><br><span class="line">                client是不是一定要是集群内的？gateway</span><br><span class="line">    Worker node</span><br><span class="line">    Executor  *****</span><br><span class="line">        process</span><br><span class="line">        runs tasks</span><br><span class="line">        keeps data in memory or disk storage across them</span><br><span class="line">        Each application has its own executors.</span><br><span class="line">            A：executor1 2 3</span><br><span class="line">            B：executor1 2 3</span><br><span class="line">    Task	*****</span><br><span class="line">        A unit of work that will be sent to one executor</span><br><span class="line">        RDD: partitions == task</span><br><span class="line">    Job    *****</span><br><span class="line">        action ==&gt; job</span><br><span class="line">    Stage</span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">spark-shell  应用程序</span><br><span class="line">一个application：1到n个job</span><br><span class="line">一个job ： 1到n个stage构成</span><br><span class="line">一个stage： 1到n个task  task与partition一一对应</span><br></pre></td></tr></table></figure></div>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>User program built on Spark. Consists of a driver program and executors on the cluster.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.User program built on Spark</span><br><span class="line">2.a driver program</span><br><span class="line">3. executors on the cluster.</span><br></pre></td></tr></table></figure></div>
<h2 id="Application-jar"><a href="#Application-jar" class="headerlink" title="Application jar"></a>Application jar</h2><p>A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. <strong>The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</strong></p>
<h2 id="Driver-program"><a href="#Driver-program" class="headerlink" title="Driver program"></a>Driver program</h2><p>The process running the main() function of the application and creating the SparkContext</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. running the main() function of the application </span><br><span class="line">2.  creating the SparkContext</span><br></pre></td></tr></table></figure></div>
<h2 id="Cluster-manager"><a href="#Cluster-manager" class="headerlink" title="Cluster manager"></a>Cluster manager</h2><p>An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</p>
<p>所以你的代码里 上传到集群的时候 不要写 死  appname 和 master<br>到时候通过 spark-submit 来指定就 可以选择 外部的cluster </p>
<h2 id="Deploy-mode"><a href="#Deploy-mode" class="headerlink" title="Deploy mode"></a>Deploy mode</h2><p><strong>Distinguishes where the driver process runs.</strong> In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.</p>
<p>一切以yarn为主</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YARN: RM NM(container)</span><br><span class="line">           cluster: Driver是跑在container  （跑在NM 里的container的 ）</span><br><span class="line">           client：Driver就运行在你提交机器的本地</span><br><span class="line">               client是不是一定要是集群内的？gateway</span><br></pre></td></tr></table></figure></div>
<h2 id="Worker-node"><a href="#Worker-node" class="headerlink" title="Worker node"></a>Worker node</h2><p>Any node that can run application code in the cluster</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn模式下 就是 nm </span><br><span class="line"></span><br><span class="line">run application code</span><br></pre></td></tr></table></figure></div>
<h2 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h2><p>a process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</p>
<p>一个进程 jps 就能看到<br><strong>对应yarn上 就跑在 nm的container里的</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 1.  runs tasks</span><br><span class="line">2.  keeps data in memory or disk storage across them</span><br><span class="line">3. Each application has its own executors.</span><br><span class="line">            A：executor1 2 3</span><br><span class="line">            B：executor1 2 3</span><br><span class="line">这6个东西 是跑在container里的</span><br></pre></td></tr></table></figure></div>
<h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>A unit of work that will be sent to one executor</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.task 是发送到 executor里的</span><br><span class="line">2.RDD: partitions == tasks</span><br><span class="line">RDD是由多个partition所构成的，</span><br><span class="line">一个partition就对应一个task</span><br></pre></td></tr></table></figure></div>
<h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.一个action算子对应一个job</span><br></pre></td></tr></table></figure></div>
<h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><p>Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.一组task的集合 叫一个stage</span><br><span class="line">2.遇到一个shuffle算子  就会被拆成两个stage </span><br><span class="line"></span><br><span class="line">那遇到两个shuffle算子 会被拆成几个stage呢？ 3个哈  </span><br><span class="line">（跟一个桌子切掉一个桌角 还有几个角一样的哈 你别把桌子对角切就行 ）</span><br></pre></td></tr></table></figure></div>
<h2 id="总结案例"><a href="#总结案例" class="headerlink" title="总结案例"></a>总结案例</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eg：</span><br><span class="line">spark-shell  应用程序</span><br><span class="line">一个application：1到n个job</span><br><span class="line">一个job ： 1到n个stage构成</span><br><span class="line">一个stage： 1到n个task  task与partition一一对应</span><br></pre></td></tr></table></figure></div>

<h2 id="Components"><a href="#Components" class="headerlink" title="Components  ***"></a>Components  ***</h2><p>Spark applications run as independent sets of processes on a cluster<br> <strong>independent sets of processes：就是executor</strong></p>
<p>Spark applications run as <strong>independent sets of processes on a cluster</strong>, coordinated by the SparkContext object in your main program (called the driver program).</p>
<p>Specifically, to run on a cluster, <strong>the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications.</strong> <strong>Once connected, Spark acquires executors on nodes in the cluster,</strong> <strong>which are processes that run computations and store data for your application.</strong> Next<strong>, <strong>it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors</strong></strong>. Finally, <strong>SparkContext sends tasks to the executors to run.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. which allocate resources across applications.  which  ===》  cluster managers</span><br><span class="line">2.  Once connected, Spark acquires executors on nodes in the cluster</span><br><span class="line">     一旦连接上 spark 就yarn集群的 nm 的contatiner 里启动executor</span><br><span class="line">3.which are processes that run computations and store data for your application ：  </span><br><span class="line">        which  ===》executor</span><br><span class="line">4.it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors </span><br><span class="line">    it ==》 sc</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/2019102214595656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.driver ： main方法里有个 sc</span><br></pre></td></tr></table></figure></div>



<p>There are several useful things to note about this architecture:</p>
<p>1.<strong>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.</strong> <strong>This has the benefit of isolating applications from each other,</strong> <strong>on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs).</strong> However, <strong>it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system</strong>.<br>2.Spark is agnostic to the underlying cluster manager. <strong>As long as it can acquire executor processes, and these communicate with each other</strong>, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. which stay up for the duration of the whole application and run tasks in multiple threads </span><br><span class="line">		which  ==》executor processes</span><br><span class="line">		1.stay up for the duration of the whole application</span><br><span class="line">		2.run tasks in multiple threads</span><br><span class="line">2.This has the benefit of isolating applications from each other</span><br><span class="line">		Each application gets its own executor processes 就是每个application有自己独立的 executor processes</span><br><span class="line">		带来的好处就是 applications之间是隔离的</span><br><span class="line">3.cannot be shared across different Spark applications</span><br><span class="line">	without writing it to an external storage system    ===》Alluxio这个框架 现在可以实现 多个application之间共享</span><br><span class="line"></span><br><span class="line">4. agnostic  不关注</span><br><span class="line">    As long as it can acquire executor processes, and these communicate with each other</span><br><span class="line">    指的是 driver 和 executor之间的通信</span><br></pre></td></tr></table></figure></div>


<p>3.<strong>The driver program must listen for and accept incoming connections from its executors throughout its lifetime</strong> (e.g., see spark.driver.port in the network config section). As such, the driver program must be <strong>network addressable</strong> from the worker nodes.<br>4.Because the driver schedules tasks on the cluster, <strong>it should be run close to the worker nodes</strong>, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from <strong>nearby than to run a driver far away from the worker nodes</strong>.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.The driver program must listen for and accept incoming connections from its executors throughout its lifetime</span><br><span class="line"> driver一定要知道你的executor在哪台机器上的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191022152821973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这块可以看到 executor 在那台机器的哪个端口上</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2.As such, the driver program must be network addressable from the worker nodes. </span><br><span class="line"></span><br><span class="line">所以yarn的client模式下 </span><br><span class="line">必须保证这台本地的机器能与yarn通了 ，client是不是一定要是集群内的？不一定的哈 能连上yarn就可以  </span><br><span class="line">     最开始的文章 gateway什么意思呢？你亲我一下我就告诉你 （这是对未来女朋友说的）</span><br><span class="line"></span><br><span class="line">3.driver进程最好离executor进程 近一点</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark004-总结前面的基本操作" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/22/Spark004-%E6%80%BB%E7%BB%93%E5%89%8D%E9%9D%A2%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/">Spark004--总结前面的基本操作</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/22/Spark004-%E6%80%BB%E7%BB%93%E5%89%8D%E9%9D%A2%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" class="article-date">
  <time datetime="2018-01-22T12:00:42.000Z" itemprop="datePublished">2018-01-22</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="实现排序功能"><a href="#实现排序功能" class="headerlink" title="实现排序功能"></a>实现排序功能</h2><p>（1）按照价格排序</p>
<p><strong>第一种</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;))</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          (name, price, amount)</span><br><span class="line">        &#125;).sortBy(-_._2).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">输出的结果：</span><br><span class="line">(iphone11,7000.0,1000)</span><br><span class="line">(皮鞭,20.0,10)</span><br><span class="line">(蜡烛,20.0,100)</span><br><span class="line">(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p> 这里一定要注意：<br> <strong>你运行出来的结果可能顺序不对，sortBy是全局排序的 所以你测试的时候 可以设置分区数为 1</strong></p>
<p>上面的结果当然也可以按多个排：<br>价格相同，再按照库存的数量排序</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          (name, price, amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;(-x._2,-x._3)).printInfo()</span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">(iphone11,7000.0,1000)</span><br><span class="line">(蜡烛,20.0,100)</span><br><span class="line">(皮鞭,20.0,10)</span><br><span class="line">(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>上面那种是使用Tuple的方式 ，生产上还是要封装一个类来实现的 </p>
<p><strong>第二种</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp02 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          new Products(name,price,amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>直接报错：<br>Error:(21, 18) No implicit Ordering defined for com.ruozedata.spark.spark02.Products.<br>        }).sortBy(x=&gt;x).printInfo()<br>Error:(21, 18) not enough arguments for method sortBy: (implicit ord: Ordering[com.ruozedata.spark.spark02.Products], implicit ctag: scala.reflect.ClassTag[com.ruozedata.spark.spark02.Products])org.apache.spark.rdd.RDD[com.ruozedata.spark.spark02.Products].<br>Unspecified value parameters ord, ctag.<br>        }).sortBy(x=&gt;x).printInfo()</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">按上面那么写有什么问题？</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Return this RDD sorted by the given key function.</span><br><span class="line">   */</span><br><span class="line">  def sortBy[K](</span><br><span class="line">      f: (T) =&gt; K,</span><br><span class="line">      ascending: Boolean = true,</span><br><span class="line">      numPartitions: Int = this.partitions.length)</span><br><span class="line">      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope &#123;</span><br><span class="line">    this.keyBy[K](f)</span><br><span class="line">        .sortByKey(ascending, numPartitions)</span><br><span class="line">        .values</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.有一个隐式转换的 需要一个Ordering 这个东西 ，而我们传进去的是x ==》 product </span><br><span class="line">它根本找不到任何排序规则</span><br></pre></td></tr></table></figure></div>

<p>因为：<br>Error:(21, 18) not enough arguments for method sortBy: (implicit ord: Ordering[com.ruozedata.spark.spark02.Products], implicit ctag: scala.reflect.ClassTag[com.ruozedata.spark.spark02.Products])org.apache.spark.rdd.RDD[com.ruozedata.spark.spark02.Products].<br>所以：<br>1.not enough arguments<br>2.(implicit ord: Ordering[com.ruozedata.spark.spark02.Products]   一个Products类型</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">class Products(val name:String , val price:Double,val amount:Int) </span><br><span class="line">      extends  Ordered[Products]&#123;</span><br><span class="line">  override def compare(that: Products): Int = &#123;</span><br><span class="line">    </span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>修改之后再看：<br><img src="https://img-blog.csdnimg.cn/2019102213123198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>又报错了：<br>Serialization stack:<br>    - object not serializable (class: com.ruozedata.spark.spark02.Products, value: com.ruozedata.spark.spark02.Products@237c4154)</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Products(val name:String , val price:Double,val amount:Int)</span><br><span class="line">      extends  Ordered[Products] with Serializable&#123;</span><br><span class="line">  override def compare(that: Products): Int = &#123;</span><br><span class="line"></span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>修改之后：<br><img src="https://img-blog.csdnimg.cn/20191022131456435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>没问题了 那么我想看输出结果 该怎么办呢？<br>toString 就可以了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Products(val name:String , val price:Double,val amount:Int)</span><br><span class="line">      extends  Ordered[Products] with Serializable&#123;</span><br><span class="line">  override def compare(that: Products): Int = &#123;</span><br><span class="line"></span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def toString: String = name + &quot;\t&quot; + price + &quot;\t&quot; +amount</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp02 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          new Products(name,price,amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">皮鞭	20.0	10</span><br><span class="line">蜡烛	20.0	100</span><br><span class="line">iphone11	7000.0	1000</span><br><span class="line">扑克牌	5.0	2000</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>工作当中别这么用 毕竟自己实现类 还需要序列化 挺麻烦的</p>
<p><strong>第三种  case class</strong><br>生产上用的比较多</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">case  class  Products02( name:String,  price:Double,  amount:Int)</span><br><span class="line">      extends  Ordered[Products02] &#123;</span><br><span class="line">  override def compare(that: Products02): Int = &#123;</span><br><span class="line"></span><br><span class="line">    this.amount - that.amount</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp02 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">//          new Products(name,price,amount)</span><br><span class="line">          Products02(name,price,amount)</span><br><span class="line"></span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">Products02(皮鞭,20.0,10)</span><br><span class="line">Products02(蜡烛,20.0,100)</span><br><span class="line">Products02(iphone11,7000.0,1000)</span><br><span class="line">Products02(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>那么 class  和case class 的区别是什么？ 看scala篇 </p>
<p>*<em>第四种     使用隐式转换的方式 *</em></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line">          new Products03(name,price,amount)</span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">class Products03( name:String,  price:Double,  amount:Int)</span><br></pre></td></tr></table></figure></div>
<p><strong>不准对Products03 做任何修改 完成排序功能该怎么办？</strong><br>Product3默认是不能排序的 ====implicit==&gt; 能排序的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line"></span><br><span class="line">object SortApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">        products.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot; &quot;)</span><br><span class="line">          val name = splits(0)</span><br><span class="line">          val price = splits(1).toDouble</span><br><span class="line">          val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">          new Products03(name,price,amount)</span><br><span class="line"></span><br><span class="line">        &#125;).sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * Product3默认是不能排序的 ====implicit==&gt; 能排序的</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">    implicit def product2Ordered(products: Products03):Ordered[Products03] = new Ordered[Products03]&#123;</span><br><span class="line">      override def compare(that: Products03): Int = &#123;</span><br><span class="line">        products.amount - that.amount</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Products03( val  name:String, val  price:Double,  val amount:Int) extends Serializable &#123;</span><br><span class="line">  override def toString: String = name + &quot;\t&quot; + price + &quot;\t&quot; + amount</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><strong>第五种 花里胡哨</strong><br> 能看懂就行 不需要掌握<br> 实际上也是 隐式转换   隐式变量</p>
<p>思路就是 ：<br>(implicit ord: Ordering[com.ruozedata.spark.spark02.Products]   一个Products类型</p>
<p>既然你却这个 就隐式转换给你 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line"></span><br><span class="line">object SortApp04 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">    val product = products.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot; &quot;)</span><br><span class="line">      val name = splits(0)</span><br><span class="line">      val price = splits(1).toDouble</span><br><span class="line">      val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">      (name, price, amount)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * Ordering on</span><br><span class="line">      *</span><br><span class="line">      * -x._2, -x._3  排序规则</span><br><span class="line">      * (Double,Int) 定义的是规则的返回值的类型  就是参与排序的 类型</span><br><span class="line">      * (String,Double,Int) 数据的类型</span><br><span class="line">      */</span><br><span class="line">    implicit val ord = Ordering[(Double,Int)].on[(String,Double,Int)](x=&gt;(-x._2, -x._3))</span><br><span class="line">    product.sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(iphone11,7000.0,1000)</span><br><span class="line">(蜡烛,20.0,100)</span><br><span class="line">(皮鞭,20.0,10)</span><br><span class="line">(扑克牌,5.0,2000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line"></span><br><span class="line">object SortApp04 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val products = sc.parallelize(List(&quot;皮鞭 20 10&quot;,&quot;蜡烛 20 100&quot;,&quot;扑克牌 5 2000&quot;,&quot;iphone11 7000 1000&quot;),1)</span><br><span class="line">    val product = products.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot; &quot;)</span><br><span class="line">      val name = splits(0)</span><br><span class="line">      val price = splits(1).toDouble</span><br><span class="line">      val amount = splits(2).toInt</span><br><span class="line"></span><br><span class="line">      Products02(name, price, amount)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * Ordering on</span><br><span class="line">      *</span><br><span class="line">      * -x._2, -x._3  排序规则</span><br><span class="line">      * (Double,Int) 定义的是规则的返回值的类型  就是参与排序的 类型</span><br><span class="line">      * (String,Double,Int) 数据的类型</span><br><span class="line">      */</span><br><span class="line">    implicit val ord = Ordering[(Double,Int)].on[Products02](x=&gt;(-x.price, -x.amount))</span><br><span class="line">    product.sortBy(x=&gt;x).printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">后面两种都是隐式转换：</span><br><span class="line">一个是增强类</span><br><span class="line">一个是隐式参数 </span><br><span class="line"></span><br><span class="line">隐式转换没什么 听着挺吓人的</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark002-transform-action" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/21/Spark002-transform-action/">Spark002-transform&amp;action</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/21/Spark002-transform-action/" class="article-date">
  <time datetime="2018-01-21T11:58:56.000Z" itemprop="datePublished">2018-01-21</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>转换操作不会立即执行的，不触发作业的执行 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RDD操作</span><br><span class="line">    transformation  转换    它不会立即执行  你写了1亿个转换  白写   lazy</span><br><span class="line">    action          动作    只有遇到action才会提交作业开始执行      eager</span><br></pre></td></tr></table></figure></div>

<h2 id="官网RDD算子介绍："><a href="#官网RDD算子介绍：" class="headerlink" title="官网RDD算子介绍："></a>官网RDD算子介绍：</h2><p>RDD Operations：<br>RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).</p>
<p>All <strong>transformations</strong> in Spark are <strong>lazy</strong>, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. <strong>This design enables Spark to run more efficiently.</strong> For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>
<p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">This design enables Spark to run more efficiently：</span><br><span class="line">对比Hadoop举一个例子：</span><br><span class="line">	1+1+1+1 这个动作</span><br><span class="line">MR：1+1=2  --&gt;落地--&gt;读取 +1=3  --&gt;落地---&gt;读取+1=4</span><br><span class="line">Spark：1+1+1+1 这块全部用transformations 来完成 ，真正计算的时候，才一次性提交上去。一个流水先就全都执行完了。</span><br></pre></td></tr></table></figure></div>
<h2 id="Transformations讲解前的说明"><a href="#Transformations讲解前的说明" class="headerlink" title="Transformations讲解前的说明"></a>Transformations讲解前的说明</h2><p>先说明我们的程序里创建SparkContex的方式，由于每次创建都要写appname，master，以及RDD数据集在Driver端打印出来查看都要写foreach(println)，每次都要写很麻烦，这里我们给封装一下。<br>效果展示：<br>    <img src="https://img-blog.csdnimg.cn/2019101211271935.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>如果我不想输出 我输入一个1进去即可：<br><img src="https://img-blog.csdnimg.cn/20191012113652954.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这是自己写的哈 ，老师上课留的作业 人家要求动手能力。不会全部给你，让你做一个伸手党就没意义。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.homework.utils</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">object ContextUtils &#123;</span><br><span class="line">  /**</span><br><span class="line">    * 获取sc</span><br><span class="line">    */</span><br><span class="line">  def getSparkContext(appname:String,defalut:String = &quot;local[2]&quot;): SparkContext = &#123;</span><br><span class="line">    val sparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line">    new SparkContext(sparkConf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>下面这个主要使用隐式转换，看不懂可以查看Scala博客：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.homework.utils</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">object ImplicitAspect &#123;</span><br><span class="line">  implicit def rdd2RichRDD[T](rdd : RDD[T]) : RichRDD[T] = new RichRDD[T](rdd)</span><br><span class="line">&#125;</span><br><span class="line">class RichRDD[T](rdd : RDD[T])&#123;</span><br><span class="line"> def printInfo(num : Int =0): Unit =&#123;</span><br><span class="line">    num match &#123;</span><br><span class="line">      case 0 =&gt; rdd.foreach(println);println(&quot;-------------------------&quot;)</span><br><span class="line">      case _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><p>源码面前，了无秘密。通过源码进行学习</p>
<h2 id="（1）Map相关的算子"><a href="#（1）Map相关的算子" class="headerlink" title="（1）Map相关的算子"></a>（1）Map相关的算子</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1.makeRDD / parallelize </span><br><span class="line"></span><br><span class="line"> def makeRDD[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">  &#125;</span><br><span class="line">注意：makeRDD底层调用的就是parallelize </span><br><span class="line"></span><br><span class="line"> /** Distribute a local Scala collection to form an RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call</span><br><span class="line">   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the</span><br><span class="line">   * modified collection. Pass a copy of the argument to avoid this.</span><br><span class="line">   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an</span><br><span class="line">   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.</span><br><span class="line">   * @param seq Scala collection to distribute</span><br><span class="line">   * @param numSlices number of partitions to divide the collection into</span><br><span class="line">   * @return RDD representing distributed collection</span><br><span class="line">   */</span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2.map : 处理每一条数据</span><br><span class="line">/**</span><br><span class="line">   * Return a new RDD by applying a function to all elements of this RDD.</span><br><span class="line">   */</span><br><span class="line">  def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012114402942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">RDD是有多个partition所构成的，</span><br><span class="line">3.mapPartitions: Return a new RDD by applying a function to each partition of this RDD. 对分区做处理的，一个分区里有很多的元素的。</span><br><span class="line"> /**</span><br><span class="line">   * Return a new RDD by applying a function to each partition of this RDD.</span><br><span class="line">   *</span><br><span class="line">   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span><br><span class="line">   * should be `false` unless this is a pair RDD and the input function doesn&apos;t modify the keys.</span><br><span class="line">   */</span><br><span class="line">  def mapPartitions[U: ClassTag](</span><br><span class="line">      f: Iterator[T] =&gt; Iterator[U],</span><br><span class="line">      preservesPartitioning: Boolean = false): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanedF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD(</span><br><span class="line">      this,</span><br><span class="line">      (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019101211493594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>一个是作用于每个元素，一个是作用于每个分区。<br><img src="https://img-blog.csdnimg.cn/20191012115049522.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这是作用于每个分区里的每个元素<br><img src="https://img-blog.csdnimg.cn/20191012115140493.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结果和map是一样的。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line"> /**</span><br><span class="line">      * map 处理每一条数据</span><br><span class="line">      * mapPartitions 对每个分区进行处理</span><br><span class="line">      *</span><br><span class="line">      * map：100个元素  10个分区 ==&gt; 知识点：要把RDD的数据写入MySQL  Connection次数</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">4.mapPartitionsWithIndex</span><br><span class="line">你想看哪个分区里的东西 这个算子可以拿的到</span><br><span class="line">/**</span><br><span class="line">   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index</span><br><span class="line">   * of the original partition.</span><br><span class="line">   *</span><br><span class="line">   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span><br><span class="line">   * should be `false` unless this is a pair RDD and the input function doesn&apos;t modify the keys.</span><br><span class="line">   */</span><br><span class="line">  def mapPartitionsWithIndex[U: ClassTag](</span><br><span class="line">      f: (Int, Iterator[T]) =&gt; Iterator[U],</span><br><span class="line">      preservesPartitioning: Boolean = false): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanedF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD(</span><br><span class="line">      this,</span><br><span class="line">      (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(index, iter),</span><br><span class="line">      preservesPartitioning)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012115918554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_30,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2019101212020396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>元素为什么这么存放的呢？之后再来讲解。<br>生产上是不关注这个分区里的哪个元素的 只是用来学。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">5.mapValues</span><br><span class="line"> /**</span><br><span class="line">   * Pass each value in the key-value pair RDD through a map function without changing the keys;</span><br><span class="line">   * this also retains the original RDD&apos;s partitioning.</span><br><span class="line">   */</span><br><span class="line">  def mapValues[U](f: V =&gt; U): RDD[(K, U)] = self.withScope &#123;</span><br><span class="line">    val cleanF = self.context.clean(f)</span><br><span class="line">    new MapPartitionsRDD[(K, U), (K, V)](self,</span><br><span class="line">      (context, pid, iter) =&gt; iter.map &#123; case (k, v) =&gt; (k, cleanF(v)) &#125;,</span><br><span class="line">      preservesPartitioning = true)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019101212080774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191012120846720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_50,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">6.flatmap = map + flatten   就是打扁以后 做map</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   *  Return a new RDD by first applying a function to all elements of this</span><br><span class="line">   *  RDD, and then flattening the results.</span><br><span class="line">   */</span><br><span class="line">  def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TraversableOnce:  一次遍历的意思   flattening the results.也就是压扁</span><br></pre></td></tr></table></figure></div>
<p>对比map：<br><img src="https://img-blog.csdnimg.cn/20191012121548376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2019101212132486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>map对里面元素做处理，是不会改变 内部的结构的 。</p>
<p>flatMap:<br><img src="https://img-blog.csdnimg.cn/20191012121907803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结果：<br><img src="https://img-blog.csdnimg.cn/20191012122329639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_60,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">秀操作的：没什么用</span><br><span class="line">scala&gt; sc.parallelize(1 to 5).flatMap(1 to _).collect</span><br><span class="line">res3: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure></div>
<h2 id="（2）glom"><a href="#（2）glom" class="headerlink" title="（2）glom"></a>（2）glom</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">glom：把每一个分区里的数据 形成一个数组  比mapwithindex好用</span><br><span class="line"> /**</span><br><span class="line">   * Return an RDD created by coalescing all elements within each partition into an array.</span><br><span class="line">   */</span><br><span class="line">  def glom(): RDD[Array[T]] = withScope &#123;</span><br><span class="line">    new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) =&gt; Iterator(iter.toArray))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 30).glom().collect</span><br><span class="line">res4: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), Array(16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="3-sample"><a href="#3-sample" class="headerlink" title="(3)sample"></a>(3)sample</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">sample:</span><br><span class="line">/**</span><br><span class="line">   * Return a sampled subset of this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span><br><span class="line">   * @param fraction expected size of the sample as a fraction of this RDD&apos;s size</span><br><span class="line">   *  without replacement: probability that each element is chosen; fraction must be [0, 1]</span><br><span class="line">   *  with replacement: expected number of times each element is chosen; fraction must be greater</span><br><span class="line">   *  than or equal to 0</span><br><span class="line">   * @param seed seed for the random number generator</span><br><span class="line">   *</span><br><span class="line">   * @note This is NOT guaranteed to provide exactly the fraction of the count</span><br><span class="line">   * of the given [[RDD]].</span><br><span class="line">   */</span><br><span class="line">  def sample(</span><br><span class="line">      withReplacement: Boolean,</span><br><span class="line">      fraction: Double,</span><br><span class="line">      seed: Long = Utils.random.nextLong): RDD[T] = &#123;</span><br><span class="line">    require(fraction &gt;= 0,</span><br><span class="line">      s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    withScope &#123;</span><br><span class="line">      require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction)</span><br><span class="line">      if (withReplacement) &#123;</span><br><span class="line">        new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191012123922980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">解释：</span><br><span class="line">	withReplacement：抽样的时候要不要放回去</span><br></pre></td></tr></table></figure></div>
<h2 id="（4）filter"><a href="#（4）filter" class="headerlink" title="（4）filter"></a>（4）filter</h2><p><img src="https://img-blog.csdnimg.cn/20191012124445377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 30).filter(_ &gt; 20).collect</span><br><span class="line">res5: Array[Int] = Array(21, 22, 23, 24, 25, 26, 27, 28, 29, 30)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(1 to 30).filter(x=&gt; x % 2 ==0 &amp;&amp; x &gt;10).collect</span><br><span class="line">res6: Array[Int] = Array(12, 14, 16, 18, 20, 22, 24, 26, 28, 30)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<h2 id="5-other类型的"><a href="#5-other类型的" class="headerlink" title="(5)other类型的"></a>(5)other类型的</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">union:  就是简单的合并  是不去重的哈 </span><br><span class="line">/**</span><br><span class="line">   * Return the union of this RDD and another one. Any identical elements will appear multiple</span><br><span class="line">   * times (use `.distinct()` to eliminate them).</span><br><span class="line">   */</span><br><span class="line">  def union(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    sc.union(this, other)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; a.union(b).collect</span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 5, 6, 4, 5, 6, 77, 7, 7)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">intersection:  交集</span><br><span class="line">  /**</span><br><span class="line">   * Return the intersection of this RDD and another one. The output will not contain any duplicate</span><br><span class="line">   * elements, even if the input RDDs did.</span><br><span class="line">   *</span><br><span class="line">   * @note This method performs a shuffle internally.</span><br><span class="line">   */</span><br><span class="line">  def intersection(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null)))</span><br><span class="line">        .filter &#123; case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;</span><br><span class="line">        .keys</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; a.intersection(b).collect</span><br><span class="line">res8: Array[Int] = Array(4, 6, 5)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">subtract:差集  出现在a里面的没有出现在b里面的 叫差集</span><br><span class="line">/**</span><br><span class="line">   * Return an RDD with the elements from `this` that are not in `other`.</span><br><span class="line">   *</span><br><span class="line">   * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting</span><br><span class="line">   * RDD will be &amp;lt;= us.</span><br><span class="line">   */</span><br><span class="line">  def subtract(other: RDD[T]): RDD[T] = withScope &#123;</span><br><span class="line">    subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val a = sc.parallelize(List(1,2,3,4,5,6))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt;     val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; a.subtract(b).collect</span><br><span class="line">res9: Array[Int] = Array(2, 1, 3)</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">去重：distinct</span><br><span class="line">	 /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(): RDD[T] = withScope &#123;</span><br><span class="line">    distinct(partitions.length)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; b.distinct.collect</span><br><span class="line">res10: Array[Int] = Array(4, 6, 77, 7, 5)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">去重这块 是可以传入分区参数的 ： 也可以没有的 没有的就是默认的分区</span><br><span class="line">你传进来多少分区就意味着 重新分区了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;val b = sc.parallelize(List(4,5,6,77,7,7))</span><br><span class="line">scala&gt;    b.distinct(4).mapPartitionsWithIndex((index,partition)=&gt;&#123;</span><br><span class="line">     |       partition.map(x=&gt; s&quot;分区是$index, 元素是 $x&quot;)</span><br><span class="line">     |     &#125;).collect()</span><br><span class="line">res11: Array[String] = Array(分区是0, 元素是 4, 分区是1, 元素是 77, 分区是1, 元素是 5, 分区是2, 元素是 6, 分区是3, 元素是 7)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">四个分区： 那是怎么分区的呢？  元素%partitions</span><br><span class="line">即元素对分区个数取模 来分的</span><br><span class="line"></span><br><span class="line">分区是0, 元素是 4,    4%4=0</span><br><span class="line">分区是1, 元素是 77, </span><br><span class="line">分区是1, 元素是 5,  5%4 =1</span><br><span class="line">分区是2, 元素是 6,  6%4=2</span><br><span class="line">分区是3, 元素是 7  7%4 = 3</span><br><span class="line"></span><br><span class="line">明白了吧  通过这个例子 知道是怎么进行分区的。</span><br></pre></td></tr></table></figure></div>
<h2 id="KV类型的"><a href="#KV类型的" class="headerlink" title="KV类型的"></a>KV类型的</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">groupKeyKey：不怎么用的哈</span><br><span class="line">把key相同的分到一组</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Group the values for each key in the RDD into a single sequence. Hash-partitions the</span><br><span class="line">   * resulting RDD with the existing partitioner/parallelism level. The ordering of elements</span><br><span class="line">   * within each group is not guaranteed, and may even differ each time the resulting RDD is</span><br><span class="line">   * evaluated.</span><br><span class="line">   *</span><br><span class="line">   * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">   */</span><br><span class="line">  def groupByKey(): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Group the values for each key in the RDD into a single sequence. Allows controlling the</span><br><span class="line">   * partitioning of the resulting key-value pair RDD by passing a Partitioner.</span><br><span class="line">   * The ordering of elements within each group is not guaranteed, and may even differ</span><br><span class="line">   * each time the resulting RDD is evaluated.</span><br><span class="line">   *</span><br><span class="line">   * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">   *</span><br><span class="line">   * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span><br><span class="line">   * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`.</span><br><span class="line">   */</span><br><span class="line">  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    // groupByKey shouldn&apos;t use map side combine because map side combine does not</span><br><span class="line">    // reduce the amount of data shuffled and requires all map side data be inserted</span><br><span class="line">    // into a hash table, leading to more objects in the old gen.</span><br><span class="line">    val createCombiner = (v: V) =&gt; CompactBuffer(v)</span><br><span class="line">    val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v</span><br><span class="line">    val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2</span><br><span class="line">    val bufs = combineByKeyWithClassTag[CompactBuffer[V]](</span><br><span class="line">      createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span><br><span class="line">    bufs.asInstanceOf[RDD[(K, Iterable[V])]]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey()</span><br><span class="line">res12: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[35] at groupByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().collect</span><br><span class="line">res13: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(2)), (a,CompactBuffer(1, 99)), (c,CompactBuffer(3)))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>接着上面的值 求相同的key的和 是多少使用什么算子呢？ 上面讲过了哈</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().collect</span><br><span class="line">res13: Array[(String, Iterable[Int])] = Array((b,CompactBuffer(2)), (a,CompactBuffer(1, 99)), (c,CompactBuffer(3)))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).groupByKey().mapValues(x=&gt;x.sum).collect</span><br><span class="line">res14: Array[(String, Int)] = Array((b,2), (a,100), (c,3))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey：</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).reduceByKey(_+_)</span><br><span class="line">res15: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at reduceByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,99))).reduceByKey(_+_).collect</span><br><span class="line">res16: Array[(String, Int)] = Array((b,2), (a,100), (c,3))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="distinct-底层"><a href="#distinct-底层" class="headerlink" title="distinct 底层"></a>distinct 底层</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">   * Return a new RDD containing the distinct elements in this RDD.</span><br><span class="line">   */</span><br><span class="line">  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">  &#125;</span><br><span class="line">注意：</span><br><span class="line">distinct底层是使用 map+reduceByKey 的 是不是很简单</span><br><span class="line">reduceByKey就把两两相同的东西 丢到一块去</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      * distinct 去重</span><br><span class="line">      * 不允许使用distinct做去重</span><br><span class="line">      *</span><br><span class="line">      * x =&gt; (x,null)</span><br><span class="line">      *</span><br><span class="line">      * 8 =&gt; (8,null)</span><br><span class="line">      * 8 =&gt; (8,null)</span><br><span class="line">      */</span><br><span class="line">    val b = sc.parallelize(List(3,4,5,6,7,8,8))</span><br><span class="line">    b.map(x =&gt; (x,null)).reduceByKey((x,y) =&gt; x).map(_._1)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.rdd.RDD[(Int, Null)] :一定要看清reduceByKey的数据结构哈</span><br><span class="line"></span><br><span class="line">scala&gt; val r1 = sc.parallelize(List(1,1,12,3,3,4,6,6,6))</span><br><span class="line">r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[47] at parallelize at &lt;console&gt;:24</span><br><span class="line">scala&gt; r1.map(x=&gt;(x,null)).reduceByKey((x,y)=&gt;x)</span><br><span class="line">res17: org.apache.spark.rdd.RDD[(Int, Null)] = ShuffledRDD[49] at reduceByKey at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">实际上到这一步 去重已经去掉了吧 明白吗   再把null去掉就可以了</span><br><span class="line"></span><br><span class="line">scala&gt; r1.map(x=&gt;(x,null)).reduceByKey((x,y)=&gt;x).map(_._1).collect</span><br><span class="line">res18: Array[Int] = Array(4, 6, 12, 1, 3)</span><br></pre></td></tr></table></figure></div>
<p>所以使用常用的算子一定要手点进去看看底层的实现哈 。</p>
<h2 id="个人理解-这个算子超重要"><a href="#个人理解-这个算子超重要" class="headerlink" title="个人理解 这个算子超重要"></a>个人理解 这个算子超重要</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">groupBy：自定义分组  分组条件就是自定义传进去的</span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">  * Return an RDD of grouped items. Each group consists of a key and a sequence of elements</span><br><span class="line">  * mapping to that key. The ordering of elements within each group is not guaranteed, and</span><br><span class="line">  * may even differ each time the resulting RDD is evaluated.</span><br><span class="line">  *</span><br><span class="line">  * @note This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">  * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span><br><span class="line">  * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span><br><span class="line">  */</span><br><span class="line"> def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)</span><br><span class="line">     : RDD[(K, Iterable[T])] = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   this.map(t =&gt; (cleanF(t), t)).groupByKey(p)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">参数是传入一个分组条件 怎么传呀？不要紧 可以测试</span><br><span class="line">1.传自己进去</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x)</span><br><span class="line">res19: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[55] at groupBy at &lt;console&gt;:25</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x).collect</span><br><span class="line">res20: Array[(String, Iterable[String])] = Array((b,CompactBuffer(b, b)), (a,CompactBuffer(a, a, a)), (c,CompactBuffer(c)))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">那我给一个需求：把上面的字母次数算出来</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x)</span><br><span class="line">res19: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[55] at groupBy at &lt;console&gt;:25</span><br><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;)).groupBy(x=&gt;x).mapValues(x=&gt;x.size).collect</span><br><span class="line">res21: Array[(String, Int)] = Array((b,2), (a,3), (c,1))</span><br><span class="line"></span><br><span class="line">注意：所以 算子都会用 怎么串起来 很重要的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sortBy：自定义排序   你想怎么排序就怎么排序  默认是升序的</span><br><span class="line">/**</span><br><span class="line">   * Return this RDD sorted by the given key function.</span><br><span class="line">   */</span><br><span class="line">  def sortBy[K](</span><br><span class="line">      f: (T) =&gt; K,</span><br><span class="line">      ascending: Boolean = true,</span><br><span class="line">      numPartitions: Int = this.partitions.length)</span><br><span class="line">      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope &#123;</span><br><span class="line">    this.keyBy[K](f)</span><br><span class="line">        .sortByKey(ascending, numPartitions)</span><br><span class="line">        .values</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2)</span><br><span class="line">res22: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[68] at sortBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2).collect</span><br><span class="line">res23: Array[(String, Int)] = Array((老哥,18), (double_happy,30), (娜娜,60))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(_._2,false).collect</span><br><span class="line">res24: Array[(String, Int)] = Array((娜娜,60), (double_happy,30), (老哥,18))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortBy(-_._2).collect</span><br><span class="line">res25: Array[(String, Int)] = Array((娜娜,60), (double_happy,30), (老哥,18))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sortBykey:是按key进行排序的哈 注意和sortby的区别   sortby是自定义排序 非常的灵活</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling</span><br><span class="line">   * `collect` or `save` on the resulting RDD will return or output an ordered list of records</span><br><span class="line">   * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in</span><br><span class="line">   * order of the keys).</span><br><span class="line">   */</span><br><span class="line">  // TODO: this currently doesn&apos;t work on P other than Tuple2!</span><br><span class="line">  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</span><br><span class="line">      : RDD[(K, V)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    val part = new RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">    new ShuffledRDD[K, V, V](self, part)</span><br><span class="line">      .setKeyOrdering(if (ascending) ordering else ordering.reverse)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).sortByKey().collect</span><br><span class="line">res29: Array[(String, Int)] = Array((double_happy,30), (娜娜,60), (老哥,18))</span><br><span class="line"></span><br><span class="line">如果要求 就是按年龄来排 应该怎么排序：</span><br><span class="line">  反转</span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).map(x=&gt;(x._2,x._1)).sortByKey().collect</span><br><span class="line">res30: Array[(Int, String)] = Array((18,老哥), (30,double_happy), (60,娜娜))</span><br><span class="line"></span><br><span class="line">数据是要和开始格式类似 再转回来就可以了：</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List((&quot;double_happy&quot;,30),(&quot;老哥&quot;,18),(&quot;娜娜&quot;,60))).map(x=&gt;(x._2,x._1)).sortByKey().map(x=&gt;(x._2,x._1)).collect</span><br><span class="line">res31: Array[(String, Int)] = Array((老哥,18), (double_happy,30), (娜娜,60))</span><br></pre></td></tr></table></figure></div>
<h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">join:  默认是内连接</span><br><span class="line">一定是需要条件的，条件就是key的</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each</span><br><span class="line">   * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and</span><br><span class="line">   * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD.</span><br><span class="line">   */</span><br><span class="line">  def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = self.withScope &#123;</span><br><span class="line">    this.cogroup(other, partitioner).flatMapValues( pair =&gt;</span><br><span class="line">      for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w)</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">第一个：名字  第二个：城市  第三个：年龄  (B,(上海,18))</span><br><span class="line"></span><br><span class="line">scala&gt; val j1 = sc.parallelize(List((&quot;A&quot;,&quot;北京&quot;),(&quot;B&quot;,&quot;上海&quot;),(&quot;C&quot;,&quot;杭州&quot;)))</span><br><span class="line">j1: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[106] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val j2 = sc.parallelize(List((&quot;A&quot;,&quot;30&quot;),(&quot;B&quot;,&quot;18&quot;),(&quot;D&quot;,&quot;60&quot;)))</span><br><span class="line">j2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[107] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;   j1.join(j2).collect</span><br><span class="line">res32: Array[(String, (String, String))] = Array((B,(上海,18)), (A,(北京,30)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.leftOuterJoin(j2).collect</span><br><span class="line">res33: Array[(String, (String, Option[String]))] = Array((B,(上海,Some(18))), (A,(北京,Some(30))), (C,(杭州,None)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.rightOuterJoin(j2).collect</span><br><span class="line">res34: Array[(String, (Option[String], String))] = Array((B,(Some(上海),18)), (D,(None,60)), (A,(Some(北京),30)))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      * join底层就是使用了cogroup</span><br><span class="line">      * RDD[K,V]</span><br><span class="line">      *</span><br><span class="line">      * 根据key进行关联，返回两边RDD的记录，没关联上的是空</span><br><span class="line">      * join返回值类型  RDD[(K, (Option[V], Option[W]))]      这块参数的类型是option要注意 </span><br><span class="line">      * cogroup返回值类型  RDD[(K, (Iterable[V], Iterable[W]))]</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cogroup:</span><br><span class="line"> /**</span><br><span class="line">   * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the</span><br><span class="line">   * list of values for that key in `this` as well as `other`.</span><br><span class="line">   */</span><br><span class="line">  def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner)</span><br><span class="line">      : RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope &#123;</span><br><span class="line">    if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) &#123;</span><br><span class="line">      throw new SparkException(&quot;HashPartitioner cannot partition array keys.&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val cg = new CoGroupedRDD[K](Seq(self, other), partitioner)</span><br><span class="line">    cg.mapValues &#123; case Array(vs, w1s) =&gt;</span><br><span class="line">      (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]])</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;     j1.fullOuterJoin(j2).collect</span><br><span class="line">res35: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(上海),Some(18))), (D,(None,Some(60))), (A,(Some(北京),Some(30))), (C,(Some(杭州),None)))</span><br><span class="line"></span><br><span class="line">scala&gt;     j1.cogroup(j2).collect</span><br><span class="line">res36: Array[(String, (Iterable[String], Iterable[String]))] = Array((B,(CompactBuffer(上海),CompactBuffer(18))), (D,(CompactBuffer(),CompactBuffer(60))), (A,(CompactBuffer(北京),CompactBuffer(30))), (C,(CompactBuffer(杭州),CompactBuffer())))</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark003-Action-接着Spark002" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/19/Spark003-Action-%E6%8E%A5%E7%9D%80Spark002/">Spark003--Action  接着Spark002</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/19/Spark003-Action-%E6%8E%A5%E7%9D%80Spark002/" class="article-date">
  <time datetime="2018-01-19T11:59:54.000Z" itemprop="datePublished">2018-01-19</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="回顾上篇文章："><a href="#回顾上篇文章：" class="headerlink" title="回顾上篇文章："></a>回顾上篇文章：</h2><p>RDD:<br>    是什么<br>    五大特性对应五大方法<br>    创建方式：3<br>    操作：2 action &amp; transformation</p>
<p>Spark作业开发流程：<br><img src="https://img-blog.csdnimg.cn/20191022080418772.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>也就是：<br>数据源–&gt;经过一堆transformtion–&gt;action 触发spark作业 —&gt;输出到某个地方</p>
<p>你的业务无论多么复杂 都是这样的。</p>
<h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p>（1）collect</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Return an array that contains all of the elements in this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">   * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">   */</span><br><span class="line">  def collect(): Array[T] = withScope &#123;</span><br><span class="line">    val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)</span><br><span class="line">    Array.concat(results: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.Return an array that contains all of the elements in this RDD.</span><br><span class="line">2.resulting array is expected to be small</span><br><span class="line">3. the data is loaded into the driver&apos;s memory.</span><br><span class="line">所以生产上你想看这个rdd里的数据 是不太现实的 会导致某种oom的，(oom有好多种的)</span><br><span class="line">如果你还是想看rdd里的元素 该怎么办呢？</span><br><span class="line">两种方法：</span><br><span class="line">1) 取出部分数据</span><br><span class="line">2) 把rdd输出到文件系统</span><br><span class="line">真正生产上使用collect只有一个地方：？？？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure></div>
<p>(2)foreach<br><img src="https://img-blog.csdnimg.cn/20191022081833635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Applies a function f to all elements of this RDD.</span><br><span class="line">  */</span><br><span class="line"> def foreach(f: T =&gt; Unit): Unit = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreach(println)</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>注意：<br>我在spark-shell  –master local[2] 模式下 rdd.foreach(println) 会显示出结果，如果在<br>spark-shell  –master yarn 模式下 rdd.foreach(println) 会显示出结果么？为什么呢？</p>
<p>(3)foreachPartition<br><img src="https://img-blog.csdnimg.cn/20191022082000435.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Applies a function f to each partition of this RDD.</span><br><span class="line">  */</span><br><span class="line"> def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope &#123;</span><br><span class="line">   val cleanF = sc.clean(f)</span><br><span class="line">   sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreachPartition(println)</span><br><span class="line">non-empty iterator</span><br><span class="line">non-empty iterator</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.partitions.size</span><br><span class="line">res5: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">返回的是non-empty iterator 怎么才能把里面的内容输出出来呢？</span><br><span class="line"></span><br><span class="line">如果这样写呢？</span><br><span class="line"> rdd.foreachPartition(paritition =&gt; paritition.map(println))</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreachPartition(paritition =&gt; paritition.map(println))</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>能不能想到这是什么问题导致的？<br>foreachPartition(paritition =&gt; paritition.map(println)) 输出结果在正在执行的机器上面是有的<br>而控制台看到的是driver的 </p>
<p>正好引入一个东西：<br>sortBy 上次的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2.sortBy(_._2,false)</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">sortBy是全局排序的还是分区排序的？</span><br><span class="line"></span><br><span class="line">上面的两行代码看仔细了 ， 是两个分区 ,按照降序排</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">同样的代码我再运行一次：</span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[12] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>sortBy是全局排序的还是分区排序的？通过上面的测试知道了吗？ 知道个鬼<br>是不是感觉是分区排序</p>
<p>去idea上输出结果看一下：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark02</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line"></span><br><span class="line">object ActionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd2 = sc.parallelize(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4)),2)</span><br><span class="line">    rdd2.sortBy(_._2,false).saveAsTextFile(&quot;file:///Users/double_happy/zz/G7-03/工程/scala-spark/doc/out&quot;)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191022085445570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191022085458568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>难道真的是分区排序么？在进行测试。<br><img src="https://img-blog.csdnimg.cn/20191022085757146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.sortBy(_._2,false).foreach(println)</span><br><span class="line">(d,4)</span><br><span class="line">(c,3)</span><br><span class="line">(b,2)</span><br><span class="line">(a,1)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>为什么rdd2.sortBy(<em>.</em>2,false).foreach(println)的结果不一样？<br>所以使用foreach在这里根本看不出来sortBy是全局排序还是分区排序</p>
<p><strong>因为 rdd2是两个分区的 ，foreach执行的时候 不确定是哪个task先println 出来 明白吗？</strong></p>
<p><strong>所以sortBy 到底是什么排序？</strong><br>全局排序      你看idea里的 </p>
<p>所以你测试的时候 sortBy 后面不能跟着 foreach 来测试 要输出文件</p>
<p>通过 读取文件  来测试 </p>
<p>(3)count</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Return the number of elements in the RDD.</span><br><span class="line"> */</span><br><span class="line">def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(4) reduce   两两做操作</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce(_+_)</span><br><span class="line">res6: Int = 15</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(5) first</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return the first element in this RDD.</span><br><span class="line">  */</span><br><span class="line"> def first(): T = withScope &#123;</span><br><span class="line">   take(1) match &#123;</span><br><span class="line">     case Array(t) =&gt; t</span><br><span class="line">     case _ =&gt; throw new UnsupportedOperationException(&quot;empty collection&quot;)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>(6)take</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Take the first num elements of the RDD. It works by first scanning one partition, and use the</span><br><span class="line">  * results from that partition to estimate the number of additional partitions needed to satisfy</span><br><span class="line">  * the limit.</span><br><span class="line">  *</span><br><span class="line">  * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">  * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">  *</span><br><span class="line">  * @note Due to complications in the internal implementation, this method will raise</span><br><span class="line">  * an exception if called on an RDD of `Nothing` or `Null`.</span><br><span class="line">  */</span><br><span class="line"> def take(num: Int): Array[T] = withScope &#123;</span><br><span class="line">   val scaleUpFactor = Math.max(conf.getInt(&quot;spark.rdd.limit.scaleUpFactor&quot;, 4), 2)</span><br><span class="line">   if (num == 0) &#123;</span><br><span class="line">     new Array[T](0)</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     val buf = new ArrayBuffer[T]</span><br><span class="line">     val totalParts = this.partitions.length</span><br><span class="line">     var partsScanned = 0</span><br><span class="line">     while (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123;</span><br><span class="line">       // The number of partitions to try in this iteration. It is ok for this number to be</span><br><span class="line">       // greater than totalParts because we actually cap it at totalParts in runJob.</span><br><span class="line">       var numPartsToTry = 1L</span><br><span class="line">       val left = num - buf.size</span><br><span class="line">       if (partsScanned &gt; 0) &#123;</span><br><span class="line">         // If we didn&apos;t find any rows after the previous iteration, quadruple and retry.</span><br><span class="line">         // Otherwise, interpolate the number of partitions we need to try, but overestimate</span><br><span class="line">         // it by 50%. We also cap the estimation in the end.</span><br><span class="line">         if (buf.isEmpty) &#123;</span><br><span class="line">           numPartsToTry = partsScanned * scaleUpFactor</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">           // As left &gt; 0, numPartsToTry is always &gt;= 1</span><br><span class="line">           numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt</span><br><span class="line">           numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)</span><br><span class="line">       val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p)</span><br><span class="line"></span><br><span class="line">       res.foreach(buf ++= _.take(num - buf.size))</span><br><span class="line">       partsScanned += p.size</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     buf.toArray</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>first底层调用take方法</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res5: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce(_+_)</span><br><span class="line">res6: Int = 15</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.first</span><br><span class="line">res7: Int = 1</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(2)</span><br><span class="line">res8: Array[Int] = Array(1, 2)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(7) top<br>里面肯定是做了排序的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Returns the top k (largest) elements from this RDD as defined by the specified</span><br><span class="line">   * implicit Ordering[T] and maintains the ordering. This does the opposite of</span><br><span class="line">   * [[takeOrdered]]. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)</span><br><span class="line">   *   // returns Array(12)</span><br><span class="line">   *</span><br><span class="line">   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)</span><br><span class="line">   *   // returns Array(6, 5)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @note This method should only be used if the resulting array is expected to be small, as</span><br><span class="line">   * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">   *</span><br><span class="line">   * @param num k, the number of top elements to return</span><br><span class="line">   * @param ord the implicit ordering for T</span><br><span class="line">   * @return an array of top elements</span><br><span class="line">   */</span><br><span class="line">  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123;</span><br><span class="line">    takeOrdered(num)(ord.reverse)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1. This does the opposite of</span><br><span class="line">   * [[takeOrdered]].</span><br><span class="line"></span><br><span class="line">2.top 底层调用的是 takeOrdered</span><br><span class="line"></span><br><span class="line">3.top 柯里化的 Ordering 看scala篇这部分 讲的很详细</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.top(2)</span><br><span class="line">res9: Array[Int] = Array(5, 4)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.takeOrdered(2)</span><br><span class="line">res10: Array[Int] = Array(1, 2)</span><br></pre></td></tr></table></figure></div>

<p>(8)zipWithIndex</p>
<p>给你一个算子 你怎么知道他是 action还是 transformation？？</p>
<p><strong>action算子里面是有sc.runJob()方法的</strong>  </p>
<p>eg：<br><img src="https://img-blog.csdnimg.cn/20191022092526573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>所以zipWithIndex 它不是action算子</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Zips this RDD with its element indices. The ordering is first based on the partition index</span><br><span class="line">  * and then the ordering of items within each partition. So the first item in the first</span><br><span class="line">  * partition gets index 0, and the last item in the last partition receives the largest index.</span><br><span class="line">  *</span><br><span class="line">  * This is similar to Scala&apos;s zipWithIndex but it uses Long instead of Int as the index type.</span><br><span class="line">  * This method needs to trigger a spark job when this RDD contains more than one partitions.</span><br><span class="line">  *</span><br><span class="line">  * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of</span><br><span class="line">  * elements in a partition. The index assigned to each element is therefore not guaranteed,</span><br><span class="line">  * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee</span><br><span class="line">  * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.</span><br><span class="line">  */</span><br><span class="line"> def zipWithIndex(): RDD[(T, Long)] = withScope &#123;</span><br><span class="line">   new ZippedWithIndexRDD(this)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex</span><br><span class="line">res11: org.apache.spark.rdd.RDD[(Int, Long)] = ZippedWithIndexRDD[31] at zipWithIndex at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex.collect</span><br><span class="line">res12: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4))</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p>(9)countByKey</p>
<p>这是action算子</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Count the number of elements for each key, collecting the results to a local Map.</span><br><span class="line">  *</span><br><span class="line">  * @note This method should only be used if the resulting map is expected to be small, as</span><br><span class="line">  * the whole thing is loaded into the driver&apos;s memory.</span><br><span class="line">  * To handle very large results, consider using rdd.mapValues(_ =&gt; 1L).reduceByKey(_ + _), which</span><br><span class="line">  * returns an RDD[T, Long] instead of a map.</span><br><span class="line">  */</span><br><span class="line"> def countByKey(): Map[K, Long] = self.withScope &#123;</span><br><span class="line">   self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>
<p>(10)collectAsMap  针对kv类型的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return the key-value pairs in this RDD to the master as a Map.</span><br><span class="line">  *</span><br><span class="line">  * Warning: this doesn&apos;t return a multimap (so if you have multiple values to the same key, only</span><br><span class="line">  *          one value per key is preserved in the map returned)</span><br><span class="line">  *</span><br><span class="line">  * @note this method should only be used if the resulting data is expected to be small, as</span><br><span class="line">  * all the data is loaded into the driver&apos;s memory.</span><br><span class="line">  */</span><br><span class="line"> def collectAsMap(): Map[K, V] = self.withScope &#123;</span><br><span class="line">   val data = self.collect()</span><br><span class="line">   val map = new mutable.HashMap[K, V]</span><br><span class="line">   map.sizeHint(data.length)</span><br><span class="line">   data.foreach &#123; pair =&gt; map.put(pair._1, pair._2) &#125;</span><br><span class="line">   map</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex.collect</span><br><span class="line">res12: Array[(Int, Long)] = Array((1,0), (2,1), (3,2), (4,3), (5,4))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex().countByKey()</span><br><span class="line">res13: scala.collection.Map[Int,Long] = Map(5 -&gt; 1, 1 -&gt; 1, 2 -&gt; 1, 3 -&gt; 1, 4 -&gt; 1)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.zipWithIndex().collectAsMap()</span><br><span class="line">res14: scala.collection.Map[Int,Long] = Map(2 -&gt; 1, 5 -&gt; 4, 4 -&gt; 3, 1 -&gt; 0, 3 -&gt; 2)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<p>Action算子官网：<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions" target="_blank" rel="noopener">Action 算子</a></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark001-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/15/Spark001-double-happy/">Spark001--double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/15/Spark001-double-happy/" class="article-date">
  <time datetime="2018-01-15T11:57:42.000Z" itemprop="datePublished">2018-01-15</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="Speed"><a href="#Speed" class="headerlink" title="Speed"></a>Speed</h2><p>Spark是支持pipline操作的，根据Shufle进行切分的，中间的过程是不落地的。<br>运行的角度来说：<br>    线程的<br>    mapreduce是进程的   map task 、reduce task</p>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>1.Represents an immutable,<br>   partitioned collection of elements that can be operated on in parallel.<br>2. </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">5大特性： 弹性分布式数据集</span><br><span class="line">	1）一系列的partition     分区里是有index的</span><br><span class="line">      protected def getPartitions: Array[Partition]</span><br><span class="line"></span><br><span class="line">解释：</span><br><span class="line">	 *  - A list of partitions</span><br><span class="line">	 *  - A function for computing each split</span><br><span class="line"> scala中 List（1，2，3，4).map(_*2)   scala中的list是单机的</span><br><span class="line"> 而RDD中 数据是分区的，如果rdd.map(_*2)是对每个分区里的元素做计算 是 分布式的</span><br><span class="line"></span><br><span class="line">    2）针对RDD做操作其实就是针对RDD底层的partition进行操作</span><br><span class="line">    rdd.map(_*2)</span><br><span class="line">    def compute(split: Partition, context: TaskContext): Iterator[T]</span><br><span class="line"></span><br><span class="line">	3）rdd之间的依赖（血缘关系）</span><br><span class="line">      protected def getDependencies: Seq[Dependency[_]] = deps</span><br><span class="line"></span><br><span class="line">	4）partitioner（针对 kv类型的rdd）</span><br><span class="line">      @transient val partitioner: Option[Partitioner] = None</span><br><span class="line"></span><br><span class="line">	5）locations （优先把作业调度到数据所在节点）</span><br><span class="line">      protected def getPreferredLocations(split: Partition): Seq[String] = Nil</span><br><span class="line">	好处是 如果你的数据不在这个节点上 优先把作业调度到数据所在节点 好处是 直接本地读数据就可以了</span><br><span class="line">	理想化状态。</span><br><span class="line">	 也有 作业调度在别的节点上 数据在另一台节点上，那么 只能把数据通过网络把数据传到 作业调度的节点上去，进行计算。那么5这个特性就是减少网络数据传输。</span><br></pre></td></tr></table></figure></div>
<h2 id="程序开发入口"><a href="#程序开发入口" class="headerlink" title="程序开发入口"></a>程序开发入口</h2><p>开发Spark应用程序<br>    1）SparkConf<br>        appName<br>        master<br>    2）SparkContext(sparkConf)<br>    3）spark-shell –master local[2] 底层自动为我们创建了SparkContext sc</p>
<p><img src="https://img-blog.csdnimg.cn/2019092812275919.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190928122827724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RDD:创建</span><br><span class="line"> parallelize :</span><br><span class="line"> 	sc.parallelize(List(1,2,3,4))</span><br><span class="line"> textFile:</span><br><span class="line"> 	sc.textFile(path)</span><br><span class="line"> 通过RDD转换生成的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List(1,2,3,4))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">collect   collectAsync</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3, 4)                                            </span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1 = sc.textFile(&quot;file:///home/sxwang/data&quot;)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[String] = file:///home/sxwang/data MapPartitionsRDD[2] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.collect</span><br><span class="line">res1: Array[String] = Array(spark       flink   hadoop, spark   kafka   scala)</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = rdd.map(_*2)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at map at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">collect   collectAsync</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res2: Array[Int] = Array(2, 4, 6, 8)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.toDebugString</span><br><span class="line">res3: String =</span><br><span class="line">(2) MapPartitionsRDD[3] at map at &lt;console&gt;:26 []</span><br><span class="line"> |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br></pre></td></tr></table></figure></div>

<h2 id="并行度-–简单版"><a href="#并行度-–简单版" class="headerlink" title="并行度 –简单版"></a>并行度 –简单版</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd</span><br><span class="line">res7: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res8: Array[Int] = Array(1, 2, 3, 4)</span><br></pre></td></tr></table></figure></div>
<p>产看ui界面：<br><img src="https://img-blog.csdnimg.cn/20190928124416679.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为什么是2呢？<br>查看源码：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def parallelize[T: ClassTag](</span><br><span class="line">    seq: Seq[T],</span><br><span class="line">    numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">defaultParallelism：</span><br><span class="line">	</span><br><span class="line">	 /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */</span><br><span class="line">  def defaultParallelism: Int = &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    taskScheduler.defaultParallelism</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p>看他的实现的<br><img src="https://img-blog.csdnimg.cn/20190928124821226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">taskScheduler.defaultParallelism：</span><br><span class="line">	override def defaultParallelism(): Int = backend.defaultParallelism()</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019092812500940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">查看local的点进去：</span><br><span class="line">  override def defaultParallelism(): Int =</span><br><span class="line">    scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)</span><br><span class="line">默认从配置文件里去  ，但是我们没有设置，</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Used when running a local version of Spark where the executor, backend, and master all run in</span><br><span class="line"> * the same JVM. It sits behind a [[TaskSchedulerImpl]] and handles launching tasks on a single</span><br><span class="line"> * Executor (created by the [[LocalSchedulerBackend]]) running locally.</span><br><span class="line"> */</span><br><span class="line">private[spark] class LocalSchedulerBackend(</span><br><span class="line">    conf: SparkConf,</span><br><span class="line">    scheduler: TaskSchedulerImpl,</span><br><span class="line">    val totalCores: Int)</span><br><span class="line">  extends SchedulerBackend with ExecutorBackend with Logging &#123;</span><br><span class="line"></span><br><span class="line">totalCores 说明是我们构建的时候传进来的 </span><br><span class="line"></span><br><span class="line">可以看：</span><br><span class="line">	scala&gt; sc.parallelize(List(1,2,3,4),3).collect</span><br><span class="line">res9: Array[Int] = Array(1, 2, 3, 4)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20190928125348377.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
  </div>
  
  
</article>

  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/">Next &amp;raquo;</a>
  </nav>


</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/01/05/Kudu-Impala%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B01-double-happy/">Kudu+Impala故障案例01--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink04-double-happy/">Flink04--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink03-double-happy/">Flink03--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink02-double-happy/">Flink02--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink01-double-happy/">Flink01-double_happy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>