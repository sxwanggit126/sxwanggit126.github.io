<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>DoubleHappy or Jepson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="DoubleHappy or Jepson">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;2&#x2F;index.html">
<meta property="og:site_name" content="DoubleHappy or Jepson">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="DoubleHappy or Jepson" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/archives" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main">
  
    <article id="post-雅恩资源调优-double-happy" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/" class="article-date">
  <time datetime="2018-03-17T12:13:41.000Z" itemprop="datePublished">2018-03-17</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一台机器能运行多少个container 到底是由谁决定的 ？</span><br></pre></td></tr></table></figure></div>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">生产上一台机器：</span><br><span class="line">	48G物理内存  8core--》16vcore</span><br><span class="line">	Linux系统本身要占内存+空留:  20% =9.6G</span><br><span class="line">	剩余: 80% =38.4G=38G    这些留给大数据组件</span><br><span class="line">	</span><br><span class="line">DN进程: 生产4G  </span><br><span class="line">	1000m</span><br><span class="line">	hadoop-env.sh</span><br><span class="line">	HADOOP_NAMENODE_OPTS=-Xmx1024m</span><br><span class="line">	HADOOP_DATANODE_OPTS=-Xmx1024m</span><br><span class="line">	</span><br><span class="line">NM进程: 生产4G</span><br><span class="line">	yarn-env.sh</span><br><span class="line">	export YARN_RESOURCEMANAGER_HEAPSIZE=1024</span><br><span class="line">	export YARN_NODEMANAGER_HEAPSIZE=1024</span><br><span class="line">NM 与DN 部署在同一台机器上： 数据本地化</span><br><span class="line"></span><br><span class="line">NN RM 经常性部署同一台  说白了 集群节点少</span><br><span class="line"></span><br><span class="line">*****</span><br><span class="line">资源内存: 38G-4-4=30G   这就是运行container容器 的 </span><br><span class="line">yarn.nodemanager.resource.memory-mb   30*1024   总的  </span><br><span class="line">默认配置</span><br><span class="line">yarn.scheduler.minimum-allocation-mb  1024     给容器最小的 内存     生产上 2g</span><br><span class="line">yarn.scheduler.maximum-allocation-mb  8192   给容器最大的 内存       生产上 30g</span><br><span class="line"></span><br><span class="line">按照官网默认的算：</span><br><span class="line">30G 30G/1G=30个container </span><br><span class="line">30G 30/8G=3个container ...6G    这里面 30这个值不好  30/8   还剩6g  32g 比较好 </span><br><span class="line">30个~3个</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191108162536413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>内存：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">生产一: </span><br><span class="line">yarn.nodemanager.resource.memory-mb   30G</span><br><span class="line">yarn.scheduler.minimum-allocation-mb  2G</span><br><span class="line">yarn.scheduler.maximum-allocation-mb  30G</span><br><span class="line"></span><br><span class="line">2g--》  yarn给你分配的时候先给你最小的  当计算过程中发现内存不够了 yarn会给你长一个g</span><br><span class="line"></span><br><span class="line">15个~1个</span><br><span class="line"></span><br><span class="line">30G 是不是太大了 根据你作业来分的  你作业就是需要30g 你只能给30g</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">生产二：</span><br><span class="line">yarn.nodemanager.resource.memory-mb   32G</span><br><span class="line">yarn.scheduler.minimum-allocation-mb  2G</span><br><span class="line">yarn.scheduler.maximum-allocation-mb  8G</span><br><span class="line"></span><br><span class="line">16c~4c</span><br><span class="line"></span><br><span class="line">如果container  memory oom      那么调大yarn.scheduler.maximum-allocation-mb 这个 先把oom这个进程kill掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">生产三:</span><br><span class="line">256G:</span><br><span class="line"></span><br><span class="line">yarn.nodemanager.resource.memory-mb   168G</span><br><span class="line">yarn.scheduler.minimum-allocation-mb  4G</span><br><span class="line">yarn.scheduler.maximum-allocation-mb  24G</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">container p memory oom kill</span><br><span class="line"></span><br><span class="line">生产默认 </span><br><span class="line">yarn.nodemanager.pmem-check-enabled	true       //物理内存</span><br><span class="line">yarn.nodemanager.vmem-check-enabled	true      </span><br><span class="line">yarn.nodemanager.vmem-pmem-ratio	2.1     //物理内存 和虚拟内存的比例 </span><br><span class="line"></span><br><span class="line">物理内存 1m  虚拟内存 2.1m</span><br><span class="line"></span><br><span class="line">这两个内存超了 都会抱 oom</span><br></pre></td></tr></table></figure></div>
<p><strong>cpu</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">CPU:</span><br><span class="line">yarn.nodemanager.resource.cpu-vcores	 12</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores  1</span><br><span class="line">yarn.scheduler.maximum-allocation-vcores  4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">container: </span><br><span class="line">	memory 16c~4c</span><br><span class="line">	vcores 12c~3c </span><br><span class="line"></span><br><span class="line">所以这个两个变量</span><br><span class="line">cpu 和 mem  你应该怎么调 才能 资源最大化呢?  </span><br><span class="line">	到底是根据mem 来算 还是 core来算呢？ 已加密</span><br></pre></td></tr></table></figure></div>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SS04" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/22/SS04/">SS04</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/22/SS04/" class="article-date">
  <time datetime="2018-02-22T12:12:50.000Z" itemprop="datePublished">2018-02-22</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">之前的ss程序都是运行在idea</span><br><span class="line">那么如何提交到服务器上运行呢？</span><br><span class="line">  演示：</span><br><span class="line">  一步一步来  先不管理offset 把代码提交到yarn上 把wc统计出来</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">数据从Kafka过来然后 ss消费到 把wc统计出来：</span><br><span class="line"></span><br><span class="line">object StreamingKakfaDirectYarnApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    //参数从外面传进 来    topics groupId brokers</span><br><span class="line">    if(args.size != 3)&#123;</span><br><span class="line">      System.err.println(&quot;Usage:StreamingKakfaDirectYarnApp &lt;brokers&gt; &lt;topic&gt; &lt;groupId&gt;&quot;)</span><br><span class="line">      System.exit(-1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val Array(brokers,topic,groupId) = args</span><br><span class="line">    </span><br><span class="line">    val sparkConf: SparkConf = new SparkConf()</span><br><span class="line">    val ssc =new StreamingContext(sparkConf,Seconds(10))</span><br><span class="line">   // val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; brokers, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">idea测试结果：</span><br><span class="line">Usage:StreamingKakfaDirectYarnApp &lt;brokers&gt; &lt;topic&gt; &lt;groupId&gt;</span><br><span class="line"></span><br><span class="line">注意：idea里怎么把参数传进去呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103144003315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191103144048102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">运行结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572763410000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,19)</span><br><span class="line">(b,18)</span><br><span class="line">(f,21)</span><br><span class="line">(e,17)</span><br><span class="line">(a,24)</span><br><span class="line">(c,21)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572763420000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572763430000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">说明本地改造完成 那么我们打包上传到服务器上运行</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">提交命令：</span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; --name StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">19/11/03 15:08:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/11/03 15:08:42 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/11/03 15:08:43 INFO SparkContext: Submitted application: StreamingKakfaDirectYarnApp</span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/11/03 15:08:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/11/03 15:08:43 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 40978.</span><br><span class="line">19/11/03 15:08:43 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/11/03 15:08:43 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/11/03 15:08:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d01d319f-1fe4-4025-bcf4-418a06809ccc</span><br><span class="line">19/11/03 15:08:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/11/03 15:08:43 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/11/03 15:08:43 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/11/03 15:08:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/11/03 15:08:43 INFO SparkContext: Added JAR file:/home/double_happy/lib/spark-core-1.0.jar at spark://hadoop101:40978/jars/spark-core-1.0.jar with timestamp 1572764923743</span><br><span class="line">19/11/03 15:08:43 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/11/03 15:08:43 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 33748.</span><br><span class="line">19/11/03 15:08:43 INFO NettyBlockTransferService: Server created on hadoop101:33748</span><br><span class="line">19/11/03 15:08:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:33748 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 33748, None)</span><br><span class="line">19/11/03 15:08:45 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572764923782</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp$.main(StreamingKakfaDirectYarnApp.scala:36)</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp.main(StreamingKakfaDirectYarnApp.scala)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 14 more</span><br><span class="line">19/11/03 15:08:45 INFO SparkContext: Invoking stop() from shutdown hook</span><br><span class="line">19/11/03 15:08:45 INFO SparkUI: Stopped Spark web UI at http://hadoop101:4040</span><br><span class="line">19/11/03 15:08:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">19/11/03 15:08:45 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">19/11/03 15:08:45 INFO BlockManager: BlockManager stopped</span><br><span class="line">19/11/03 15:08:46 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">19/11/03 15:08:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">19/11/03 15:08:46 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">19/11/03 15:08:46 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">19/11/03 15:08:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-6ffb645c-d7fd-44e8-b0e5-256cae7b11ea</span><br><span class="line">19/11/03 15:08:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a31f335-3d75-4c59-b7a5-c5bf023d1265</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line">为什么呢？ 在idea里都可以的 </span><br><span class="line">StringDeserializer 类是在</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">这个包里面的</span><br><span class="line"></span><br><span class="line">而这个包Spark本身是没有的 是我们额外加进来的喽 </span><br><span class="line">那么这个包没有在服务器上 为什么该怎么办呢？ 看看官网怎么说的</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#deploying" target="_blank" rel="noopener">Deploying</a>：部署<br>As with any Spark applications, spark-submit is used to launch your application.</p>
<p>For Scala and Java applications, if you are using SBT or Maven for project management, <strong>then package spark-streaming-kafka-0-10_2.12 and its dependencies into the application JAR.</strong> <strong>Make sure spark-core_2.12 and spark-streaming_2.12 are marked as provided dependencies as those are already present in a Spark installation.</strong> Then use spark-submit to launch your application (see Deploying section in the main programming guide).<br>这种方式不好 换一个</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">因为需要把这个spark-streaming-kafka-0-10_2.11包 传到服务器上</span><br><span class="line">./spark-submit --help  查查 可以加maven 的依赖  怎么加呢？</span><br><span class="line"></span><br><span class="line"> --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line"></span><br><span class="line">这个参数 可以指向 maven的一些jar包 **** </span><br><span class="line">修改提交命令：</span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line"> --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.4 \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line"></span><br><span class="line">但是这个东西 需要联网 不能联网是不行的 一会看日志就清除了 它需要联网去下载 maven依赖</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --master local[2] --name StreamingKakfaDirectYarnApp  --packageg.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.4 --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp /home/double_happy/lib/spark-core-1.0.jar hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">Ivy Default Cache set to: /home/double_happy/.ivy2/cache</span><br><span class="line">The jars for the packages stored in: /home/double_happy/.ivy2/jars</span><br><span class="line">:: loading settings :: url = jar:file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml</span><br><span class="line">org.apache.spark#spark-streaming-kafka-0-10_2.11 added as a dependency</span><br><span class="line">:: resolving dependencies :: org.apache.spark#spark-submit-parent-c0781f34-3101-4762-8694-9aa38b463184;1.0</span><br><span class="line">        confs: [default]</span><br><span class="line">        found org.apache.spark#spark-streaming-kafka-0-10_2.11;2.4.4 in central</span><br><span class="line">        found org.apache.kafka#kafka-clients;2.0.0 in central</span><br><span class="line">        found org.lz4#lz4-java;1.4.0 in central</span><br><span class="line">        found org.xerial.snappy#snappy-java;1.1.7.3 in central</span><br><span class="line">        found org.slf4j#slf4j-api;1.7.16 in central</span><br><span class="line">        found org.spark-project.spark#unused;1.0.0 in central</span><br><span class="line">:: resolution report :: resolve 496ms :: artifacts dl 9ms</span><br><span class="line">        :: modules in use:</span><br><span class="line">        org.apache.kafka#kafka-clients;2.0.0 from central in [default]</span><br><span class="line">        org.apache.spark#spark-streaming-kafka-0-10_2.11;2.4.4 from central in [default]</span><br><span class="line">        org.lz4#lz4-java;1.4.0 from central in [default]</span><br><span class="line">        org.slf4j#slf4j-api;1.7.16 from central in [default]</span><br><span class="line">        org.spark-project.spark#unused;1.0.0 from central in [default]</span><br><span class="line">        org.xerial.snappy#snappy-java;1.1.7.3 from central in [default]</span><br><span class="line">        ---------------------------------------------------------------------</span><br><span class="line">        |                  |            modules            ||   artifacts   |</span><br><span class="line">        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|</span><br><span class="line">        ---------------------------------------------------------------------</span><br><span class="line">        |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |</span><br><span class="line">        ---------------------------------------------------------------------</span><br><span class="line">:: retrieving :: org.apache.spark#spark-submit-parent-c0781f34-3101-4762-8694-9aa38b463184</span><br><span class="line">        confs: [default]</span><br><span class="line">        0 artifacts copied, 6 already retrieved (0kB/10ms)</span><br><span class="line"></span><br><span class="line">我截取了一小部分日志 你看 第一次需要下载maven依赖的  </span><br><span class="line">所以这个 参数也有弊端的  (毕竟公司的服务器是不可能连接外网的  )</span><br><span class="line"></span><br><span class="line">还有其他的方式可以解决 一会介绍</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103152748544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="结果"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">那么刚刚packages  有小问题  那么怎么办呢 ？</span><br><span class="line"></span><br><span class="line">1.先把spark-streaming-kafka-0-10_2.11依赖包 上传到服务器上 </span><br><span class="line">2.通过--jars 来指定</span><br><span class="line"></span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line">--jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; --name StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; --jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar \</span><br><span class="line">&gt; --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">19/11/03 15:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Submitted application: StreamingKakfaDirectYarnApp</span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/11/03 15:38:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/11/03 15:38:39 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 45422.</span><br><span class="line">19/11/03 15:38:39 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/11/03 15:38:39 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/11/03 15:38:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/11/03 15:38:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/11/03 15:38:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ed285e0b-aab6-4fa6-a09d-6776f02d7a71</span><br><span class="line">19/11/03 15:38:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/11/03 15:38:39 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/11/03 15:38:39 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/11/03 15:38:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Added JAR file:///home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar at spark://hadoop101:45422/jars/spark-streaming-kafka-0-10_2.11-2.4.4.jar with timestamp 1572766719888</span><br><span class="line">19/11/03 15:38:39 INFO SparkContext: Added JAR file:/home/double_happy/lib/spark-core-1.0.jar at spark://hadoop101:45422/jars/spark-core-1.0.jar with timestamp 1572766719889</span><br><span class="line">19/11/03 15:38:39 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/11/03 15:38:40 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 40256.</span><br><span class="line">19/11/03 15:38:40 INFO NettyBlockTransferService: Server created on hadoop101:40256</span><br><span class="line">19/11/03 15:38:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/11/03 15:38:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:40256 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 40256, None)</span><br><span class="line">19/11/03 15:38:40 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572766719940</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp$.main(StreamingKakfaDirectYarnApp.scala:36)</span><br><span class="line">        at com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp.main(StreamingKakfaDirectYarnApp.scala)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 14 more</span><br><span class="line">19/11/03 15:38:41 INFO SparkContext: Invoking stop() from shutdown hook</span><br><span class="line">19/11/03 15:38:41 INFO SparkUI: Stopped Spark web UI at http://hadoop101:4040</span><br><span class="line">19/11/03 15:38:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span><br><span class="line">19/11/03 15:38:41 INFO MemoryStore: MemoryStore cleared</span><br><span class="line">19/11/03 15:38:41 INFO BlockManager: BlockManager stopped</span><br><span class="line">19/11/03 15:38:41 INFO BlockManagerMaster: BlockManagerMaster stopped</span><br><span class="line">19/11/03 15:38:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span><br><span class="line">19/11/03 15:38:41 INFO SparkContext: Successfully stopped SparkContext</span><br><span class="line">19/11/03 15:38:41 INFO ShutdownHookManager: Shutdown hook called</span><br><span class="line">19/11/03 15:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-7780e105-fa8c-4592-ac12-7d27fc631ccd</span><br><span class="line">19/11/03 15:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-58bc8e97-e4ea-4708-9819-b98f98cb2212</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/kafka/common/serialization/StringDeserializer</span><br><span class="line"></span><br><span class="line">这个东西和上面一样 因为这个东西是在哪？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103154351394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">因为sparkStreaming-kafka包里面包含kafka-client </span><br><span class="line">你idea里pom 配置一个ss-kafka是可以的 但是到服务器上 是需要kafka-client这个jar包的</span><br><span class="line">所以把它 也上传到服务器上</span><br><span class="line"></span><br><span class="line">./spark-submit \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name StreamingKakfaDirectYarnApp \</span><br><span class="line">--jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar,/home/double_happy/lib/kafka-clients-2.0.0.jar \</span><br><span class="line">--class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --master local[2] --name StreamingKakfaDirectYarnApp --jars /home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar,/home/double_happy/lib/kafka-clients-2.0.0.jar --class com.ruozedata.spark.ss04.StreamingKakfaDirectYarnApp /home/double_happy/lib/spark-core-1.0.jar hadoop101:9092,hadoop101:9093,hadoop101:9094 double_happy_offset double_happy_group3</span><br><span class="line">19/11/03 15:51:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/11/03 15:51:12 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/11/03 15:51:12 INFO SparkContext: Submitted application: StreamingKakfaDirectYarnApp</span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/11/03 15:51:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/11/03 15:51:13 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 44185.</span><br><span class="line">19/11/03 15:51:13 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/11/03 15:51:13 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/11/03 15:51:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d86d96f2-228c-4046-b62b-bbc683c696e8</span><br><span class="line">19/11/03 15:51:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/11/03 15:51:13 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/11/03 15:51:13 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/11/03 15:51:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/11/03 15:51:13 INFO SparkContext: Added JAR file:///home/double_happy/lib/spark-streaming-kafka-0-10_2.11-2.4.4.jar at spark://hadoop101:44185/jars/spark-streaming-kafka-0-10_2.11-2.4.4.jar with timestamp 1572767473466</span><br><span class="line">19/11/03 15:51:13 INFO SparkContext: Added JAR file:///home/double_happy/lib/kafka-clients-2.0.0.jar at spark://hadoop101:44185/jars/kafka-clients-2.0.0.jar with timestamp 1572767473467</span><br><span class="line">19/11/03 15:51:13 INFO SparkContext: Added JAR file:/home/double_happy/lib/spark-core-1.0.jar at spark://hadoop101:44185/jars/spark-core-1.0.jar with timestamp 1572767473467</span><br><span class="line">19/11/03 15:51:13 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/11/03 15:51:13 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 34854.</span><br><span class="line">19/11/03 15:51:13 INFO NettyBlockTransferService: Server created on hadoop101:34854</span><br><span class="line">19/11/03 15:51:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:34854 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 34854, None)</span><br><span class="line">19/11/03 15:51:14 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572767473528</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding enable.auto.commit to false for executor</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding auto.offset.reset to none for executor</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding executor group.id to spark-executor-double_happy_group3</span><br><span class="line">19/11/03 15:51:14 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@5427abd</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7fbff13b</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO FlatMappedDStream: Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@57b2814e</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4b876f31</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ShuffledDStream: Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@5d06636e</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Slide time = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Storage level = Serialized 1x Replicated</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Checkpoint interval = null</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Remember interval = 10000 ms</span><br><span class="line">19/11/03 15:51:14 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5db077dc</span><br><span class="line">19/11/03 15:51:15 INFO ConsumerConfig: ConsumerConfig values: </span><br><span class="line">        auto.commit.interval.ms = 5000</span><br><span class="line">        auto.offset.reset = earliest</span><br><span class="line">        bootstrap.servers = [hadoop101:9092, hadoop101:9093, hadoop101:9094]</span><br><span class="line">        check.crcs = true</span><br><span class="line">        client.id = </span><br><span class="line">        connections.max.idle.ms = 540000</span><br><span class="line">        default.api.timeout.ms = 60000</span><br><span class="line">        enable.auto.commit = false</span><br><span class="line">        exclude.internal.topics = true</span><br><span class="line">        fetch.max.bytes = 52428800</span><br><span class="line">        fetch.max.wait.ms = 500</span><br><span class="line">        fetch.min.bytes = 1</span><br><span class="line">        group.id = double_happy_group3</span><br><span class="line">        heartbeat.interval.ms = 3000</span><br><span class="line">        interceptor.classes = []</span><br><span class="line">        internal.leave.group.on.close = true</span><br><span class="line">        isolation.level = read_uncommitted</span><br><span class="line">        key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">        max.partition.fetch.bytes = 1048576</span><br><span class="line">        max.poll.interval.ms = 300000</span><br><span class="line">        max.poll.records = 500</span><br><span class="line">        metadata.max.age.ms = 300000</span><br><span class="line">        metric.reporters = []</span><br><span class="line">        metrics.num.samples = 2</span><br><span class="line">        metrics.recording.level = INFO</span><br><span class="line">        metrics.sample.window.ms = 30000</span><br><span class="line">        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]</span><br><span class="line">        receive.buffer.bytes = 65536</span><br><span class="line">        reconnect.backoff.max.ms = 1000</span><br><span class="line">        reconnect.backoff.ms = 50</span><br><span class="line">        request.timeout.ms = 30000</span><br><span class="line">        retry.backoff.ms = 100</span><br><span class="line">        sasl.client.callback.handler.class = null</span><br><span class="line">        sasl.jaas.config = null</span><br><span class="line">        sasl.kerberos.kinit.cmd = /usr/bin/kinit</span><br><span class="line">        sasl.kerberos.min.time.before.relogin = 60000</span><br><span class="line">        sasl.kerberos.service.name = null</span><br><span class="line">        sasl.kerberos.ticket.renew.jitter = 0.05</span><br><span class="line">        sasl.kerberos.ticket.renew.window.factor = 0.8</span><br><span class="line">        sasl.login.callback.handler.class = null</span><br><span class="line">        sasl.login.class = null</span><br><span class="line">        sasl.login.refresh.buffer.seconds = 300</span><br><span class="line">        sasl.login.refresh.min.period.seconds = 60</span><br><span class="line">        sasl.login.refresh.window.factor = 0.8</span><br><span class="line">        sasl.login.refresh.window.jitter = 0.05</span><br><span class="line">        sasl.mechanism = GSSAPI</span><br><span class="line">        security.protocol = PLAINTEXT</span><br><span class="line">        send.buffer.bytes = 131072</span><br><span class="line">        session.timeout.ms = 10000</span><br><span class="line">        ssl.cipher.suites = null</span><br><span class="line">        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]</span><br><span class="line">        ssl.endpoint.identification.algorithm = https</span><br><span class="line">        ssl.key.password = null</span><br><span class="line">        ssl.keymanager.algorithm = SunX509</span><br><span class="line">        ssl.keystore.location = null</span><br><span class="line">        ssl.keystore.password = null</span><br><span class="line">        ssl.keystore.type = JKS</span><br><span class="line">        ssl.protocol = TLS</span><br><span class="line">        ssl.provider = null</span><br><span class="line">        ssl.secure.random.implementation = null</span><br><span class="line">        ssl.trustmanager.algorithm = PKIX</span><br><span class="line">        ssl.truststore.location = null</span><br><span class="line">        ssl.truststore.password = null</span><br><span class="line">        ssl.truststore.type = JKS</span><br><span class="line">        value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"></span><br><span class="line">19/11/03 15:51:15 INFO AppInfoParser: Kafka version : 2.0.0</span><br><span class="line">19/11/03 15:51:15 INFO AppInfoParser: Kafka commitId : 3402a8361b734732</span><br><span class="line">19/11/03 15:51:15 INFO Metadata: Cluster ID: QW2v3GZOQYCYmgUBgDaicA</span><br><span class="line">19/11/03 15:51:15 INFO AbstractCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Discovered group coordinator hadoop101:9092 (id: 2147483647 rack: null)</span><br><span class="line">19/11/03 15:51:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Revoking previously assigned partitions []</span><br><span class="line">19/11/03 15:51:15 INFO AbstractCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] (Re-)joining group</span><br><span class="line">19/11/03 15:51:15 INFO AbstractCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Successfully joined group with generation 7</span><br><span class="line">19/11/03 15:51:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-1, groupId=double_happy_group3] Setting newly assigned partitions [double_happy_offset-0, double_happy_offset-1, double_happy_offset-2]</span><br><span class="line">19/11/03 15:51:15 INFO Fetcher: [Consumer clientId=consumer-1, groupId=double_happy_group3] Resetting offset for partition double_happy_offset-1 to offset 0.</span><br><span class="line">19/11/03 15:51:15 INFO Fetcher: [Consumer clientId=consumer-1, groupId=double_happy_group3] Resetting offset for partition double_happy_offset-2 to offset 0.</span><br><span class="line">19/11/03 15:51:15 INFO Fetcher: [Consumer clientId=consumer-1, groupId=double_happy_group3] Resetting offset for partition double_happy_offset-0 to offset 0.</span><br><span class="line">19/11/03 15:51:15 INFO RecurringTimer: Started timer for JobGenerator at time 1572767480000</span><br><span class="line">19/11/03 15:51:15 INFO JobGenerator: Started JobGenerator at 1572767480000 ms</span><br><span class="line">19/11/03 15:51:15 INFO JobScheduler: Started JobScheduler</span><br><span class="line">19/11/03 15:51:15 INFO StreamingContext: StreamingContext started</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">StreamingContext started    ok没有问题</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这样做 只需要把你需要的依赖包拿过来就可以了 </span><br><span class="line"></span><br><span class="line">如果你需要额外的依赖包很多怎么办？</span><br><span class="line"></span><br><span class="line">虽然 --packages  不能去中央仓库去下载 但是你公司应该有一个 maven私服 那么你直接用私服里的就可以  </span><br><span class="line"></span><br><span class="line"> 这样做的好处 就是你spark代码包很小的</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 lib]$ ll -lh</span><br><span class="line">total 2.4M</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 1.9M Nov  3 14:16 kafka-clients-2.0.0.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy  48K Oct 24 23:24 local-1571929727692</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 1.1K Sep 25 19:45 site.log</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 225K Nov  3 15:05 spark-core-1.0.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy 212K Nov  3 14:16 spark-streaming-kafka-0-10_2.11-2.4.4.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy  37K Sep 23 18:32 udf.jar</span><br><span class="line">-rw-r--r-- 1 double_happy double_happy  36K Sep 23 11:21 wc.jar</span><br><span class="line">[double_happy@hadoop101 lib]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其实还有一种方式 ：</span><br><span class="line">我们开发的的都是 瘦包 ：仅仅只包含你自己开发的代码 不包括其他的依赖</span><br><span class="line"> </span><br><span class="line"> 瘦包 ：仅仅只包含你自己开发的代码 不包括其他的依赖</span><br><span class="line"> 		包小</span><br><span class="line"> 		需要的依赖的包自己来挑选</span><br><span class="line">胖包：不仅仅会把你自己开发的打包 还会把你的指定的依赖包一起打进去 </span><br><span class="line">		包大</span><br><span class="line">		所有的东西(Hadoop/Spark 除外 )都在里面 运行起来方便</span><br><span class="line">那么胖包怎么使用呢？就是我上面不推荐的链接    因为我之前就用这个方式 修改代码的时候 还得把 那个选项打开 我不喜欢</span><br><span class="line"></span><br><span class="line">瘦包还有一个好处就是 ： 方便升级   胖包真的不好</span><br></pre></td></tr></table></figure></div>


<p><strong>transformation</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">之前写的算子 都是按照每一个批次来处理的 或者是可以累计的等</span><br><span class="line"></span><br><span class="line">新需求：</span><br><span class="line">每隔5秒钟统计前10s钟的数据 </span><br><span class="line">每隔1分钟统计前5分钟的数据</span><br><span class="line"></span><br><span class="line">就是每隔多久统计前多久的数据  那么</span><br><span class="line">这类需求 就是 Window</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations" target="_blank" rel="noopener">Window Operations</a><br>As shown in the figure, every time the window slides over a source DStream, the source RDDs that fall within the window are combined and operated upon to produce the RDDs of the windowed DStream. In this specific case, the operation is applied over the last 3 time units of data, and slides by 2 time units. This shows that any window operation needs to specify two parameters.</p>
<p>window length - The duration of the window (3 in the figure).<br>sliding interval - The interval at which the window operation is performed (2 in the figure).<br>These two parameters must be multiples of the batch interval of the source DStream (1 in the figure).<br><img src="https://img-blog.csdnimg.cn/20191103162228877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>案列</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Return a new DStream by applying `reduceByKey` over a sliding window. This is similar to</span><br><span class="line">   * `DStream.reduceByKey()` but applies it over a sliding window. Hash partitioning is used to</span><br><span class="line">   * generate the RDDs with Spark&apos;s default number of partitions.</span><br><span class="line">   * @param reduceFunc associative and commutative reduce function</span><br><span class="line">   * @param windowDuration width of the window; must be a multiple of this DStream&apos;s</span><br><span class="line">   *                       batching interval</span><br><span class="line">   * @param slideDuration  sliding interval of the window (i.e., the interval after which</span><br><span class="line">   *                       the new DStream will generate RDDs); must be a multiple of this</span><br><span class="line">   *                       DStream&apos;s batching interval</span><br><span class="line">   */</span><br><span class="line">  def reduceByKeyAndWindow(</span><br><span class="line">      reduceFunc: (V, V) =&gt; V,</span><br><span class="line">      windowDuration: Duration,</span><br><span class="line">      slideDuration: Duration</span><br><span class="line">    ): DStream[(K, V)] = ssc.withScope &#123;</span><br><span class="line">    reduceByKeyAndWindow(reduceFunc, windowDuration, slideDuration, defaultPartitioner())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">只要你见Window  参数里一定带 窗口大小 和 滑动大小的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">5秒的批次 每隔5秒统计前10秒</span><br><span class="line"></span><br><span class="line">object StreamingKakfaWindowApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 5)</span><br><span class="line">    </span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1))</span><br><span class="line">      .reduceByKeyAndWindow((a:Int,b:Int) =&gt;</span><br><span class="line">      (a + b),  //窗口内统计两辆相加    业务</span><br><span class="line">      Seconds(10),  //窗口大小</span><br><span class="line">      Seconds(5)) //滑动大小</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,2)</span><br><span class="line">(b,1)</span><br><span class="line">(f,3)</span><br><span class="line">(e,2)</span><br><span class="line">(a,1)</span><br><span class="line">(c,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770065000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,2)</span><br><span class="line">(b,1)</span><br><span class="line">(f,3)</span><br><span class="line">(e,2)</span><br><span class="line">(a,1)</span><br><span class="line">(c,1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572770075000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">业务理解即可  这是最基本的统计</span><br><span class="line"></span><br><span class="line">问题：下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103164705730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>DataFrame and SQL Operations</strong><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#dataframe-and-sql-operations" target="_blank" rel="noopener">DataFrame and SQL Operations</a></p>
<p>这是批流一体带来的非常大的好处</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">object StreamingSqlApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 5)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    ).map(_.value())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line"></span><br><span class="line">    stream.foreachRDD(rdd =&gt; &#123;   //注意 stream 前面把 value取出来</span><br><span class="line"></span><br><span class="line">      // Get the singleton instance of SparkSession</span><br><span class="line">      val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()</span><br><span class="line">      import spark.implicits._</span><br><span class="line"></span><br><span class="line">      // Convert RDD[String] to DataFrame</span><br><span class="line">      val wordsDataFrame = rdd.toDF(&quot;word&quot;)</span><br><span class="line"></span><br><span class="line">      wordsDataFrame.groupBy(&quot;word&quot;).count().show(false)</span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/11/03 16:55:35 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|f   |25   |</span><br><span class="line">|e   |20   |</span><br><span class="line">|d   |25   |</span><br><span class="line">|c   |23   |</span><br><span class="line">|b   |21   |</span><br><span class="line">|a   |26   |</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line">19/11/03 16:55:40 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line">19/11/03 16:55:45 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.</span><br><span class="line">+----+-----+</span><br><span class="line">|word|count|</span><br><span class="line">+----+-----+</span><br><span class="line">+----+-----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">那么这个地方我们使用DF的方式 也可以按sql写 </span><br><span class="line"></span><br><span class="line">官网也有些累加器广播变量在ss里面的使用 和RDD都是一样的  看官网学习</span><br></pre></td></tr></table></figure></div>

<p><strong>消费语义****</strong><br><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#definitions" target="_blank" rel="noopener">Definitions</a><br>The semantics of streaming systems are often captured in terms of <strong>how many times each record can be processed by the system</strong>. There are three types of guarantees that a system can provide under all possible operating conditions (despite failures, etc.)</p>
<p>1.<strong>At most once</strong>: Each record will be <strong>either processed once or not processed at all.</strong><br>2.<strong>At least once</strong>: Each record will be processed one or more times. This is stronger than at-most once as it ensure that <strong>no data will be lost</strong>.                      <strong>But there may be duplicates</strong>.<br>3.<strong>Exactly once</strong>: Each record will be processed exactly once - <strong>no data will be lost and no data will be processed multiple times</strong>. This is obviously the strongest guarantee of the three.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1.流系统中 你的数据被处理了多少次  根据处理多少次 分为三大类</span><br><span class="line">	At most once  </span><br><span class="line">		 最多一次</span><br><span class="line">		 数据可能有丢失</span><br><span class="line">   At least once    </span><br><span class="line">   		至少一次</span><br><span class="line">   		数据不会丢失 但是数据可能会重复</span><br><span class="line">   Exactly once</span><br><span class="line">   		仅一次</span><br><span class="line">   		数据不丢失 数据不会重复 数据也不会被处理多次</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">At most once  ：</span><br><span class="line">	如果ss 消费kafka的数据 先保存offset 再处理结果 (我之前演示的代码 都是最后提交offset) </span><br><span class="line">	但是结果处理挂了 由于offset已经保存了 再处理结果 数据就丢失了 </span><br><span class="line">	所以 一定要先处理结果再保存offset</span><br><span class="line">	</span><br><span class="line"> At least once ：按着上面的方式提交offset</span><br><span class="line"> 	就是结果处理挂了 offset没有提交 再处理结果 数据就重复了 </span><br><span class="line"></span><br><span class="line"> Exactly once：</span><br><span class="line"> 	这个是最完美的 但是***</span><br><span class="line"> 	你要保证它还是有难度的   看官网</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operations" target="_blank" rel="noopener">Semantics of output operations</a><br><strong>Output operations (like foreachRDD) have at-least once semantics</strong>, that is, the <strong>transformed data may get written to an external entity more than once in the event of a worker failure.</strong> While this is acceptable for saving to file systems using the saveAs<strong><em>Files operations (as the file will simply get overwritten with the same data), *</em>additional effort may be necessary to achieve exactly-once semantics.</strong> There are two approaches.<br>1.<strong>Idempotent updates</strong>: Multiple attempts always write the same data. For example, <strong>saveAs*</strong>Files** always writes the same data to the generated files.<br>2.<strong>Transactional updates</strong>: All updates are made transactionally so that updates are made exactly once atomically. One way to do this would be the following.</p>
<p>Use the batch time (available in foreachRDD) and the partition index of the RDD to create an identifier. This identifier uniquely identifies a blob data in the streaming application.<br>Update external system with this blob transactionally (that is, exactly once, atomically) using the identifier. That is, if the identifier is not already committed, commit the partition data and the identifier atomically. Else, if this was already committed, skip the update.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">	dstream.foreachRDD &#123; (rdd, time) =&gt;   //time就是你当前批次的时间</span><br><span class="line">  rdd.foreachPartition &#123; partitionIterator =&gt;</span><br><span class="line">    val partitionId = TaskContext.get.partitionId()    //task id </span><br><span class="line">    val uniqueId = generateUniqueId(time.milliseconds, partitionId)   //根据你 的批次的时间 和 task ID 来组成  唯一的一个key (这个key 你每次的操作基于这个key)</span><br><span class="line">    // use this uniqueId to transactionally commit the data in partitionIterator</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Output operations (like foreachRDD) have at-least once semantics</span><br><span class="line"></span><br><span class="line">foreachRDD是保证 at-least onc 这个级别的奥   并不是保证 仅一次的语义</span><br><span class="line"></span><br><span class="line"> two approaches:</span><br><span class="line"> 	1.Idempotent updates  幂等    幂等可以通过主键来控制  主键设计不好等于0</span><br><span class="line">	2.Transactional updates</span><br><span class="line">   3.自己实现  把我们数据和offset绑定  </span><br><span class="line"></span><br><span class="line">也就是说 spark 默认是达到 At least once  </span><br><span class="line"></span><br><span class="line">需要借助At least once 去自己实现 Exactly once</span><br><span class="line"></span><br><span class="line">Exactly once：其实挺简单的 </span><br><span class="line">1.一个是offset提交</span><br><span class="line">2.第二个是 业务数据写出去 </span><br><span class="line">这个两个东西只要有offset能够关联的上  是没有问题的</span><br></pre></td></tr></table></figure></div>

<h2 id="调优"><a href="#调优" class="headerlink" title="调优 ****"></a>调优 ****</h2><p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#performance-tuning" target="_blank" rel="noopener">Performance Tuning</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.减少每隔批次处理的时间</span><br><span class="line">2.设置合理的批次大小  也就是说  你多久跑一个批次</span><br><span class="line"></span><br><span class="line">那么通过案例结合UI讲解</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">object StreamingTuningApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 5)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;, //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;, //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent, //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams) //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">查看UI：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103175129217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽——————————————————————————————————</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ok 我们往kafka写10条数据</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103175538708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽——————————————————————————————————<br><img src="https://img-blog.csdnimg.cn/20191103175830927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽——————————————————————————————————<br><img src="https://img-blog.csdnimg.cn/20191103180027441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input Rate：数据输入的速率</span><br><span class="line">Scheduling Delay：每个批次启动任务等待了多少时间被调度  叫 调度的延迟</span><br><span class="line">Processing Time：每个批次处理花费了多少时间</span><br><span class="line">Total Delay：调度延迟 + 处理时间</span><br><span class="line"></span><br><span class="line">这些在ui最下面都能看到</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019110318053852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">最佳实践：</span><br><span class="line">	在下一个批次启动任务之前，一定要运行完前一个批次的数据处理     </span><br><span class="line"></span><br><span class="line">如果你当前批次数据都没有处理完 下一个批次数据进来 也就意味着 你的数据逐渐逐渐堆积的 </span><br><span class="line">你的数据在堆积 也就意味着 后面的作业肯定 对于Scheduling Delay 要花一些时间的 </span><br><span class="line">整个作业运行时间也就越来越长的</span><br><span class="line"></span><br><span class="line">这个就符合官网的两点 ：</span><br><span class="line">	1.合适的batch size  也就是你的这一个批次 尽快的 处理完 不然你这个一个批次 接受数据以后 都不能很快的处理完</span><br><span class="line">		后面的作业逐渐的堆积的 越堆积越多 那么越到后面你的应用程序会完蛋</span><br><span class="line">   那么 batch time 设置多少合适？是根据需求来定的 </span><br><span class="line"></span><br><span class="line">影响任务运行时长的要素有哪些？</span><br><span class="line">	1.数据规模       </span><br><span class="line">			数据量大 一定要多放core （多放core 不一定有用 为什么？ 因为你topic的partition 和RDD的partition是一一对应的）</span><br><span class="line">			可以调整topic的分区数  分区数越多 也就意味着RDD的分区越多   RDD的分区越多task也就越多  task多 并行度就上去了</span><br><span class="line">	2.batch time  </span><br><span class="line">			time越长表示 一个批次的数据越多  数据越多你相同的资源下面 处理数据的时长肯定要多一点</span><br><span class="line">	3.业务复杂度</span><br><span class="line">			如果你的算子用的不好 也就意味着整个 带着大量的shuffle 你的性能会差很多很多  </span><br><span class="line">所以这些东西一定要先测</span><br><span class="line">	batch time 设置 需求来定是一方面  另一个一定到环境上测试 测试得到满意的结果 不是像sb产品经理拍脑袋那样 设置的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103182702262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个地方kafka是有一个限速的</span><br><span class="line"></span><br><span class="line">为了ss程序7*24小时高性能稳定的跑 所以尽可能的 你的批次处理时间和调度间隔 有一个什么关系呢？ 你的批次处理处理时间 要比调度间隔小</span><br><span class="line"></span><br><span class="line">Kafka限速：</span><br><span class="line">  	配置一个参数 </span><br><span class="line">  		 spark.streaming.kafka.maxRatePerPartition  ： </span><br><span class="line">  		 	Maximum rate (number of records per second) read from kafka</span><br><span class="line">  		 	 when using the new Kafka direct stream API</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">修改代码：</span><br><span class="line">  def getStreamingContext(appname:String,batch:Int,defalut:String = &quot;local[2]&quot;) =&#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line">    </span><br><span class="line">    //</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;10&quot;)</span><br><span class="line"></span><br><span class="line">    new StreamingContext(sparkConf,Seconds(batch))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">先测试没有修改前的：</span><br><span class="line"> 同时我写入kafka一些数据   查看结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103183912458.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试修改后的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103184011115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">说明这个参数没有生效 emm </span><br><span class="line">我的问题 因为 我们每次往kafka写的数据才10条 我调大一下在测试   改为1000条  我写了两次往kafka里 查看结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103184757992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">看 说面限速成功了  这样第一次处理就很好的限制你能处理的范围内</span><br><span class="line"></span><br><span class="line">但是 300 怎么来的？</span><br><span class="line">	而 这个参数 sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;10&quot;)</span><br><span class="line">	我们设置的是 10 </span><br><span class="line">	为什么ui上面看到的是300呢？</span><br><span class="line"></span><br><span class="line">有个计算公式的 </span><br><span class="line">	10s一个批次 </span><br><span class="line">	topic 3 个分区   ==》数据量 = 10 *3*10 =300 	</span><br><span class="line">	topic 1个分区   ===》 数据量 = 10 *1 *10 =100</span><br><span class="line"></span><br><span class="line">maxRatePerPartition 指的是每一个分区10条	  那么一个topic就是30条  10s就是 300条</span><br><span class="line"></span><br><span class="line">这个参数只适合 direct api  </span><br><span class="line"></span><br><span class="line">限速的地方：</span><br><span class="line">	1.当你topic里有大量没有处理的数据的时候 并且  &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot; 选择earliest （就是从最早消费）</span><br><span class="line">		为了防止第一个批次数据量过大 要设置限速</span><br><span class="line">    2.你的业务高峰期和低峰期的时候数据量是不一样的       高峰期是低峰期数据量的很多倍的</span><br><span class="line">    你不限速 很多作业都会处在等待状态 因为你前面批次的那一点时间已经处理不过来这一批次的数据了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">但是有一个问题哟？</span><br><span class="line"> 我们把消费进来的最大数据量是控制住了 但是这个值是个静态的值  </span><br><span class="line"> </span><br><span class="line"> 假设你的集群吞吐量可以 你的这个值设置小了 怎么办？</span><br><span class="line"> 	随着业务的数据量增长，那么这个东西在生产环境上运行一段时间以后 kafka 消费进来的数据最大的量 应该也</span><br><span class="line"> 	要随着 业务变化而变化就好了 引出一个东西   背压机制</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">背压机制  ： backpressure  1.5版本引进来的</span><br><span class="line">什么是背压呢？</span><br><span class="line">	可以在运行时根据前一个批次数据的运行情况，动态调整后续批次读入的数据量</span><br><span class="line">	这样可以很长从容的面对数据量 突增 和波动的情况 </span><br><span class="line"></span><br><span class="line">这个东西就是一个参数控制一下就ok了 </span><br><span class="line"></span><br><span class="line">spark.streaming.backpressure.enabled</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">背压：它是根据当前批次决定后一个批次 </span><br><span class="line">	</span><br><span class="line">	如果offset 从头开始消费 而且数据量很多的时候   我们启动的时候是从第一个批次启动的</span><br><span class="line">	但是第一个批次 依据谁呢？  没有的</span><br><span class="line">	所以你第一次处理 没有很好的办法评估读取的量  所以还有一个参数 初始化的一个东西</span><br><span class="line">spark.streaming.backpressure.initialRate  用来控制背压初始化读取的数据量</span><br><span class="line">	但是：看下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103191627790.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">如果按照官方这个描述 数据是从receiver过来的  </span><br><span class="line">而我们是没有receiver 这个东西的  direct是没有receiver的</span><br><span class="line"></span><br><span class="line">这个参数能起作用么？测试一下</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">我设置为150</span><br><span class="line"> def getStreamingContext(appname:String,batch:Int,defalut:String = &quot;local[2]&quot;) =&#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">    //</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;10&quot;)</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;,&quot;true&quot;)</span><br><span class="line">    sparkConf.set(&quot;spark.streaming.backpressure.initialRate&quot;,&quot;150&quot;)</span><br><span class="line"></span><br><span class="line">    new StreamingContext(sparkConf,Seconds(batch))</span><br><span class="line">  &#125;</span><br><span class="line">  查看ui：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191103192244253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>不能使用</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">那么该怎么办呢？  自己找找答案</span><br></pre></td></tr></table></figure></div>

<h2 id="优雅的关闭JVM"><a href="#优雅的关闭JVM" class="headerlink" title="优雅的关闭JVM"></a>优雅的关闭JVM</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.stopGracefullyOnShutdown  </span><br><span class="line">	If true, Spark shuts down the StreamingContext gracefully on JVM shutdown rather than immediately.</span><br><span class="line">	会缓慢的关闭 而不是直接关闭</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def getStreamingContext(appname: String, batch: Int, defalut: String = &quot;local[2]&quot;) = &#123;</span><br><span class="line"></span><br><span class="line">  val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">  //</span><br><span class="line">  sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;, &quot;10&quot;)</span><br><span class="line">  sparkConf.set(&quot;spark.streaming.backpressure.enabled&quot;, &quot;true&quot;)</span><br><span class="line">  sparkConf.set(&quot;spark.streaming.stopGracefullyOnShutdown &quot;, &quot;true&quot;)</span><br><span class="line">  new StreamingContext(sparkConf, Seconds(batch))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SS03" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/21/SS03/">SS03</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/21/SS03/" class="article-date">
  <time datetime="2018-02-21T12:12:09.000Z" itemprop="datePublished">2018-02-21</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">批流一体：未来的发展趋势</span><br><span class="line">    Spark</span><br><span class="line">    Flink</span><br><span class="line">   他们可以做到 </span><br><span class="line"></span><br><span class="line">MR/Spark/Flink on YARN：是现在是主流方式 </span><br><span class="line"></span><br><span class="line">但是 k8s是未来的主流  等学到容器 用这个</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming provides two categories of built-in streaming sources.</span><br><span class="line"></span><br><span class="line">Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.</span><br><span class="line">Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Some of these advanced sources are as follows.</span><br><span class="line"></span><br><span class="line">Kafka: Spark Streaming 2.4.4 is compatible with Kafka broker versions 0.8.2.1 or higher. See the Kafka Integration Guide for more details.</span><br><span class="line"></span><br><span class="line">Flume: Spark Streaming 2.4.4 is compatible with Flume 1.6.0. See the Flume Integration Guide for more details.</span><br><span class="line"></span><br><span class="line">Kinesis: Spark Streaming 2.4.4 is compatible with Kinesis Client Library 1.2.1. See the Kinesis Integration Guide for more details.</span><br></pre></td></tr></table></figure></div>
<p><strong>Kafka整合</strong></p>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html#spark-streaming-kafka-integration-guide" target="_blank" rel="noopener">Spark Streaming + Kafka Integration Guide</a></p>
<p>The Kafka project introduced <strong>a new consumer API between versions 0.8 and 0.10,</strong> so there are 2 separate corresponding Spark Streaming packages available. Please choose the correct package for your brokers and desired features; note that the 0.8 integration is compatible with later 0.9 and 0.10 brokers, but the 0.10 integration is not compatible with earlier brokers.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.高阶Api</span><br><span class="line">2.低阶Api  </span><br><span class="line">	就是offset需要我们自己维护</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101105050468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一定要用这个：</span><br><span class="line">		spark-streaming-kafka-0-10   和0.8的差别 </span><br><span class="line">		主要在Receiver DStream</span><br></pre></td></tr></table></figure></div>

<p><strong>区别</strong><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html#spark-streaming-kafka-integration-guide-kafka-broker-version-082" target="_blank" rel="noopener">spark-streaming-kafka-0-8</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-streaming-kafka-0-8：</span><br><span class="line"> 第一种方式：Receiver-based Approach</span><br><span class="line"> 	1.uses a Receiver to receive the data</span><br><span class="line"> 		那么Receiver是跑在哪里的？executor里面的 </span><br><span class="line"> 	2.using the Kafka high-level consumer API.</span><br><span class="line"> 	3. received from Kafka through a Receiver is stored in Spark executors</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101115349607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线———————————————————————————————————————-</p>
<p>However, under <strong>default configuration</strong>, this approach can <strong>lose data</strong> under failures (see receiver reliability. <strong>To ensure zero-data loss</strong>, you have to additionally <strong>enable Write-Ahead Logs</strong> in Spark Streaming (introduced in Spark 1.2). This synchronously saves all the <strong>received Kafka data into write-ahead logs on a distributed file system</strong> (e.g HDFS), so that all the data can be recovered on failure. See Deploying section in the streaming programming guide for more details on Write-Ahead Logs.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WAL机制：先把日志记录下来 这里就是数据</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">注意：Kafka的整合只有一个工具类 叫KafkaUtils</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create an input stream that pulls messages from Kafka Brokers.</span><br><span class="line">   * @param ssc       StreamingContext object</span><br><span class="line">   * @param zkQuorum  Zookeeper quorum (hostname:port,hostname:port,..)</span><br><span class="line">   * @param groupId   The group id for this consumer</span><br><span class="line">   * @param topics    Map of (topic_name to numPartitions) to consume. Each partition is consumed</span><br><span class="line">   *                  in its own thread</span><br><span class="line">   * @param storageLevel  Storage level to use for storing the received objects</span><br><span class="line">   *                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)</span><br><span class="line">   * @return DStream of (Kafka message key, Kafka message value)</span><br><span class="line">   */</span><br><span class="line">  def createStream(</span><br><span class="line">      ssc: StreamingContext,</span><br><span class="line">      zkQuorum: String,</span><br><span class="line">      groupId: String,</span><br><span class="line">      topics: Map[String, Int],</span><br><span class="line">      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2</span><br><span class="line">    ): ReceiverInputDStream[(String, String)] = &#123;</span><br><span class="line">    val kafkaParams = Map[String, String](</span><br><span class="line">      &quot;zookeeper.connect&quot; -&gt; zkQuorum, &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;zookeeper.connection.timeout.ms&quot; -&gt; &quot;10000&quot;)</span><br><span class="line">    createStream[String, String, StringDecoder, StringDecoder](</span><br><span class="line">      ssc, kafkaParams, topics, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">Receiver这个方式：</span><br><span class="line">注意：</span><br><span class="line">    storageLevel  ==MEMORY_AND_DISK_SER_2</span><br><span class="line">    1）数据丢失</span><br><span class="line">    2）WAL ==&gt; 数据延迟</span><br><span class="line">    3）offset我们不care</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为什么MEMORY_AND_DISK_SER_2设计为 2 ？</span><br><span class="line">	2的原因就是防止数据丢失 但是 问题是 即使是2 也不能保证数据 不丢 </span><br><span class="line"></span><br><span class="line">如果数据是通过WAL机制 写到HDFS上去  那么这个storageLevel 还有必要是 2 么？</span><br><span class="line">	一定是没有必要的    官网有写  你想想哈 如果是2  再加上hdfs本身的副本数  数据量是不是太大了 </span><br><span class="line"></span><br><span class="line">虽然WAL解决数据丢失问题 但是带来了一个问题？</span><br><span class="line">     就是数据写到HDFS上之后 更新zk 里的offset  </span><br><span class="line">     那么整体的时效性一定是下降的    你的实时跟HDFS挂钩了  实时性降低 数据延迟</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019110111454712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Points to remember:</p>
<p><strong>Topic partitions in Kafka do not correlate to partitions of RDDs generated in Spark Streaming</strong>. So increasing the number of topic-specific partitions in the KafkaUtils.createStream() only increases the number of threads using which topics that <strong>are consumed within a single receiver</strong>. <strong>It does not increase the parallelism of Spark in processing the data</strong>. Refer to the main document for more information on that.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Topic是有partition的 </span><br><span class="line">假设 一个Topic 对应3个partition</span><br><span class="line">   Kafka 里的partition 和 SS产生的RDD里面的partition 不是一个概念 </span><br><span class="line">   即：</span><br><span class="line">   	1 Topic ==》 3 parititions    RDD的并行度 并不是3  </span><br><span class="line">2.所以你增加Topic的分区 仅仅增加 使用的线程数 去处理topic的 还是a single receiver</span><br><span class="line"> 根本不会增加spark处理数据的并行度</span><br></pre></td></tr></table></figure></div>

<p>Multiple Kafka input DStreams can be created with different groups and topics for parallel receiving of data using multiple receivers.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">	也就是 spark-streaming-kafka-0-8 问题很多 别用了  要了解原理</span><br></pre></td></tr></table></figure></div>

<p><strong>spark-streaming-kafka-0-10  重点</strong><br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html" target="_blank" rel="noopener">    spark-streaming-kafka-0-10</a></p>
<p>The Spark Streaming integration for Kafka 0.10 is similar in design to the 0.8 Direct Stream approach. It provides simple parallelism, <strong>1:1 correspondence between Kafka partitions and Spark partitions</strong>, and <strong>access to offsets and metadata</strong>. However, <strong>because the newer integration uses the new Kafka consumer API instead of the simple API, there are notable differences in usage.</strong> This version of the integration is marked as experimental, so the API is potentially subject to change.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Offset管理的时候不同</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#creating-a-direct-stream" target="_blank" rel="noopener">Creating a Direct Stream</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Direct</span><br><span class="line">    不需要Receiver</span><br><span class="line">    Topic的partition和RDD的partition是1:1</span><br><span class="line">    自己手工维护offset   (那么默认offset存在哪？知道么）</span><br></pre></td></tr></table></figure></div>
<p><strong>案例</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line">Producer:  console  （测试的时候 ） 就是使用KafkaApi代码实现</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * A Kafka client that publishes records to the Kafka cluster.</span><br><span class="line"> * &lt;P&gt;</span><br><span class="line"> * The producer is &lt;i&gt;thread safe&lt;/i&gt; and sharing a single producer instance across threads will generally be faster than</span><br><span class="line"> * having multiple instances.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * Here is a simple example of using the producer to send records with strings containing sequential numbers as the key/value</span><br><span class="line"> * pairs.</span><br><span class="line"> * &lt;pre&gt;</span><br><span class="line"> * &#123;@code</span><br><span class="line"> * Properties props = new Properties();</span><br><span class="line"> * props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line"> * props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line"> * props.put(&quot;retries&quot;, 0);</span><br><span class="line"> * props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line"> * props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line"> * props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line"> * props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"> * props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"> *</span><br><span class="line"> * Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line"> * for (int i = 0; i &lt; 100; i++)</span><br><span class="line"> *     producer.send(new ProducerRecord&lt;String, String&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"> *</span><br><span class="line"> * producer.close();</span><br><span class="line"> * &#125;&lt;/pre&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The producer consists of a pool of buffer space that holds records that haven&apos;t yet been transmitted to the server</span><br><span class="line"> * as well as a background I/O thread that is responsible for turning these records into requests and transmitting them</span><br><span class="line"> * to the cluster. Failure to close the producer after use will leak these resources.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &#123;@link #send(ProducerRecord) send()&#125; method is asynchronous. When called it adds the record to a buffer of pending record sends</span><br><span class="line"> * and immediately returns. This allows the producer to batch together individual records for efficiency.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &lt;code&gt;acks&lt;/code&gt; config controls the criteria under which requests are considered complete. The &quot;all&quot; setting</span><br><span class="line"> * we have specified will result in blocking on the full commit of the record, the slowest but most durable setting.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * If the request fails, the producer can automatically retry, though since we have specified &lt;code&gt;retries&lt;/code&gt;</span><br><span class="line"> * as 0 it won&apos;t. Enabling retries also opens up the possibility of duplicates (see the documentation on</span><br><span class="line"> * &lt;a href=&quot;http://kafka.apache.org/documentation.html#semantics&quot;&gt;message delivery semantics&lt;/a&gt; for details).</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The producer maintains buffers of unsent records for each partition. These buffers are of a size specified by</span><br><span class="line"> * the &lt;code&gt;batch.size&lt;/code&gt; config. Making this larger can result in more batching, but requires more memory (since we will</span><br><span class="line"> * generally have one of these buffers for each active partition).</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * By default a buffer is available to send immediately even if there is additional unused space in the buffer. However if you</span><br><span class="line"> * want to reduce the number of requests you can set &lt;code&gt;linger.ms&lt;/code&gt; to something greater than 0. This will</span><br><span class="line"> * instruct the producer to wait up to that number of milliseconds before sending a request in hope that more records will</span><br><span class="line"> * arrive to fill up the same batch. This is analogous to Nagle&apos;s algorithm in TCP. For example, in the code snippet above,</span><br><span class="line"> * likely all 100 records would be sent in a single request since we set our linger time to 1 millisecond. However this setting</span><br><span class="line"> * would add 1 millisecond of latency to our request waiting for more records to arrive if we didn&apos;t fill up the buffer. Note that</span><br><span class="line"> * records that arrive close together in time will generally batch together even with &lt;code&gt;linger.ms=0&lt;/code&gt; so under heavy load</span><br><span class="line"> * batching will occur regardless of the linger configuration; however setting this to something larger than 0 can lead to fewer, more</span><br><span class="line"> * efficient requests when not under maximal load at the cost of a small amount of latency.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &lt;code&gt;buffer.memory&lt;/code&gt; controls the total amount of memory available to the producer for buffering. If records</span><br><span class="line"> * are sent faster than they can be transmitted to the server then this buffer space will be exhausted. When the buffer space is</span><br><span class="line"> * exhausted additional send calls will block. The threshold for time to block is determined by &lt;code&gt;max.block.ms&lt;/code&gt; after which it throws</span><br><span class="line"> * a TimeoutException.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The &lt;code&gt;key.serializer&lt;/code&gt; and &lt;code&gt;value.serializer&lt;/code&gt; instruct how to turn the key and value objects the user provides with</span><br><span class="line"> * their &lt;code&gt;ProducerRecord&lt;/code&gt; into bytes. You can use the included &#123;@link org.apache.kafka.common.serialization.ByteArraySerializer&#125; or</span><br><span class="line"> * &#123;@link org.apache.kafka.common.serialization.StringSerializer&#125; for simple string or byte types.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * From Kafka 0.11, the KafkaProducer supports two additional modes: the idempotent producer and the transactional producer.</span><br><span class="line"> * The idempotent producer strengthens Kafka&apos;s delivery semantics from at least once to exactly once delivery. In particular</span><br><span class="line"> * producer retries will no longer introduce duplicates. The transactional producer allows an application to send messages</span><br><span class="line"> * to multiple partitions (and topics!) atomically.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * To enable idempotence, the &lt;code&gt;enable.idempotence&lt;/code&gt; configuration must be set to true. If set, the</span><br><span class="line"> * &lt;code&gt;retries&lt;/code&gt; config will default to &lt;code&gt;Integer.MAX_VALUE&lt;/code&gt; and the &lt;code&gt;acks&lt;/code&gt; config will</span><br><span class="line"> * default to &lt;code&gt;all&lt;/code&gt;. There are no API changes for the idempotent producer, so existing applications will</span><br><span class="line"> * not need to be modified to take advantage of this feature.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * To take advantage of the idempotent producer, it is imperative to avoid application level re-sends since these cannot</span><br><span class="line"> * be de-duplicated. As such, if an application enables idempotence, it is recommended to leave the &lt;code&gt;retries&lt;/code&gt;</span><br><span class="line"> * config unset, as it will be defaulted to &lt;code&gt;Integer.MAX_VALUE&lt;/code&gt;. Additionally, if a &#123;@link #send(ProducerRecord)&#125;</span><br><span class="line"> * returns an error even with infinite retries (for instance if the message expires in the buffer before being sent),</span><br><span class="line"> * then it is recommended to shut down the producer and check the contents of the last produced message to ensure that</span><br><span class="line"> * it is not duplicated. Finally, the producer can only guarantee idempotence for messages sent within a single session.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;To use the transactional producer and the attendant APIs, you must set the &lt;code&gt;transactional.id&lt;/code&gt;</span><br><span class="line"> * configuration property. If the &lt;code&gt;transactional.id&lt;/code&gt; is set, idempotence is automatically enabled along with</span><br><span class="line"> * the producer configs which idempotence depends on. Further, topics which are included in transactions should be configured</span><br><span class="line"> * for durability. In particular, the &lt;code&gt;replication.factor&lt;/code&gt; should be at least &lt;code&gt;3&lt;/code&gt;, and the</span><br><span class="line"> * &lt;code&gt;min.insync.replicas&lt;/code&gt; for these topics should be set to 2. Finally, in order for transactional guarantees</span><br><span class="line"> * to be realized from end-to-end, the consumers must be configured to read only committed messages as well.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The purpose of the &lt;code&gt;transactional.id&lt;/code&gt; is to enable transaction recovery across multiple sessions of a</span><br><span class="line"> * single producer instance. It would typically be derived from the shard identifier in a partitioned, stateful, application.</span><br><span class="line"> * As such, it should be unique to each producer instance running within a partitioned application.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;All the new transactional APIs are blocking and will throw exceptions on failure. The example</span><br><span class="line"> * below illustrates how the new APIs are meant to be used. It is similar to the example above, except that all</span><br><span class="line"> * 100 messages are part of a single transaction.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * &lt;pre&gt;</span><br><span class="line"> * &#123;@code</span><br><span class="line"> * Properties props = new Properties();</span><br><span class="line"> * props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line"> * props.put(&quot;transactional.id&quot;, &quot;my-transactional-id&quot;);</span><br><span class="line"> * Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props, new StringSerializer(), new StringSerializer());</span><br><span class="line"> *</span><br><span class="line"> * producer.initTransactions();</span><br><span class="line"> *</span><br><span class="line"> * try &#123;</span><br><span class="line"> *     producer.beginTransaction();</span><br><span class="line"> *     for (int i = 0; i &lt; 100; i++)</span><br><span class="line"> *         producer.send(new ProducerRecord&lt;&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"> *     producer.commitTransaction();</span><br><span class="line"> * &#125; catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123;</span><br><span class="line"> *     // We can&apos;t recover from these exceptions, so our only option is to close the producer and exit.</span><br><span class="line"> *     producer.close();</span><br><span class="line"> * &#125; catch (KafkaException e) &#123;</span><br><span class="line"> *     // For all other exceptions, just abort the transaction and try again.</span><br><span class="line"> *     producer.abortTransaction();</span><br><span class="line"> * &#125;</span><br><span class="line"> * producer.close();</span><br><span class="line"> * &#125; &lt;/pre&gt;</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * As is hinted at in the example, there can be only one open transaction per producer. All messages sent between the</span><br><span class="line"> * &#123;@link #beginTransaction()&#125; and &#123;@link #commitTransaction()&#125; calls will be part of a single transaction. When the</span><br><span class="line"> * &lt;code&gt;transactional.id&lt;/code&gt; is specified, all messages sent by the producer must be part of a transaction.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * The transactional producer uses exceptions to communicate error states. In particular, it is not required</span><br><span class="line"> * to specify callbacks for &lt;code&gt;producer.send()&lt;/code&gt; or to call &lt;code&gt;.get()&lt;/code&gt; on the returned Future: a</span><br><span class="line"> * &lt;code&gt;KafkaException&lt;/code&gt; would be thrown if any of the</span><br><span class="line"> * &lt;code&gt;producer.send()&lt;/code&gt; or transactional calls hit an irrecoverable error during a transaction. See the &#123;@link #send(ProducerRecord)&#125;</span><br><span class="line"> * documentation for more details about detecting errors from a transactional send.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;/p&gt;By calling</span><br><span class="line"> * &lt;code&gt;producer.abortTransaction()&lt;/code&gt; upon receiving a &lt;code&gt;KafkaException&lt;/code&gt; we can ensure that any</span><br><span class="line"> * successful writes are marked as aborted, hence keeping the transactional guarantees.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * This client can communicate with brokers that are version 0.10.0 or newer. Older or newer brokers may not support</span><br><span class="line"> * certain client features.  For instance, the transactional APIs need broker versions 0.11.0 or later. You will receive an</span><br><span class="line"> * &lt;code&gt;UnsupportedVersionException&lt;/code&gt; when invoking an API that is not available in the running broker version.</span><br><span class="line"> * &lt;/p&gt;</span><br><span class="line"> */</span><br><span class="line">public class KafkaProducer&lt;K, V&gt; implements Producer&lt;K, V&gt; &#123;&#125;</span><br><span class="line"></span><br><span class="line">KafkaProducer源码里都介绍了怎么使用  之后讲解略过</span><br><span class="line"></span><br><span class="line">Consumer:console  （测试的时候 ）</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">KafkaProducer：</span><br><span class="line"></span><br><span class="line">先测试 ：发送abcdef </span><br><span class="line">object DataGenerator &#123;</span><br><span class="line">  private val logger: Logger = LoggerFactory.getLogger(DataGenerator.getClass)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val props = new Properties()</span><br><span class="line">    props.put(&quot;bootstrap.servers&quot;, &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;)</span><br><span class="line">    props.put(&quot;acks&quot;, &quot;all&quot;)</span><br><span class="line">    props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">    props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line"></span><br><span class="line">    val producer = new KafkaProducer[String, String](props)</span><br><span class="line"></span><br><span class="line">    for (i &lt;- 1 to 10) &#123;</span><br><span class="line">      Thread.sleep(100)</span><br><span class="line">      //拿一个abcdef</span><br><span class="line">      val word: String = String.valueOf((new Random().nextInt(6) + &apos;a&apos;).toChar)</span><br><span class="line">      val part = i % 3   //发到哪个分区 因为是三个分区</span><br><span class="line"></span><br><span class="line">      logger.error(&quot;word : &#123;&#125;&quot;,word)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果： 注意这代码 全是源码注释里有 你不需要记住 </span><br><span class="line">你只需要记住：</span><br><span class="line">	1.KafkaProducer 创建   </span><br><span class="line">	2.发送数据 要序列化</span><br><span class="line">	3.怎么发   </span><br><span class="line">	这三个东西 源码注释里全都是写好的 </span><br><span class="line"></span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : d</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : d</span><br><span class="line">19/11/01 13:23:18 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : e</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : a</span><br><span class="line">19/11/01 13:23:19 ERROR DataGenerator$: word : a</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">怎么发送呢？</span><br><span class="line"> /**</span><br><span class="line">     * Creates a record to be sent to a specified topic and partition</span><br><span class="line">     *</span><br><span class="line">     * @param topic The topic the record will be appended to</span><br><span class="line">     * @param partition The partition to which the record should be sent</span><br><span class="line">     * @param key The key that will be included in the record</span><br><span class="line">     * @param value The record contents</span><br><span class="line">     */</span><br><span class="line">    public ProducerRecord(String topic, Integer partition, K key, V value) &#123;</span><br><span class="line">        this(topic, partition, null, key, value, null);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object DataGenerator &#123;</span><br><span class="line"></span><br><span class="line">  private val logger: Logger = LoggerFactory.getLogger(DataGenerator.getClass)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val props = new Properties()</span><br><span class="line">    props.put(&quot;bootstrap.servers&quot;, &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;)</span><br><span class="line">    props.put(&quot;acks&quot;, &quot;all&quot;)</span><br><span class="line">    props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">    props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;)</span><br><span class="line">    val producer = new KafkaProducer[String, String](props)</span><br><span class="line"></span><br><span class="line">    for (i &lt;- 1 to 10) &#123;</span><br><span class="line">      Thread.sleep(100)</span><br><span class="line"></span><br><span class="line">      //拿一个abcdef</span><br><span class="line">      val word: String = String.valueOf((new Random().nextInt(6) + &apos;a&apos;).toChar)</span><br><span class="line">      val part = i % 3 //发到哪个分区 因为是三个分区</span><br><span class="line"></span><br><span class="line">      logger.error(&quot;word : &#123;&#125;&quot;, word)</span><br><span class="line"></span><br><span class="line">      val record = producer.send(new ProducerRecord[String, String](&quot;double_happy_offset&quot;, part, &quot;&quot;,word))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    producer.close()</span><br><span class="line">    println(&quot;double_happy 数据产生完毕..........&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ok进行测试</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line">&gt; --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic double_happy_offset \</span><br><span class="line">&gt; --from-beginning</span><br><span class="line"></span><br><span class="line">运行idea代码结果：</span><br><span class="line">19/11/01 13:33:56 ERROR DataGenerator$: word : a</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : e</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : a</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : c</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : f</span><br><span class="line">19/11/01 13:33:57 ERROR DataGenerator$: word : e</span><br><span class="line">19/11/01 13:33:58 ERROR DataGenerator$: word : a</span><br><span class="line">double_happy 数据产生完毕..........</span><br><span class="line"></span><br><span class="line">kafka控制台结果：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line">&gt; --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic double_happy_offset \</span><br><span class="line">&gt; --from-beginning</span><br><span class="line">a</span><br><span class="line">e</span><br><span class="line">f</span><br><span class="line">a</span><br><span class="line">f</span><br><span class="line">c</span><br><span class="line">c</span><br><span class="line">f</span><br><span class="line">e</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line">对接kafka + ss前期准备工作ok</span><br></pre></td></tr></table></figure></div>
<p><strong>对接</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * :: Experimental ::</span><br><span class="line">   * Scala constructor for a DStream where</span><br><span class="line">   * each given Kafka topic/partition corresponds to an RDD partition.</span><br><span class="line">   * The spark configuration spark.streaming.kafka.maxRatePerPartition gives the maximum number</span><br><span class="line">   *  of messages</span><br><span class="line">   * per second that each &apos;&apos;&apos;partition&apos;&apos;&apos; will accept.</span><br><span class="line">   * @param locationStrategy In most cases, pass in [[LocationStrategies.PreferConsistent]],</span><br><span class="line">   *   see [[LocationStrategies]] for more details.</span><br><span class="line">   * @param consumerStrategy In most cases, pass in [[ConsumerStrategies.Subscribe]],</span><br><span class="line">   *   see [[ConsumerStrategies]] for more details</span><br><span class="line">   * @tparam K type of Kafka message key</span><br><span class="line">   * @tparam V type of Kafka message value</span><br><span class="line">   */</span><br><span class="line">  @Experimental</span><br><span class="line">  def createDirectStream[K, V](</span><br><span class="line">      ssc: StreamingContext,</span><br><span class="line">      locationStrategy: LocationStrategy,</span><br><span class="line">      consumerStrategy: ConsumerStrategy[K, V]</span><br><span class="line">    ): InputDStream[ConsumerRecord[K, V]] = &#123;</span><br><span class="line">    val ppc = new DefaultPerPartitionConfig(ssc.sparkContext.getConf)</span><br><span class="line">    createDirectStream[K, V](ssc, locationStrategy, consumerStrategy, ppc)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">locationStrategy 策略什么意思呢？官网有  一会代码里我写了解释</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    stream.map(record =&gt; (record.key, record.value))  </span><br><span class="line">      .map(_._2).print()   //因为我们key就没有设置  只取value</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587100000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">a</span><br><span class="line">a</span><br><span class="line">c</span><br><span class="line">a</span><br><span class="line">f</span><br><span class="line">c</span><br><span class="line">e</span><br><span class="line">e</span><br><span class="line">f</span><br><span class="line">f</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587110000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587120000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587130000 ms</span><br><span class="line">-------------------------------------------     这块 我们kafka又发了一次数据 ssc接收到了</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572587140000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">c</span><br><span class="line">a</span><br><span class="line">e</span><br><span class="line">c</span><br><span class="line">d</span><br><span class="line">d</span><br><span class="line">a</span><br><span class="line">b</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">   //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    //结果入库 写到redis里</span><br><span class="line">    result.foreachRDD(rdd =&gt;&#123;</span><br><span class="line">      rdd.foreachPartition(paritition =&gt;&#123;</span><br><span class="line">        val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line"></span><br><span class="line">        paritition.foreach(pair =&gt;&#123;</span><br><span class="line">          jedis.hincrBy(&quot;kafka_ss_redis_wc&quot;,pair._1,pair._2)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        jedis.close()</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;kafka_ss_redis_wc&quot;</span><br><span class="line">3) &quot;doublehappy_redis_wc&quot;</span><br><span class="line">hadoop101:6379&gt; HGETALL kafka_ss_redis_wc</span><br><span class="line"> 1) &quot;e&quot;</span><br><span class="line"> 2) &quot;3&quot;</span><br><span class="line"> 3) &quot;d&quot;</span><br><span class="line"> 4) &quot;2&quot;</span><br><span class="line"> 5) &quot;a&quot;</span><br><span class="line"> 6) &quot;6&quot;</span><br><span class="line"> 7) &quot;b&quot;</span><br><span class="line"> 8) &quot;2&quot;</span><br><span class="line"> 9) &quot;c&quot;</span><br><span class="line">10) &quot;4&quot;</span><br><span class="line">11) &quot;f&quot;</span><br><span class="line">12) &quot;3&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019110114001642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个时候 把实时代码关掉 ：</span><br><span class="line"></span><br><span class="line">redis里的数据翻倍了 因为又写了一次嘛  </span><br><span class="line">但是这样是不行的  </span><br><span class="line">	因为代码里 这个控制的 </span><br><span class="line">		&quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">  </span><br><span class="line">那么接下来 看看offset 怎么获取呢？怎么提交offset呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101140557306.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#obtaining-offsets" target="_blank" rel="noopener">Obtaining Offsets</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">trait HasOffsetRanges &#123;</span><br><span class="line">  def offsetRanges: Array[OffsetRange]    //拿到offset的范围  返回值是数组 </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Represents a range of offsets from a single Kafka TopicPartition. Instances of this class</span><br><span class="line"> * can be created with `OffsetRange.create()`.</span><br><span class="line"> * @param topic Kafka topic name</span><br><span class="line"> * @param partition Kafka partition id</span><br><span class="line"> * @param fromOffset Inclusive starting offset</span><br><span class="line"> * @param untilOffset Exclusive ending offset</span><br><span class="line"> */</span><br><span class="line">final class OffsetRange private(</span><br><span class="line">    val topic: String,    </span><br><span class="line">    val partition: Int,</span><br><span class="line">    val fromOffset: Long,</span><br><span class="line">    val untilOffset: Long) extends Serializable &#123;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">获取offset：</span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">   //TODO...业务逻辑</span><br><span class="line">    val result: DStream[(String, Int)] = stream.map(_.value()).</span><br><span class="line">      flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    result.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  </span><br><span class="line"></span><br><span class="line">      //获取分区数</span><br><span class="line">      println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">      //获取当前批次的offset数据</span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">        println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding enable.auto.commit to false for executor</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding auto.offset.reset to none for executor</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream</span><br><span class="line">19/11/01 14:18:18 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135</span><br><span class="line">19/11/01 14:18:20 ERROR JobScheduler: Error running job streaming job 1572589100000 ms.0</span><br><span class="line">---------2</span><br><span class="line">java.lang.ClassCastException: org.apache.spark.rdd.ShuffledRDD cannot be cast to org.apache.spark.streaming.kafka010.HasOffsetRanges</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:48)</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:42)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.ClassCastException: org.apache.spark.rdd.ShuffledRDD cannot be cast to org.apache.spark.streaming.kafka010.HasOffsetRanges</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:48)</span><br><span class="line">	at com.ruozedata.spark.ss03.StreamingKakfaDirectApp$$anonfun$main$1.apply(StreamingKakfaDirectApp.scala:42)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101142414924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">为什么报错呢？  *****  而且分区数还是2  说明不对</span><br><span class="line">java.lang.ClassCastException: org.apache.spark.rdd.ShuffledRDD cannot be cast to org.apache.spark.streaming.kafka010.HasOffsetRanges</span><br><span class="line"></span><br><span class="line">ShuffledRDD 不能转成 HasOffsetRanges 上面图片解释了 </span><br><span class="line"></span><br><span class="line">为什么是ShuffledRDD 呢？ </span><br><span class="line">	因为reduceBykey之后的流里面 的rdd   就是ShuffledRDD 类型的   主要是经过了reduceBykey 明白吗</span><br><span class="line"></span><br><span class="line">所以上面的代码 不对 </span><br><span class="line">业务逻辑的代码是要放在里面写  而不是在前面做   ***</span><br><span class="line">所以直接用stream + foreachRDD   (业务逻辑在foreachRDD 里面写)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101143014894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">获取offset：</span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  </span><br><span class="line"></span><br><span class="line">      //获取分区数</span><br><span class="line">      println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">      //获取当前批次的offset数据</span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">        println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：他是一直在跑的哈</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 0 8</span><br><span class="line">double_happy_offset 2 0 6</span><br><span class="line">double_happy_offset 0 0 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line"></span><br><span class="line">看sparkUI   解释 结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101143701991.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 0 8</span><br><span class="line">double_happy_offset 2 0 6</span><br><span class="line">double_happy_offset 0 0 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8         从8 开始   因为没有数据进来了 </span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">20哪里来的？  8 +6+6 = 20</span><br><span class="line"></span><br><span class="line">由于数据过来 第一个批次全部处理完了  </span><br><span class="line">所以第二个批次 结果 是从8开始  </span><br><span class="line"></span><br><span class="line">那么我再生产10条数据 看结果</span><br><span class="line"></span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 8 8</span><br><span class="line">double_happy_offset 2 6 6</span><br><span class="line">double_happy_offset 0 6 6</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 6 9          也就是说 这个拿了3条数据</span><br><span class="line">double_happy_offset 1 8 12        拿了4条数据</span><br><span class="line">double_happy_offset 0 6 9          拿了3条数据</span><br><span class="line">---------3</span><br><span class="line"></span><br><span class="line">那么 我把程序关掉 结果一定是这样的：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 0 9</span><br><span class="line">double_happy_offset 2 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line"></span><br><span class="line">你获得到了偏移量  由于你没有提交 没有保存偏移量 </span><br><span class="line">所以重启之后都是从头开始跑的  就意味着 第一批数据 会很多 </span><br><span class="line"></span><br><span class="line">接下来就是提交offset</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets" target="_blank" rel="noopener">Storing Offsets</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">很多方式 ：</span><br><span class="line">1.Checkpoints   别用了 不好用 </span><br><span class="line">2.Kafka itself    </span><br><span class="line">	Kafka has an offset commit API that stores offsets in a special Kafka topic.</span><br><span class="line">3.Your own data store</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">2.Kafka itself    ：</span><br><span class="line"></span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; &quot;use_a_separate_group_id_for_each_stream&quot;,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(&quot;double_happy_offset&quot;)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  因为</span><br><span class="line"></span><br><span class="line">      //获取分区数</span><br><span class="line">      println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">      //获取当前批次的offset数据</span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">        println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      //kafka自身的方式  提交 更新的offset</span><br><span class="line">      stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 0 9</span><br><span class="line">double_happy_offset 1 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 9 9</span><br><span class="line">double_happy_offset 1 12 12</span><br><span class="line">double_happy_offset 0 9 9</span><br><span class="line">---------3</span><br><span class="line"></span><br><span class="line">关闭重启 之后结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 9 9</span><br><span class="line">double_happy_offset 1 12 12</span><br><span class="line">double_happy_offset 0 9 9</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 9 9</span><br><span class="line">double_happy_offset 1 12 12</span><br><span class="line">double_happy_offset 0 9 9</span><br><span class="line"></span><br><span class="line">说明offset 提交ok了 </span><br><span class="line"></span><br><span class="line">而且两次操作sparkUi也证实了</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101145733172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="第一次"><br>第二次 重启之后<br><img src="https://img-blog.csdnimg.cn/2019110114582759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">数据是0 对吧  因为数据已经提交过了 </span><br><span class="line">通过 kafka 命令可以查到offset </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$ ./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoop101:9092,hadoop101:9093,hadoop101:9094 --topic double_happy_offset</span><br><span class="line">double_happy_offset:0:9</span><br><span class="line">double_happy_offset:1:12</span><br><span class="line">double_happy_offset:2:9</span><br><span class="line">[double_happy@hadoop101 bin]$ </span><br><span class="line"></span><br><span class="line">那么kafka自身维护的offset存在哪里呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101150455299.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>However, you can <strong>commit offsets to Kafka after you know your output has been stored,</strong> using the commitAsync API. The benefit as compared to checkpoints is that Kafka is a durable store regardless of changes to your application code. However, <strong>Kafka is not transactional, so your outputs must still be idempotent.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1.你的业务逻辑完成之后再提交offset</span><br><span class="line">2.kafka并不是事务性的 所以你的输出 必须保证幂等性</span><br><span class="line"></span><br><span class="line">假设 </span><br><span class="line">double_happy_offset:0:9</span><br><span class="line">double_happy_offset:1:12</span><br><span class="line">double_happy_offset:2:9</span><br><span class="line"></span><br><span class="line">你第一次处理完了 结果也写到redis里了  </span><br><span class="line">因为种种原因我们可以手工指定 kafka的偏移量的  </span><br><span class="line"></span><br><span class="line">假设 第一个批次 5 接下来是 5-9   这里 5-9 已经消费了对吧 </span><br><span class="line"></span><br><span class="line">那么 我们手工指定 5-9 这个批次再消费一次 也就是说 </span><br><span class="line">redis 里面的结果又是错的  因为结果又重复了呀  这就是上面刚开始的 演示  </span><br><span class="line"></span><br><span class="line">这就是说你输出的代码 也要有幂等性  不管你输出跑多少次  即使重复消费 也要保证结果是具有幂等性的</span><br></pre></td></tr></table></figure></div>

<p><strong>3.Your own data store</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">这里我使用redis  你选择MySQL也是可以的    为了测试换一个groupid  让他重新消费</span><br><span class="line">注意：这种方式 提交offset</span><br><span class="line">手动提交offset的时候  要与groupid 对应 </span><br><span class="line"></span><br><span class="line">key： Topic + groupid </span><br><span class="line"></span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams)  //固定写法</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  因为</span><br><span class="line"></span><br><span class="line">      if(!rdd.isEmpty())&#123;</span><br><span class="line"></span><br><span class="line">        //获取分区数</span><br><span class="line">        println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">        //获取当前批次的offset数据</span><br><span class="line">        val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //TODO ... 处理业务逻辑 wc</span><br><span class="line"></span><br><span class="line">        //ToDO ... 提交Offset到Redis  使用第三种方式</span><br><span class="line">        val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line"></span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          val topicGroupId = x.topic + &quot;_&quot;+ groupId    //key = topic + groupId    </span><br><span class="line">          jedis.hset(topicGroupId,x.partition+&quot;&quot;,x.untilOffset+&quot;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line">        jedis.close()</span><br><span class="line"></span><br><span class="line">      &#125;else&#123;</span><br><span class="line">        println(&quot;当前批次没有数据.....&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 0 9</span><br><span class="line">double_happy_offset 1 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;kafka_ss_redis_wc&quot;</span><br><span class="line">3) &quot;doublehappy_redis_wc&quot;</span><br><span class="line">4) &quot;double_happy_offset_double_happy_group&quot;</span><br><span class="line">hadoop101:6379&gt; HGETALL double_happy_offset_double_happy_group</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) &quot;9&quot;</span><br><span class="line">3) &quot;1&quot;</span><br><span class="line">4) &quot;12&quot;</span><br><span class="line">5) &quot;0&quot;</span><br><span class="line">6) &quot;9&quot;</span><br><span class="line">hadoop101:6379&gt; </span><br><span class="line">结果是没有问题 的 </span><br><span class="line">key = topic + groupId      </span><br><span class="line"> jedis.hset(topicGroupId,x.partition+&quot;&quot;,x.untilOffset+&quot;&quot;)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101162513463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">再生产一批数据 查看结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 2 0 9</span><br><span class="line">double_happy_offset 1 0 12</span><br><span class="line">double_happy_offset 0 0 9</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 12 16</span><br><span class="line">double_happy_offset 2 9 12</span><br><span class="line">double_happy_offset 0 9 12</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; HGETALL double_happy_offset_double_happy_group</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) &quot;9&quot;</span><br><span class="line">3) &quot;1&quot;</span><br><span class="line">4) &quot;12&quot;</span><br><span class="line">5) &quot;0&quot;</span><br><span class="line">6) &quot;9&quot;</span><br><span class="line">hadoop101:6379&gt; HGETALL double_happy_offset_double_happy_group</span><br><span class="line">1) &quot;2&quot;</span><br><span class="line">2) &quot;12&quot;</span><br><span class="line">3) &quot;1&quot;</span><br><span class="line">4) &quot;16&quot;</span><br><span class="line">5) &quot;0&quot;</span><br><span class="line">6) &quot;12&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191101162924889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">我把程序关掉 先生成两个批次的数据  再把程序打开  查看结果</span><br><span class="line"></span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 0 0 18</span><br><span class="line">double_happy_offset 2 0 18</span><br><span class="line">double_happy_offset 1 0 24</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">为什么从0开始消费？？</span><br><span class="line">因为 </span><br><span class="line">&quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;   控制的 </span><br><span class="line">应该是 你offset保存在哪里  下次启动的时候去哪里取</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">测试：取出 offset 从redis</span><br><span class="line"></span><br><span class="line">object RedisOffsetApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line"></span><br><span class="line">    //TODO...从保存offset的地方 eg：redis 去获取已经提交的offset的记录信息</span><br><span class="line"></span><br><span class="line">    val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line">    val offsets: util.Map[String, String] = jedis.hgetAll(topics(0)+&quot;_&quot;+groupId)</span><br><span class="line"></span><br><span class="line">    import  scala.collection.JavaConversions._   //offsets 想使用  scala 里集合的map方法 要进行隐式转换 java-&gt; scala</span><br><span class="line"></span><br><span class="line">    offsets.map(x=&gt; &#123;</span><br><span class="line"></span><br><span class="line">      //  offsets map后要获取一种什么样的数据结构呢？ </span><br><span class="line"></span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">//  offsets map后要获取一种什么样的数据结构呢？ 之前KafkaUtils.createDirectStream 里面的消费策略</span><br><span class="line">Subscribe 传入 topics 和 kafkaParams  查看源码发现 </span><br><span class="line"></span><br><span class="line">  def Subscribe[K, V](</span><br><span class="line">      topics: Iterable[jl.String],</span><br><span class="line">      kafkaParams: collection.Map[String, Object],</span><br><span class="line">      offsets: collection.Map[TopicPartition, Long]): ConsumerStrategy[K, V] = &#123;</span><br><span class="line">    new Subscribe[K, V](</span><br><span class="line">      new ju.ArrayList(topics.asJavaCollection),</span><br><span class="line">      new ju.HashMap[String, Object](kafkaParams.asJava),</span><br><span class="line">      new ju.HashMap[TopicPartition, jl.Long](offsets.mapValues(l =&gt; new jl.Long(l)).asJava))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> offsets: collection.Map[TopicPartition, Long])    即  TopicPartition 和 偏移量  组成的这样的格式  Map[TopicPartition, Long]</span><br><span class="line"></span><br><span class="line">    public TopicPartition(String topic, int partition) &#123;</span><br><span class="line">        this.partition = partition;</span><br><span class="line">        this.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">所以取出 offset 从redis  取出的数据结构设计要符合 </span><br><span class="line">createDirectStream  里的 Subscribe 参数的数据结构</span><br><span class="line"></span><br><span class="line">(因为 取出 offset 从redis  是在  createDirectStream之前执行的 )</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">object RedisOffsetApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line">    val topics = Array(topic)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //TODO...从保存offset的地方 eg：redis 去获取已经提交的offset的记录信息</span><br><span class="line"></span><br><span class="line">    val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line">    val offsets: util.Map[String, String] = jedis.hgetAll(topics(0) + &quot;_&quot; + groupId)</span><br><span class="line"></span><br><span class="line">    var fromOffsets: Map[TopicPartition, Long] = Map[TopicPartition, Long]()</span><br><span class="line"></span><br><span class="line">    import scala.collection.JavaConversions._ //offsets 想使用  scala 里集合的map方法 要进行隐式转换 java-&gt; scala</span><br><span class="line">    offsets.map(x =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      //  offsets map后要获取一种什么样的数据结构呢？ offsets  Map[TopicPartition, Long]()</span><br><span class="line">      fromOffsets += new TopicPartition(topics(0), x._1.toInt) -&gt; x._2.toLong</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    fromOffsets.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">(double_happy_offset-0,18)</span><br><span class="line">(double_happy_offset-1,24)</span><br><span class="line">(double_happy_offset-2,18)</span><br><span class="line"></span><br><span class="line">取出 offset 从redis</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">所以我们再测试实时的代码  ：</span><br><span class="line">	我们先不提交 看看控制台 </span><br><span class="line">object StreamingKakfaDirectApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val groupId = &quot;double_happy_group&quot;</span><br><span class="line"></span><br><span class="line">    val topic = &quot;double_happy_offset&quot;</span><br><span class="line"></span><br><span class="line">    val kafkaParams = Map[String, Object](</span><br><span class="line">      &quot;bootstrap.servers&quot; -&gt; &quot;hadoop101:9092,hadoop101:9093,hadoop101:9094&quot;,   //Kafka地址</span><br><span class="line">      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],      //反序列化  接收端是反序列化   数据发送是要序列化</span><br><span class="line">      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],</span><br><span class="line">      &quot;group.id&quot; -&gt; groupId,</span><br><span class="line">      &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;,    //偏移量 从哪开始</span><br><span class="line">      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)  //自动提交么？ 选择不自动提交  手工来管理</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = Array(topic)</span><br><span class="line">    var fromOffsets: Map[TopicPartition, Long] = Map[TopicPartition, Long]()</span><br><span class="line"></span><br><span class="line">    //TODO...从保存offset的地方 eg：redis 去获取已经提交的offset的记录信息</span><br><span class="line">    val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line">    val offsets: util.Map[String, String] = jedis.hgetAll(topics(0)+&quot;_&quot;+groupId)</span><br><span class="line"></span><br><span class="line">     //offsets 想使用  scala 里集合的map方法 要进行隐式转换 java-&gt; scala</span><br><span class="line">    import scala.collection.JavaConversions._</span><br><span class="line">    offsets.map(x =&gt; &#123;</span><br><span class="line">      //offsets map后要获取一种什么样的数据结构呢？ offsets  Map[TopicPartition, Long]()</span><br><span class="line">      fromOffsets += new TopicPartition(topics(0), x._1.toInt) -&gt; x._2.toLong</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val stream = KafkaUtils.createDirectStream[String, String](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,  //数据尽量均匀分布到各个executor上去</span><br><span class="line">      Subscribe[String, String](topics, kafkaParams,fromOffsets)  //从已有的offset里读取数据 开始消费</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    //结果</span><br><span class="line">    stream.foreachRDD(rdd =&gt;&#123;   //这块的rdd一定要注意的  因为</span><br><span class="line"></span><br><span class="line">      if(!rdd.isEmpty())&#123;</span><br><span class="line"></span><br><span class="line">        //获取分区数</span><br><span class="line">        println(&quot;---------&quot;+rdd.partitions.size)   //这个值应该是3</span><br><span class="line"></span><br><span class="line">        //获取当前批次的offset数据</span><br><span class="line">        val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          println(s&quot;$&#123;x.topic&#125; $&#123;x.partition&#125; $&#123;x.fromOffset&#125; $&#123;x.untilOffset&#125;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        //TODO ... 处理业务逻辑 wc</span><br><span class="line"></span><br><span class="line">/*        //ToDO ... 提交Offset到Redis  使用第三种方式</span><br><span class="line">        val jedis: Jedis = RedisUtils.getJedis</span><br><span class="line"></span><br><span class="line">        offsetRanges.foreach(x=&gt;&#123;</span><br><span class="line">          val topicGroupId = x.topic + &quot;_&quot;+ groupId</span><br><span class="line">          jedis.hset(topicGroupId,x.partition+&quot;&quot;,x.untilOffset+&quot;&quot;)</span><br><span class="line">        &#125;)</span><br><span class="line">        jedis.close()*/</span><br><span class="line"></span><br><span class="line">      &#125;else&#123;</span><br><span class="line"></span><br><span class="line">        println(&quot;当前批次没有数据.....&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/11/01 17:14:51 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">说明ok  没有重新消费 </span><br><span class="line"></span><br><span class="line">那么我们再写一批数据 查看结果：</span><br><span class="line"></span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">当前批次没有数据.....</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 0 18 21</span><br><span class="line">double_happy_offset 1 24 28</span><br><span class="line">double_happy_offset 2 18 21</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">说明程序ok</span><br><span class="line"></span><br><span class="line">那么我们把实时程序停掉 再产生两次数据 再重启实时程序  对比最初的那次测试：</span><br><span class="line"></span><br><span class="line">还记得么？最初那次 是从0 开始消费的  那么这次测试结果呢？</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">---------3</span><br><span class="line">double_happy_offset 1 24 36</span><br><span class="line">double_happy_offset 0 18 27</span><br><span class="line">double_happy_offset 2 18 27</span><br><span class="line">当前批次没有数据.....</span><br><span class="line"></span><br><span class="line">终于ok了 (这是打印在控制台)</span><br><span class="line"></span><br><span class="line">那么写入redis offset 测试也是ok的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">虽然上面的东西一大坨 实际上思路很清晰 很简单 主要是几行破代码 </span><br><span class="line"></span><br><span class="line">而且大部分演示的都是不能用的 但是目的是让你知道 这些坑 而不是直接拿代码直接用 (了解原理之后可以 要不然之后出错了你都不知道怎么维护)</span><br></pre></td></tr></table></figure></div>
<p><strong>总结</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.  &quot;auto.offset.reset&quot; -&gt; &quot;earliest&quot;  </span><br><span class="line">  最终成品 这个参数设置选别的还是 这个 都无所谓的 </span><br><span class="line">  因为你的offsets 是走的 fromOffsets 你自己定义的那个 </span><br><span class="line">  	(就是把你们保存的offset 拿出来丢到 fromOffsets  这里(格式是重点)   创建流的时候 把fromOffsets 丢到消费策略那个参数里)</span><br><span class="line"></span><br><span class="line">2.业务处理前的第一件事是把偏移量拿到 (就是foreachRDD里面第一件事)</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SS02" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/20/SS02/">SS02</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/20/SS02/" class="article-date">
  <time datetime="2018-02-20T12:11:27.000Z" itemprop="datePublished">2018-02-20</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams" target="_blank" rel="noopener">Transformations on DStreams</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">updateStateByKey：</span><br><span class="line"></span><br><span class="line">先看一个案例</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,a,d,d</span><br><span class="line">a,a,a,d,d</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572519050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(d,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572519060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572519070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">这个计算 只计算当前批次的 之后批次 没有数据 </span><br><span class="line"></span><br><span class="line">需求：</span><br><span class="line">	统计 从现在时间点 到 10分钟之后的 a出现的次数  ？对于</span><br><span class="line">	上面的代码是无法满足 的    (也可以满足 存起来 再加 也可以)</span><br><span class="line"></span><br><span class="line">对于累计的需求该这么办呢？</span><br><span class="line"></span><br><span class="line">这就引出一个有没有状态的问题。</span><br><span class="line"></span><br><span class="line">状态：State</span><br><span class="line">    无状态的        只与当前批次有关的 叫无状态</span><br><span class="line">    有状态的        前后批次是有关系的   eg：需要把之前的历史到当前的时间点 需要累计起来</span><br><span class="line"></span><br><span class="line">实现有状态的 需求 使用updateStateByKey算子***</span><br><span class="line"></span><br><span class="line">updateStateByKey ：更新你的状态 通过key 来更新   怎么更新 传入一个function 即可 eg:累加 还是别的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">updateStateByKey(func)	：</span><br><span class="line">	Return a new &quot;state&quot; DStream where the state for each key is updated </span><br><span class="line">	by applying the given function on the previous state of the key </span><br><span class="line">	and the new values for the key. </span><br><span class="line">	This can be used to maintain arbitrary state data for each key.</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#updatestatebykey-operation" target="_blank" rel="noopener">UpdateStateByKey Operation</a></p>
<p>The updateStateByKey operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.</p>
<p>   1.Define the state - The state can be an arbitrary data type.</p>
<p>   2.Define the state update function - Specify with a function how to update the state<br>     using the <strong>previous state and the new values from an input stream.</strong></p>
<p>In every batch, Spark will apply the state update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.</p>
<p>Let’s illustrate this with an example. Say you want to maintain a running count of each word seen in a text data stream. Here, the running count is the state and it is an integer. We define the update function as:</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">updateStateByKey operation ：</span><br><span class="line">	1.Define the state</span><br><span class="line">	2.Define the state update function</span><br><span class="line"></span><br><span class="line">对于上面给的wc例子 ：</span><br><span class="line">哪个东西是state    应该是 value </span><br><span class="line"></span><br><span class="line">updateStateByKey  通过key 来更新谁 ( 你可以这么理解)</span><br></pre></td></tr></table></figure></div>
<p>案例：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. </span><br><span class="line"> val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .reduceByKey(_+_)</span><br><span class="line">      </span><br><span class="line">reduceByKey(_+_)  是对当前批次的累计 所以这里不能这么写</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *</span><br><span class="line">    * 1批次：  a a a d d</span><br><span class="line">    * 2批次：  b b b c c a</span><br><span class="line">    *</span><br><span class="line">    *newValues : 当前批次的值</span><br><span class="line">    *           key对应的新值(或者有新的key)  可能有多个 所以是一个Seq</span><br><span class="line">    * preValues : 以前批次的累加值</span><br><span class="line">    *             key已经存在的值  有可能没有 有可能有  所以定义成Option  有就返回some  没有返回none</span><br><span class="line">    *</span><br><span class="line">    */</span><br><span class="line">  def updateFunction(newValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    //newValues : (b,1)(b,1)(b,1)(c,1)(c,1) (a,1)</span><br><span class="line"></span><br><span class="line">    val curr = newValues.sum // 当前批次</span><br><span class="line">    val pre = preValues.getOrElse(0)   //老的值   (a,3) (d,2)   拿出值  key没有的  就赋值为0</span><br><span class="line">    Some(curr + pre)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/10/31 19:21:07 ERROR StreamingContext: Error starting the context, marking it as stopped</span><br><span class="line">java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().</span><br><span class="line">	at scala.Predef$.require(Predef.scala:224)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:243)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at scala.collection.immutable.List.foreach(List.scala:381)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph.start(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.startFirstTime(JobGenerator.scala:194)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:100)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:103)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:583)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01$.main(StreamingWCApp01.scala:19)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01.main(StreamingWCApp01.scala)</span><br><span class="line">19/10/31 19:21:08 WARN ReceiverSupervisorImpl: Skip stopping receiver because it has not yet stared</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().</span><br><span class="line">	at scala.Predef$.require(Predef.scala:224)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:243)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$validateAtStart$8.apply(DStream.scala:276)</span><br><span class="line">	at scala.collection.immutable.List.foreach(List.scala:381)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.validateAtStart(DStream.scala:276)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$4.apply(DStreamGraph.scala:51)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at org.apache.spark.streaming.DStreamGraph.start(DStreamGraph.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.startFirstTime(JobGenerator.scala:194)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobGenerator.start(JobGenerator.scala:100)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler.start(JobScheduler.scala:103)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply$mcV$sp(StreamingContext.scala:583)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext$$anonfun$liftedTree1$1$1.apply(StreamingContext.scala:578)</span><br><span class="line">	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:578)</span><br><span class="line">	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01$.main(StreamingWCApp01.scala:19)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp01.main(StreamingWCApp01.scala)</span><br><span class="line"></span><br><span class="line">Process finished with exit code 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Please set it by StreamingContext.checkpoint().</span><br></pre></td></tr></table></figure></div>
<p>修改代码</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a         第一次输入</span><br><span class="line"></span><br><span class="line">a,a,b,b,a         第二次输入</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line">    </span><br><span class="line">    ssc.checkpoint(&quot;file:///C:/IdeaProjects/spark/checkponit&quot;)</span><br><span class="line"></span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *</span><br><span class="line">    * 1批次：  a a a d d</span><br><span class="line">    * 2批次：  b b b c c a</span><br><span class="line">    *</span><br><span class="line">    *newValues : 当前批次的值</span><br><span class="line">    *           key对应的新值(或者有新的key)  可能有多个 所以是一个Seq</span><br><span class="line">    * preValues : 以前批次的累加值</span><br><span class="line">    *             key已经存在的值  有可能没有 有可能有  所以定义成Option  有就返回some  没有返回none</span><br><span class="line">    *</span><br><span class="line">    */</span><br><span class="line">  def updateFunction(newValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    //newValues : (b,1)(b,1)(b,1)(c,1)(c,1) (a,1)</span><br><span class="line"></span><br><span class="line">    val curr = newValues.sum // 当前批次</span><br><span class="line">    val pre = preValues.getOrElse(0)   //老的值   (a,3) (d,2)   拿出值  key没有的  就赋值为0</span><br><span class="line">    Some(curr + pre)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 19:24:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:24:13 WARN BlockManager: Block input-0-1572521053200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,2)</span><br><span class="line">(a,3)</span><br><span class="line"></span><br><span class="line">19/10/31 19:24:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:24:25 WARN BlockManager: Block input-0-1572521064800 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521080000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">为什么要checkpoint呢？</span><br><span class="line">之前的代码都是没有设置checkpoint 的 为什么之前不需要设置 呢？</span><br><span class="line">因为之前的是没有状态的 没有状态 就是当前批次处理完就ok了 </span><br><span class="line"></span><br><span class="line">但是现在 需要把当前批次 和 以前批次累加起来的  这个东西在哪里呢？下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031192844428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ok 现在我把程序关掉 重启以后 是多少呢？  </span><br><span class="line">之前值是：</span><br><span class="line">	(b,4)</span><br><span class="line">    (a,6)</span><br><span class="line">重启之后的值是：空的 </span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521460000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521470000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">也就是说 ：</span><br><span class="line">	如果你的作业 中途挂掉了 重启之后 什么都没有了 </span><br><span class="line">为什么呢？</span><br><span class="line">	因为之前的结果写到 checkponit里了 ，而且当前批次 也没有数据输入进来</span><br><span class="line">那么：</span><br><span class="line">	我们有什么办法 把 checkponit里的数据读取出来呢？</span><br><span class="line">	看官网</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">Checkpointing</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">最好直接看官网：我只是截取我认为重要的</span><br><span class="line">Spark Streaming needs to checkpoint enough information </span><br><span class="line">to a fault- tolerant storage system such that it can recover from failures.</span><br><span class="line"> There are two types of data that are checkpointed.</span><br><span class="line"></span><br><span class="line">1. a fault- tolerant storage system    可以选用HDFS</span><br><span class="line">2. two types of data that are checkpointed</span><br><span class="line">      1.Metadata checkpointing</span><br><span class="line">      			Configuration     配置文件</span><br><span class="line">      			DStream operations     算子 </span><br><span class="line">      			Incomplete batches    未完成的</span><br><span class="line">      2.Data checkpointing    就是你真正传过来的数据</span><br><span class="line"></span><br><span class="line">When to enable Checkpointing？</span><br><span class="line">   1.Usage of stateful transformations </span><br><span class="line">   2.Recovering from failures of the driver running the application </span><br><span class="line">   		driver挂了 你的作业就挂了 当你作业挂了 从Checkpoint中恢复</span><br><span class="line"></span><br><span class="line">How to configure Checkpointing？</span><br><span class="line">	看代码   就是说什么代码得改动 不能像之前那样写</span><br><span class="line">1.需要定义一个函数 这个函数就是 创建StreamingContext</span><br><span class="line">2.之后 再 val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _) </span><br><span class="line">才可以解决 重启之后能够拿到之前的值 </span><br><span class="line"></span><br><span class="line">这个就是利用了 ：</span><br><span class="line">	从Checkpoint中恢复 StreamingContext思想(driver 里的 )</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp02 &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  val checkpointDirectory = &quot;file:///C:/IdeaProjects/spark/checkponit&quot;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    // 当作业挂了时，从checkpoint中去获取StreamingContext</span><br><span class="line">    val ssc = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def functionToCreateContext(): StreamingContext = &#123;</span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line">    ssc.checkpoint(checkpointDirectory)</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_,1))</span><br><span class="line">      .updateStateByKey(updateFunction)</span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    *</span><br><span class="line">    * 1)  a a a d d</span><br><span class="line">    * 2)  b b b c c a</span><br><span class="line">    *</span><br><span class="line">    * @param newValues  当前批次的值</span><br><span class="line">    *        key对应的新值  可能有多个 所以是一个Seq</span><br><span class="line">    * @param preValues  以前批次的累加值</span><br><span class="line">    *        key已经存在的值  有可能没有 有可能有  所以定义成Option</span><br><span class="line">    * @return</span><br><span class="line">    */</span><br><span class="line">  def updateFunction(newValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    val curr = newValues.sum // 当前</span><br><span class="line">    val pre = preValues.getOrElse(0)</span><br><span class="line">    Some(curr + pre)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">	-------------------------------------------</span><br><span class="line">Time: 1572521710000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521720000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521730000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521740000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521750000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572521760000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">为什么呢？因为 我改动代码了 虽然 checkpoint目录没有变 </span><br><span class="line">先把之前的 checkpoint 目录删掉 再测试 (第一次 之后关闭程序 再重启)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a    第一次输入 </span><br><span class="line">a,a,b,b,a</span><br><span class="line"></span><br><span class="line">a,a,b,b,a   第二次输入</span><br><span class="line">a,a,b,b,a</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">结果：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523040000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,6)</span><br><span class="line"></span><br><span class="line">19/10/31 19:57:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:57:25 WARN BlockManager: Block input-0-1572523045400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 19:57:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 19:57:26 WARN BlockManager: Block input-0-1572523046200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">重启后的结果：</span><br><span class="line">	-------------------------------------------</span><br><span class="line">Time: 1572523070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523080000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523090000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523100000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523110000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523120000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523130000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523140000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523150000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572523160000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,12)</span><br><span class="line"></span><br><span class="line">ok啦</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Stream + Kafka == CP</span><br><span class="line">Kafka 的offset肯定是需要手工维护：有哪些呢？很多的 </span><br><span class="line">    1.checkpoint： 就是把offset维护在checkponit里面的    </span><br><span class="line">    	(代码不能发生任何的变化   只要你代码发生了变化 就意味着 checkpoint 的 matadata 发生了变化  )</span><br><span class="line">    2.Kafka     </span><br><span class="line">    3.ZK   </span><br><span class="line">    4.MySQL    </span><br><span class="line">    5.Redis</span><br><span class="line"></span><br><span class="line">所以生产上 checkpoint 根本没法用  (你的代码怎么可能不变呢？或者不修改呢？所以用不了 )</span><br></pre></td></tr></table></figure></div>

<p><strong>把数据写出去： ****</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">foreachRDD：</span><br><span class="line"></span><br><span class="line">foreachRDD(func)：</span><br><span class="line">	The most generic output operator that applies a function, func, to each RDD </span><br><span class="line">	generated from the stream. </span><br><span class="line">	This function should push the data in each RDD to an external system, </span><br><span class="line">	such as saving the RDD to files, </span><br><span class="line">	or writing it over the network to a database. Note that the function func</span><br><span class="line">	 is executed in the driver process running the streaming application,</span><br><span class="line">	 and will usually have RDD actions in it</span><br><span class="line">	  that will force the computation of the streaming RDDs.</span><br><span class="line"></span><br><span class="line">1. such as saving the RDD to files, </span><br><span class="line">	or writing it over the network to a database.</span><br><span class="line">2.闭包  优雅的方式写出去</span><br><span class="line">3.the function func</span><br><span class="line">	 is executed in the driver process </span><br><span class="line">	 running the streaming application</span><br><span class="line">	 func是运行在driver process的</span><br><span class="line"></span><br><span class="line">driver端到executor端 必然涉及到一个序列化的问题</span><br></pre></td></tr></table></figure></div>
<p><strong>把数据写到MySQL</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MySQL底层引擎有几种？各自什么区别？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019103120380049.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>咱们一步一步来 由劣到优</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    result.foreachRDD( rdd =&gt;&#123;</span><br><span class="line">      val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line"></span><br><span class="line">      rdd.foreach(pair =&gt;&#123;</span><br><span class="line">        val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">        connection.createStatement().execute(sql)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      MySQLUtils.closeResource(connection)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">19/10/31 20:43:30 ERROR JobScheduler: Error running job streaming job 1572525810000 ms.0</span><br><span class="line">org.apache.spark.SparkException: Task not serializable</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:403)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:393)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)</span><br><span class="line">	at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:926)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:925)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:32)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:29)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.NotSerializableException: java.lang.Object</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: java.lang.Object, value: java.lang.Object@4ffd7c3f)</span><br><span class="line">	- writeObject data (class: java.util.HashMap)</span><br><span class="line">	- object (class java.util.HashMap, &#123;UTF-8=java.lang.Object@4ffd7c3f, US-ASCII=com.mysql.jdbc.SingleByteCharsetConverter@53c22208, WINDOWS-1252=com.mysql.jdbc.SingleByteCharsetConverter@77cd4c6d&#125;)</span><br><span class="line">	- field (class: com.mysql.jdbc.ConnectionImpl, name: charsetConverterMap, type: interface java.util.Map)</span><br><span class="line">	- object (class com.mysql.jdbc.JDBC4Connection, com.mysql.jdbc.JDBC4Connection@65b0d4df)</span><br><span class="line">	- field (class: com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, name: connection$1, type: interface java.sql.Connection)</span><br><span class="line">	- object (class com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, &lt;function1&gt;)</span><br><span class="line">	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:400)</span><br><span class="line">	... 30 more</span><br><span class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Task not serializable</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:403)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:393)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)</span><br><span class="line">	at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:926)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:925)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">	at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:32)</span><br><span class="line">	at com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1.apply(StreamingWCApp03.scala:29)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)</span><br><span class="line">	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)</span><br><span class="line">	at scala.util.Try$.apply(Try.scala:192)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)</span><br><span class="line">	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.NotSerializableException: java.lang.Object</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: java.lang.Object, value: java.lang.Object@4ffd7c3f)</span><br><span class="line">	- writeObject data (class: java.util.HashMap)</span><br><span class="line">	- object (class java.util.HashMap, &#123;UTF-8=java.lang.Object@4ffd7c3f, US-ASCII=com.mysql.jdbc.SingleByteCharsetConverter@53c22208, WINDOWS-1252=com.mysql.jdbc.SingleByteCharsetConverter@77cd4c6d&#125;)</span><br><span class="line">	- field (class: com.mysql.jdbc.ConnectionImpl, name: charsetConverterMap, type: interface java.util.Map)</span><br><span class="line">	- object (class com.mysql.jdbc.JDBC4Connection, com.mysql.jdbc.JDBC4Connection@65b0d4df)</span><br><span class="line">	- field (class: com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, name: connection$1, type: interface java.sql.Connection)</span><br><span class="line">	- object (class com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, &lt;function1&gt;)</span><br><span class="line">	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)</span><br><span class="line">	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)</span><br><span class="line">	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:400)</span><br><span class="line">	... 30 more</span><br><span class="line">19/10/31 20:43:30 WARN SocketReceiver: Error receiving data</span><br><span class="line">java.net.SocketException: Socket closed</span><br><span class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)</span><br><span class="line">	at java.io.InputStreamReader.read(InputStreamReader.java:184)</span><br><span class="line">	at java.io.BufferedReader.fill(BufferedReader.java:161)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:324)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:389)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)</span><br><span class="line">	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)</span><br><span class="line">19/10/31 20:43:30 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data</span><br><span class="line">java.net.SocketException: Socket closed</span><br><span class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)</span><br><span class="line">	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)</span><br><span class="line">	at java.io.InputStreamReader.read(InputStreamReader.java:184)</span><br><span class="line">	at java.io.BufferedReader.fill(BufferedReader.java:161)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:324)</span><br><span class="line">	at java.io.BufferedReader.readLine(BufferedReader.java:389)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:121)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.getNext(SocketInputDStream.scala:119)</span><br><span class="line">	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)</span><br><span class="line">	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.run(SocketInputDStream.scala:72)</span><br><span class="line">19/10/31 20:43:30 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver</span><br><span class="line">19/10/31 20:43:30 WARN ReceiverSupervisorImpl: Receiver has been stopped</span><br><span class="line">Exception in thread &quot;receiver-supervisor-future-0&quot; java.lang.Error: java.lang.InterruptedException: sleep interrupted</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.lang.InterruptedException: sleep interrupted</span><br><span class="line">	at java.lang.Thread.sleep(Native Method)</span><br><span class="line">	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply$mcV$sp(ReceiverSupervisor.scala:196)</span><br><span class="line">	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)</span><br><span class="line">	at org.apache.spark.streaming.receiver.ReceiverSupervisor$$anonfun$restartReceiver$1.apply(ReceiverSupervisor.scala:189)</span><br><span class="line">	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)</span><br><span class="line">	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	... 2 more</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	org.apache.spark.SparkException: Task not serializable</span><br><span class="line">	ClosureCleaner    Closure 闭包的意思</span><br><span class="line">根本原因是：</span><br><span class="line">	Caused by: java.io.NotSerializableException: java.lang.Object</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: java.lang.Object, value: java.lang.Object@4ffd7c3f)</span><br><span class="line">	- writeObject data (class: java.util.HashMap)</span><br><span class="line">	- object (class java.util.HashMap, &#123;UTF-8=java.lang.Object@4ffd7c3f, US-ASCII=com.mysql.jdbc.SingleByteCharsetConverter@53c22208, WINDOWS-1252=com.mysql.jdbc.SingleByteCharsetConverter@77cd4c6d&#125;)</span><br><span class="line">	- field (class: com.mysql.jdbc.ConnectionImpl, name: charsetConverterMap, type: interface java.util.Map)</span><br><span class="line">	- object (class com.mysql.jdbc.JDBC4Connection, com.mysql.jdbc.JDBC4Connection@65b0d4df)</span><br><span class="line">	- field (class: com.ruozedata.spark.ss02.StreamingWCApp03$$anonfun$main$1$$anonfun$apply$1, name: connection$1, type: interface java.sql.Connection)</span><br><span class="line"></span><br><span class="line">就是 object not serializable ：com.mysql.jdbc.SingleByteCharsetConverter </span><br><span class="line">MySQL的驱动不能序列化    但是事实上 MySQL驱动就是序列化不了 </span><br><span class="line"></span><br><span class="line">该怎么办呢？ 看官网  下图</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031205307726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线————————————————————————————————————</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">上面的错误明白之后 那么什么叫做闭包？</span><br><span class="line">先看一下官网  RDD篇介绍的</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-" target="_blank" rel="noopener">Understanding closures</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">闭包：在函数内部 引用了一个外部的变量 </span><br><span class="line">eg： 这两行代码</span><br><span class="line"></span><br><span class="line">     val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line">      rdd.foreach(pair =&gt;&#123;</span><br><span class="line">        val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">        connection.createStatement().execute(sql)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">foreach 内部使用了 connection  而connection 是在foreach的外部</span><br><span class="line"></span><br><span class="line">如果 假设哈 connection 可以序列化 的  上面这种写法是没有问题的！！！</span><br><span class="line">很不幸 connection objects are rarely transferable across machines</span><br></pre></td></tr></table></figure></div>
<p><strong>修改：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">    result.foreachRDD( rdd =&gt;&#123;</span><br><span class="line">      rdd.foreach(pair =&gt;&#123;</span><br><span class="line">        val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line">        val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">        connection.createStatement().execute(sql)</span><br><span class="line">        MySQLUtils.closeResource(connection)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">connection放到里面去  那么还涉及闭包的问题么？</span><br><span class="line">一定没有闭包的问题了 避免了上次测试 出现的闭包问题</span><br><span class="line"></span><br><span class="line">运行结果：</span><br><span class="line">没有日志的 因为 foreachRDD 是没有返回值的  只能查看MySQL数据了</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------------------</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">Empty set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |    8 |</span><br><span class="line">| a    |   12 |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line">说明写也是ok的 </span><br><span class="line"></span><br><span class="line">但是也有个问题的？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031210724578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>优化</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">        val connection: Connection = MySQLUtils.getConnection()</span><br><span class="line"></span><br><span class="line">        partition.foreach(pair =&gt; &#123;</span><br><span class="line">          val sql = s&quot;insert into wc(word,cnt) values(&apos;$&#123;pair._1&#125;&apos;, $&#123;pair._2&#125;)&quot;</span><br><span class="line">          connection.createStatement().execute(sql)</span><br><span class="line">        &#125;)</span><br><span class="line">        MySQLUtils.closeResource(connection)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |    8 |</span><br><span class="line">| a    |   12 |</span><br><span class="line">| b    |    4 |</span><br><span class="line">| a    |    6 |</span><br><span class="line">+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line">这种方式比前面的好多了 但是也不行 </span><br><span class="line">分区多了  connection也会多  </span><br><span class="line">那么最好的方式是什么呢？拿一个连接池 用完之后返回回去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031211641822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">正确的写法会写了 但是 </span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |    8 |</span><br><span class="line">| a    |   12 |</span><br><span class="line">| b    |    4 |</span><br><span class="line">| a    |    6 |</span><br><span class="line">+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line">结果咋整 写了两次就这样了  怎么解决呢？这是数据问题</span><br></pre></td></tr></table></figure></div>
<p><strong>还有一种写法  建议使用它</strong></p>
<p>scalikejdbc 自带Connection Pool<br><img src="https://img-blog.csdnimg.cn/20191031213004342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    </span><br><span class="line">    DBs.setupAll() //这样就把配置文件解析出来了</span><br><span class="line">    result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line"></span><br><span class="line">      rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">      </span><br><span class="line">        partition.foreach(pair =&gt; &#123;</span><br><span class="line">          DB.autoCommit &#123; implicit session =&gt; &#123;</span><br><span class="line">            SQL(&quot;insert into wc(word,cnt) values(?, ?)&quot;)</span><br><span class="line">              .bind(pair._1,pair._2)</span><br><span class="line">              .update().apply()</span><br><span class="line">          &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">mysql&gt; select * from wc;</span><br><span class="line">+------+------+</span><br><span class="line">| word | cnt  |</span><br><span class="line">+------+------+</span><br><span class="line">| b    |   20 |</span><br><span class="line">| a    |   30 |</span><br><span class="line">+------+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">你确定scalikejdbc 默认就使用 连接池么？？？ 留一个坑</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">之前 我们用state 进行累计的  </span><br><span class="line">因为用state累加 会用到checkpoint   checkpoint自己生成小文件一大堆  等等</span><br><span class="line"></span><br><span class="line">那么 不用state 能不能累加？</span><br><span class="line">用redis </span><br><span class="line"> /**</span><br><span class="line">      * WC这种统计维度来说</span><br><span class="line">      * Redis的使用关键点：如何选择合适的数据类型</span><br><span class="line">      */</span><br><span class="line">这里我们选hash</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // transformation</span><br><span class="line">    val result = lines.flatMap(_.split(&quot;,&quot;)).map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    /**</span><br><span class="line">      * WC这种统计维度来说</span><br><span class="line">      * Redis的使用关键点：如何选择合适的数据类型</span><br><span class="line">      */</span><br><span class="line">        result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">          rdd.foreachPartition(partition =&gt; &#123;</span><br><span class="line">            val jedis = RedisUtils.getJedis  // 获取Redis连接</span><br><span class="line">            partition.foreach(pair =&gt; &#123;</span><br><span class="line">              jedis.hincrBy(&quot;doublehappy_redis_wc&quot;, pair._1, pair._2)   //String key, String field, long value</span><br><span class="line">            &#125;)</span><br><span class="line">            jedis.close() // free</span><br><span class="line">          &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">扩展：</span><br><span class="line">	这里是连接redis  ，那么连接 phoneix 、Cassandra   都一样的 </span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">2) &quot;doublehappy_redis_wc&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031220146737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">再放一些数据</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br><span class="line">a,a,b,b,a</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031220239500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>说明结果ok的哈</p>
<p><strong>transform</strong><br><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#transform-operation" target="_blank" rel="noopener">transform</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">transform(func)	;</span><br><span class="line">Return a new DStream by applying a RDD-to-RDD function </span><br><span class="line">to every RDD of the source DStream.</span><br><span class="line"> This can be used to do arbitrary RDD operations on the DStream.</span><br><span class="line"></span><br><span class="line">之前的编程都是基于DStream</span><br><span class="line">    /**</span><br><span class="line">      * 现在的编程都是基于DStream    生产上绝大多数是DStream</span><br><span class="line">      *</span><br><span class="line">      但是 </span><br><span class="line">      * DStream与RDD互操作咋整？ 使用transform</span><br><span class="line">      */</span><br><span class="line"></span><br><span class="line">好处就是 把DStream  的RDD 跟我们的RDD进行操作</span><br><span class="line"></span><br><span class="line">需求：</span><br><span class="line">     * 流处理的时候，有一个数据来源于文本或者是其他的   这部分数据是 RDD</span><br><span class="line">      * 另外一个数据是来自Kafka、或者其他的数据源 这部分数据是 DStream</span><br><span class="line">      </span><br><span class="line">做这两个关联  你需要用到 transform</span><br></pre></td></tr></table></figure></div>
<p><strong>例子</strong><br>黑名单<br>目的：<br>只要由黑名单里的东西 把黑名单的数据全部过滤掉 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">先用core的方式;</span><br><span class="line">object CoreBlackListApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val sc = ContextUtils.getSparkContext(&quot;CoreBlackListApp&quot;)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 构建黑名单  (xx, true) 或者  (xx, 1)</span><br><span class="line">      */</span><br><span class="line">    val blacks = new ListBuffer[(String,Boolean)]()</span><br><span class="line">    blacks.append((&quot;苍老师&quot;,true))  // 鉴黄</span><br><span class="line">    val blacksRDD = sc.parallelize(blacks)  // 把数据转成RDD</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 构建访问日志</span><br><span class="line">      */</span><br><span class="line">    val input = new ListBuffer[(String,String)]</span><br><span class="line">    input.append((&quot;历史第一人&quot;,&quot;被小卡干了，000000&quot;))</span><br><span class="line">    input.append((&quot;日天&quot;,&quot;也被小卡干了，111111&quot;))</span><br><span class="line">    input.append((&quot;苍老师&quot;,&quot;我们敬爱的老师，111111&quot;))</span><br><span class="line">    val inputRDD = sc.parallelize(input)</span><br><span class="line"></span><br><span class="line">    //TODO... 想从访问日志中过滤掉“苍老师”的数据</span><br><span class="line">    inputRDD.leftOuterJoin(blacksRDD)</span><br><span class="line">      .filter(_._2._2.getOrElse(false) != true)</span><br><span class="line">      .map(x =&gt;(x._1, x._2._1))</span><br><span class="line">      .printInfo()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(日天,也被小卡干了，111111)</span><br><span class="line">(历史第一人,被小卡干了，000000)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">ssc：很重要 </span><br><span class="line">生产上用的很多  生产上统计结果有些数据 有些是MySQL里的直接拿的 </span><br><span class="line"></span><br><span class="line">object StreamingWCApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input:   socket  Input DStream</span><br><span class="line">    val lines = ssc.socketTextStream(&quot;hadoop101&quot;, 9999)</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 构建黑名单  (xx, true)  (xx, 1)</span><br><span class="line">      */</span><br><span class="line">    val blacks = new ListBuffer[(String,Boolean)]()</span><br><span class="line">    blacks.append((&quot;canglaoshi&quot;,true))  // 鉴黄</span><br><span class="line">    val blacksRDD = ssc.sparkContext.parallelize(blacks)  // 把数据转成RDD</span><br><span class="line"></span><br><span class="line">    // &quot;日天&quot;,&quot;也被小卡干了，111111&quot;</span><br><span class="line">    lines.map(x =&gt; (x.split(&quot;,&quot;)(0), x))</span><br><span class="line">      .transform(rdd =&gt; &#123;</span><br><span class="line">        rdd.leftOuterJoin(blacksRDD)</span><br><span class="line">          .filter(_._2._2.getOrElse(false) != true)</span><br><span class="line">          .map(x=&gt;x._2._1)</span><br><span class="line">      &#125;).print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">canglaoshi,xxooll</span><br><span class="line">longlaoshi,11oooxxx</span><br><span class="line">james,xxxxx</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572533110000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572533120000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">james,xxxxx</span><br><span class="line">longlaoshi,11oooxxx</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572533130000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果正确 过滤掉 canglaoshi的数据</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SS01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/17/SS01/">SS01</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/17/SS01/" class="article-date">
  <time datetime="2018-02-17T12:10:43.000Z" itemprop="datePublished">2018-02-17</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Fault Tolerance：</span><br><span class="line">Stateful exactly-once semantics out of the box.</span><br><span class="line">Spark Streaming recovers both lost work and </span><br><span class="line">operator state (e.g. sliding windows) out of the box, </span><br><span class="line">without any extra code on your part.</span><br><span class="line"></span><br><span class="line">注意：容错机制</span><br><span class="line">1.recovers  lost executor</span><br><span class="line">2.operator state</span><br><span class="line"></span><br><span class="line">Spark Integration：整合</span><br><span class="line">By running on Spark, Spark Streaming lets you reuse the same code </span><br><span class="line">for batch processing, join streams against historical data,</span><br><span class="line"> or run ad-hoc queries on stream state. </span><br><span class="line"> Build powerful interactive applications, not just analytics.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">流处理</span><br><span class="line">  实时：Storm Flink   event (就是来一条数据处理一个  这是真实时)</span><br><span class="line">   近实时：Spark Streaming   mini-batch  </span><br><span class="line">   		Spark Streaming把过来的数据切割成5s 一个批次  (是小微批次处理 不是真的实时 )</span><br><span class="line">   		Spark Streaming对数据的处理是使用小批处理</span><br><span class="line"></span><br><span class="line">批处理：一次性处理某个批次的数据     数据是有始有终(有开始有结束 有头有尾的)</span><br><span class="line">	eg：处理某个文件夹下面数据 处理完就ok了  不可能跑到别的文件夹下面  (可以这么理解)</span><br><span class="line"></span><br><span class="line">流处理 ： 流氓的流 流是一直不断的 </span><br><span class="line">	eg：水龙头打开了 水一直流     不流水了说明 水龙头坏了 或者 没水了 </span><br><span class="line">	</span><br><span class="line">你们的生产上面的实时性是多高呢？</span><br><span class="line">Spark Streaming 可以做到0.5s  </span><br><span class="line">你要注意 0.5s 能进来多少数据</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#overview" target="_blank" rel="noopener">官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1. Spark Streaming is an extension of the core Spark API </span><br><span class="line"></span><br><span class="line">2.Data can be ingested from many sources </span><br><span class="line">like Kafka, Flume, Kinesis, or TCP sockets,</span><br><span class="line"> and can be processed using complex algorithms expressed </span><br><span class="line"> with high-level functions like map, reduce, join and window.</span><br><span class="line"></span><br><span class="line">数据源：</span><br><span class="line"> Kafka *****  流处理引擎+Kafka  CP</span><br><span class="line"> Flume ==&gt; 流处理引擎   可以的用的  但是 没有缓冲 </span><br><span class="line"> HDFS</span><br><span class="line"> TCP sockets ==&gt; 测试  + 电信运营商(他们用 早期的时候 15年)</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031103920758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ss：</span><br><span class="line">	Input： Kafka Socket</span><br><span class="line">    Transform：业务逻辑处理</span><br><span class="line">    Output：</span><br><span class="line"></span><br><span class="line">Spark Streaming ：他有几件事情</span><br><span class="line">    1）receives live input data streams   接受数据</span><br><span class="line">    2）divides the data into batches         把接受到的数据 拆分成batches</span><br><span class="line">比如说 ：</span><br><span class="line">	1.Spark Streaming 5秒中处理一次数据   5s时间到了  </span><br><span class="line">	2. 那么会把5s中接受的数据 把它切成 batch </span><br><span class="line">    3. 之后 把batch 交给 sparkEngine 处理  </span><br><span class="line">    4. 处理完的结果也是 batch</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031104829799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming 的编程模型：</span><br><span class="line">	DStream</span><br><span class="line">        which represents a continuous stream of data</span><br><span class="line"></span><br><span class="line">理解不了看源码：</span><br><span class="line">/**</span><br><span class="line"> * A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous</span><br><span class="line"> * sequence of RDDs (of the same type) representing a continuous stream of data (see</span><br><span class="line"> * org.apache.spark.rdd.RDD in the Spark core documentation for more details on RDDs).</span><br><span class="line"> * DStreams can either be created from live data (such as, data from TCP sockets, Kafka, Flume,</span><br><span class="line"> * etc.) using a [[org.apache.spark.streaming.StreamingContext]] or it can be generated by</span><br><span class="line"> * transforming existing DStreams using operations such as `map`,</span><br><span class="line"> * `window` and `reduceByKeyAndWindow`. While a Spark Streaming program is running, each DStream</span><br><span class="line"> * periodically generates a RDD, either from live data or by transforming the RDD generated by a</span><br><span class="line"> * parent DStream.</span><br><span class="line"> *</span><br><span class="line"> * This class contains the basic operations available on all DStreams, such as `map`, `filter` and</span><br><span class="line"> * `window`. In addition, [[org.apache.spark.streaming.dstream.PairDStreamFunctions]] contains</span><br><span class="line"> * operations available only on DStreams of key-value pairs, such as `groupByKeyAndWindow` and</span><br><span class="line"> * `join`. These operations are automatically available on any DStream of pairs</span><br><span class="line"> * (e.g., DStream[(Int, Int)] through implicit conversions.</span><br><span class="line"> *</span><br><span class="line"> * A DStream internally is characterized by a few basic properties:</span><br><span class="line"> *  - A list of other DStreams that the DStream depends on</span><br><span class="line"> *  - A time interval at which the DStream generates an RDD</span><br><span class="line"> *  - A function that is used to generate an RDD after each time interval</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">abstract class DStream[T: ClassTag] (</span><br><span class="line">    @transient private[streaming] var ssc: StreamingContext</span><br><span class="line">  ) extends Serializable with Logging &#123;</span><br><span class="line"></span><br><span class="line">注意：跟RDD 差不多 </span><br><span class="line">StreamingContext：就是流处理的上下文模型</span><br><span class="line">DStream ： is a continuous sequence of RDDs</span><br><span class="line">  就是一个流进来 按照时间批次(就是几秒一批次) 被拆成一个一个的RDD</span><br><span class="line">  DStream 由一串RDD构成  我们处理的时候 是以 RDD为单位进行处理的 </span><br><span class="line">  底层就是sparkcore </span><br><span class="line"></span><br><span class="line">DStream 这么来的呢？ 跟RDD一样 (看注释)</span><br><span class="line">	1.live data</span><br><span class="line">	2.别的DStream 转换来的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">This class contains the basic operations available on all DStreams：</span><br><span class="line">看看有多少operations</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031110110617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/201910311101467.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">所以RDD算子一点要熟练掌握</span><br><span class="line"></span><br><span class="line">特性三个：</span><br><span class="line">	1.A list of other DStreams that the DStream depends on    </span><br><span class="line">		</span><br><span class="line">	2.A time interval at which the DStream generates an RDD</span><br><span class="line">		   时间间隔产生rdd     也就是  每隔多少时间处理一次</span><br><span class="line">	3. A function that is used to generate an RDD after each time interval</span><br><span class="line">	       	因为你一个DStream 由一堆RDD构成 是有顺序的</span><br><span class="line">	       	最终 你对DStream 做操作 其实就是对RDD做操作 </span><br><span class="line">	       	对RDD做操作 就是对 RDD里的每一个元素做操作</span><br></pre></td></tr></table></figure></div>

<p><strong>案列代码准备</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">StreamingContext  有好多附属构造器的 </span><br><span class="line">class StreamingContext private[streaming] (</span><br><span class="line">    _sc: SparkContext,</span><br><span class="line">    _cp: Checkpoint,</span><br><span class="line">    _batchDur: Duration</span><br><span class="line">  ) extends Logging &#123;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create a StreamingContext using an existing SparkContext.</span><br><span class="line">   * @param sparkContext existing SparkContext</span><br><span class="line">   * @param batchDuration the time interval at which streaming data will be divided into batches</span><br><span class="line">   */</span><br><span class="line">  def this(sparkContext: SparkContext, batchDuration: Duration) = &#123;</span><br><span class="line">    this(sparkContext, null, batchDuration)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create a StreamingContext by providing the configuration necessary for a new SparkContext.</span><br><span class="line">   * @param conf a org.apache.spark.SparkConf object specifying Spark parameters</span><br><span class="line">   * @param batchDuration the time interval at which streaming data will be divided into batches</span><br><span class="line">   */</span><br><span class="line">  def this(conf: SparkConf, batchDuration: Duration) = &#123;</span><br><span class="line">    this(StreamingContext.createNewSparkContext(conf), null, batchDuration)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">我们选择 this(conf: SparkConf, batchDuration: Duration)   </span><br><span class="line"></span><br><span class="line">case class Duration (private val millis: Long)    单位是millis  </span><br><span class="line"></span><br><span class="line">你传Duration 太死板了想穿个秒数 还得算  看看有没有封装好的 </span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Helper object that creates instance of [[org.apache.spark.streaming.Duration]] representing</span><br><span class="line"> * a given number of seconds.</span><br><span class="line"> */</span><br><span class="line">object Seconds &#123;</span><br><span class="line">  def apply(seconds: Long): Duration = new Duration(seconds * 1000)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">封装一个工具类：</span><br><span class="line">object ContextUtils &#123;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 获取sc</span><br><span class="line">    */</span><br><span class="line">  def getSparkContext(appname:String,defalut:String = &quot;local[2]&quot;): SparkContext = &#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">    new SparkContext(sparkConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 获取ssc</span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">  def getStreamingContext(appname:String,batch:Int,defalut:String = &quot;local&quot;) =&#123;</span><br><span class="line"></span><br><span class="line">    val sparkConf: SparkConf = new SparkConf().setAppName(appname).setMaster(defalut)</span><br><span class="line"></span><br><span class="line">    new StreamingContext(sparkConf,Seconds(batch))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">object AppName &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    println(this.getClass.getName)    //包名+类名</span><br><span class="line">    println(this.getClass.getSimpleName)    //类名</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">com.ruozedata.spark.ss01.AppName$         </span><br><span class="line">AppName$</span><br></pre></td></tr></table></figure></div>
<p><strong>案例</strong><br>socket：<br><img src="https://img-blog.csdnimg.cn/20191031112414892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有三个 ：用哪个呢？有什么区别呢？看下面</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">数据源：socket </span><br><span class="line"> /**</span><br><span class="line">   * Creates an input stream from TCP source hostname:port. Data is received using</span><br><span class="line">   * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\n` delimited</span><br><span class="line">   * lines.</span><br><span class="line">   * @param hostname      Hostname to connect to for receiving data</span><br><span class="line">   * @param port          Port to connect to for receiving data</span><br><span class="line">   * @param storageLevel  Storage level to use for storing the received objects</span><br><span class="line">   *                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)</span><br><span class="line">   * @see [[socketStream]]</span><br><span class="line">   */</span><br><span class="line">  def socketTextStream(</span><br><span class="line">      hostname: String,</span><br><span class="line">      port: Int,</span><br><span class="line">      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2</span><br><span class="line">    ): ReceiverInputDStream[String] = withNamedScope(&quot;socket text stream&quot;) &#123;</span><br><span class="line">    socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Creates an input stream from TCP source hostname:port. Data is received using</span><br><span class="line">   * a TCP socket and the receive bytes it interpreted as object using the given</span><br><span class="line">   * converter.</span><br><span class="line">   * @param hostname      Hostname to connect to for receiving data</span><br><span class="line">   * @param port          Port to connect to for receiving data</span><br><span class="line">   * @param converter     Function to convert the byte stream to objects</span><br><span class="line">   * @param storageLevel  Storage level to use for storing the received objects</span><br><span class="line">   * @tparam T            Type of the objects received (after converting bytes to objects)</span><br><span class="line">   */</span><br><span class="line">  def socketStream[T: ClassTag](</span><br><span class="line">      hostname: String,</span><br><span class="line">      port: Int,</span><br><span class="line">      converter: (InputStream) =&gt; Iterator[T],</span><br><span class="line">      storageLevel: StorageLevel</span><br><span class="line">    ): ReceiverInputDStream[T] = &#123;</span><br><span class="line">    new SocketInputDStream[T](this, hostname, port, converter, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	socketTextStream </span><br><span class="line">		底层调用的是 socketStream</span><br><span class="line">		socketStream底层调用的是 SocketInputDStream</span><br><span class="line">    socketStream</span><br><span class="line">	底层调用的是 SocketInputDStream</span><br><span class="line"></span><br><span class="line">socketTextStream 和socketStream 就是入参不一样 用起来是一样的 </span><br><span class="line">那么SocketInputDStream：</span><br><span class="line"></span><br><span class="line">class SocketInputDStream[T: ClassTag](</span><br><span class="line">    _ssc: StreamingContext,</span><br><span class="line">    host: String,</span><br><span class="line">    port: Int,</span><br><span class="line">    bytesToObjects: InputStream =&gt; Iterator[T],</span><br><span class="line">    storageLevel: StorageLevel</span><br><span class="line">  ) extends ReceiverInputDStream[T]</span><br><span class="line"></span><br><span class="line">都是ReceiverInputDStream 这个 ******</span><br><span class="line"></span><br><span class="line">StorageLevel默认的是 MEMORY_AND_DISK_SER_2  </span><br><span class="line">跟sparkcore里是不一样的  为什么是2呢 ？</span><br></pre></td></tr></table></figure></div>

<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example" target="_blank" rel="noopener">A Quick Example</a></p>
<p>测试：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b</span><br></pre></td></tr></table></figure></div>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line">    // Input DStream</span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line"></span><br><span class="line">   //transformation</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // output</span><br><span class="line">    result.print()</span><br><span class="line">    </span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">19/10/31 11:35:31 WARN StreamingContext: spark.master should be set as local[n], n &gt; 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.</span><br><span class="line">19/10/31 11:35:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:35:40 WARN BlockManager: Block input-0-1572492940600 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:35:44 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:35:44 WARN BlockManager: Block input-0-1572492944000 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为什么没有数据？</span><br><span class="line">先把 master local  改成 local[2] 再测试</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b        这是我第一次测试</span><br><span class="line">                  第二次测试   第一批次</span><br><span class="line">a,a,a,a         </span><br><span class="line">b,b,b,b</span><br><span class="line"></span><br><span class="line">                   第二批次 </span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b</span><br><span class="line">a,a,a,a</span><br><span class="line">b,b,b,b</span><br><span class="line"></span><br><span class="line">结果是 ：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493040000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 11:37:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:24 WARN BlockManager: Block input-0-1572493044200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:37:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:25 WARN BlockManager: Block input-0-1572493045000 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493050000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,4)</span><br><span class="line">(a,4)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493060000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 11:37:42 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:42 WARN BlockManager: Block input-0-1572493062600 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:37:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:43 WARN BlockManager: Block input-0-1572493062800 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 11:37:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 11:37:43 WARN BlockManager: Block input-0-1572493063400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572493070000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,8)</span><br><span class="line">(a,8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.</span><br><span class="line"> 上面的代码是处理  当前批次的</span><br><span class="line"> 不是求累加批次的 累加是另外的算子</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">After a context is defined, you have to do the following.</span><br><span class="line"></span><br><span class="line">    1.Define the input sources by creating input DStreams.</span><br><span class="line">	2.Define the streaming computations by applying transformation and output operations to DStreams.</span><br><span class="line">	3.Start receiving data and processing it using streamingContext.start().</span><br><span class="line">	4.Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().</span><br><span class="line">	5.The processing can be manually stopped using streamingContext.stop().</span><br><span class="line">Points to remember:</span><br><span class="line">	1.Once a context has been started, no new streaming computations can be set up or added to it.</span><br><span class="line">		就是说：</span><br><span class="line">			 ssc.start()</span><br><span class="line">			 在这加入逻辑处理是没有用的</span><br><span class="line">            ssc.awaitTermination()</span><br><span class="line">            </span><br><span class="line">	2.Once a context has been stopped, it cannot be restarted.</span><br><span class="line">	3.Only one StreamingContext can be active in a JVM at the same time.</span><br><span class="line">	4.stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext</span><br><span class="line">		 set the optional parameter of stop() called stopSparkContext to false.</span><br><span class="line">	5.A SparkContext can be re-used to create multiple StreamingContexts,</span><br><span class="line">	 as long as the previous StreamingContext is stopped (without stopping the SparkContext)</span><br><span class="line">	  before the next StreamingContext is created.</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191031131615454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">上面案例讲解：</span><br><span class="line"></span><br><span class="line">1.既然是通过上下文ssc 去拿数据去 接收数据</span><br><span class="line">会有一个  接收器   在里面 </span><br><span class="line"></span><br><span class="line">socket 起在 9999 端口  需要一个接收器 把数据接收回来 </span><br><span class="line"></span><br><span class="line">下面</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="noopener">Input DStreams and Receivers</a></p>
<p><strong>Input DStreams</strong> are DStreams representing <strong>the stream of input data</strong> <strong>received from streaming sources.</strong> In the quick example, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (<strong>except file stream</strong>, discussed later in this section) is associated with a <strong>Receiver</strong> (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. lines was an input DStream</span><br><span class="line">2. Receiver :receives the data from a source and stores it in Spark’s memory for processing.</span><br><span class="line"></span><br><span class="line">所以 你假如不知道 哪个算子 里有接收器   看它返回值 返回值里是带Receiver 的 </span><br><span class="line">只要有返回值里是带Receiver   必然是有接收器的</span><br><span class="line">eg：</span><br><span class="line">			 val lines: ReceiverInputDStream[String]</span><br><span class="line"> </span><br><span class="line"> 不是所有的接收数据都需要接收器的 ***</span><br><span class="line"> 为什么呢？</span><br><span class="line"> 	eg：HDFS上的数据  直接通过API读进来就可以了 不需要接收器</span><br><span class="line"></span><br><span class="line">所以：</span><br><span class="line">上面的 master 设置 local  不是local[2]</span><br><span class="line">为什么1 不行呢？1的话 你的 jobid 0  就占用一个线程  后面没有资源线程处理了呀</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031133816240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">where &gt; n  因为 有些业务是需要多个流处理的   </span><br><span class="line">eg：你一套代码里面 有多个 socket  就有多个reciver了 明白吗？</span><br><span class="line"></span><br><span class="line">所以你 core的数量 要大于 recivers的数量   否则 你的程序只能接收数据 不能处理数据</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191031132936682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">active job   ： receiver  是接收数据用的 一直在跑</span><br><span class="line">这个是永远存在的 因为 对于 socket模式 </span><br><span class="line">返回值是 ReceiverInputDStream 所有第一个Job是一直running在那的，职责就是接收数据</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031133418106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这幅图 调优的时候详细讲解</span><br></pre></td></tr></table></figure></div>

<p>*<em>操作讲解 *</em><br><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams" target="_blank" rel="noopener">Transformations on DStreams</a></p>
<p>只有最后两个和RDD算子不一样 其他的都一样</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">a,c,b,b,b</span><br><span class="line">a,a,a,a,a</span><br><span class="line">b,b,b,b,b</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//    result.print()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    //1.统计一个批次出现了多少个单词</span><br><span class="line">    lines.count().print() //一个批次有多少条数据</span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).count().print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501470000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501470000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">19/10/31 13:57:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 13:57:55 WARN BlockManager: Block input-0-1572501475400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 13:57:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 13:57:59 WARN BlockManager: Block input-0-1572501479200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501480000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501480000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">15</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501490000 ms</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ nc -lk 9999</span><br><span class="line">b,b,b,b,b</span><br><span class="line">a,a,a,a,a</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">object StreamingWCApp01 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val ssc = ContextUtils.getStreamingContext(this.getClass.getSimpleName, 10)</span><br><span class="line"></span><br><span class="line">    //TODO... 填写我们的业务逻辑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(&quot;hadoop101&quot;,9999)</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">      .map((_, 1)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//    result.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //1.统计一个批次出现了多少个单词</span><br><span class="line">    lines.count().print() //一个批次有多少条数据</span><br><span class="line">    lines.flatMap(_.split(&quot;,&quot;)).countByValue().print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501630000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501630000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">19/10/31 14:00:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 14:00:31 WARN BlockManager: Block input-0-1572501631200 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">19/10/31 14:00:38 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.</span><br><span class="line">19/10/31 14:00:38 WARN BlockManager: Block input-0-1572501638400 replicated to only 0 peer(s) instead of 1 peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501640000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501640000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,5)</span><br><span class="line">(a,5)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1572501650000 ms</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure></div>

<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#output-operations-on-dstreams" target="_blank" rel="noopener">Output Operations on DStreams</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * Save each RDD in this DStream as at text file, using string representation</span><br><span class="line">   * of elements. The file name at each batch interval is generated based on</span><br><span class="line">   * `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.</span><br><span class="line">   */</span><br><span class="line">  def saveAsTextFiles(prefix: String, suffix: String = &quot;&quot;): Unit = ssc.withScope &#123;</span><br><span class="line">    val saveFunc = (rdd: RDD[T], time: Time) =&gt; &#123;</span><br><span class="line">      val file = rddToFileName(prefix, suffix, time)</span><br><span class="line">      rdd.saveAsTextFile(file)</span><br><span class="line">    &#125;</span><br><span class="line">    this.foreachRDD(saveFunc, displayInnerRDDOps = false)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">这个方法生产上能用么？</span><br><span class="line">假如你1s 处理一次  1s写一次 你hdfs很容易写爆掉的 </span><br><span class="line">如果要用 把写出去的文件 使用追加的方式写   或者 定期合并生成的文件 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output opearation  和rdd 大部分都类似 </span><br><span class="line">foreachRDD  这个算子 之后讲解</span><br></pre></td></tr></table></figure></div>
<p><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams-and-receivers" target="_blank" rel="noopener">Input DStreams and Receivers</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Spark Streaming provides two categories of built-in streaming sources.</span><br><span class="line"></span><br><span class="line">	1.Basic sources:</span><br><span class="line">		 Sources directly available in the StreamingContext API. </span><br><span class="line">		 Examples: file systems, and socket connections.</span><br><span class="line">	2.Advanced sources: </span><br><span class="line">		Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. </span><br><span class="line">		These require linking against extra dependencies as discussed in the linking section.</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">流处理系统 一般对接的是 kafka  读文件用的少 </span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Create an input stream that monitors a Hadoop-compatible filesystem</span><br><span class="line">   * for new files and reads them as text files (using key as LongWritable, value</span><br><span class="line">   * as Text and input format as TextInputFormat). Files must be written to the</span><br><span class="line">   * monitored directory by &quot;moving&quot; them from another location within the same</span><br><span class="line">   * file system. File names starting with . are ignored.</span><br><span class="line">   * @param directory HDFS directory to monitor for new file</span><br><span class="line">   */</span><br><span class="line">  def textFileStream(directory: String): DStream[String] = withNamedScope(&quot;text file stream&quot;) &#123;</span><br><span class="line">    fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString)</span><br><span class="line">  &#125;</span><br><span class="line">底层fileStream  跟一下源码有兴趣的 前面文章讲过</span><br><span class="line"></span><br><span class="line">返回值是 DStream  所以 可以local1来处理 </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line"> Files must be written to the</span><br><span class="line">   * monitored directory by &quot;moving&quot; them from another location within the same</span><br><span class="line">   * file system.</span><br><span class="line"></span><br><span class="line">All files must be in the same data format.  看官网</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191031141750538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Kafka01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/15/Kafka01/">Kafka01</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/15/Kafka01/" class="article-date">
  <time datetime="2018-02-15T12:10:01.000Z" itemprop="datePublished">2018-02-15</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><a href="http://kafka.apache.org/" target="_blank" rel="noopener">官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.发布和订阅</span><br><span class="line">2.process  ： kafka自身的stream processing   kafkastreams</span><br><span class="line">3.store ：  Store streams of data safely in a distributed, replicated, fault-tolerant cluster.</span><br><span class="line"></span><br><span class="line">kafka如何保证高容错 、保证数据不丢失、？</span><br><span class="line">相当于HDFS的副本机制 eg：</span><br><span class="line">一个文件 打成 多个block块 一个block块 多少个副本</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.发布和订阅</span><br><span class="line">	对应的经典架构是什么呢：</span><br><span class="line">	Flume(生产者)--&gt;Kafka--&gt;SS SSS(消费者)</span><br></pre></td></tr></table></figure></div>
<p><strong>CDK：</strong><br><a href="https://docs.cloudera.com/documentation/kafka/4-1-x/topics/kafka_packaging.html" target="_blank" rel="noopener">CDK官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CDH版本的Kafka ：Kafka是需要自定义部署的   </span><br><span class="line"></span><br><span class="line">2.1.1   |	Apache Kafka	| 0.10.0.0+kafka2.1.1+21  </span><br><span class="line">代表什么意思？</span><br><span class="line">0.10.0.0   =》 Apache Kafka的版本</span><br><span class="line">2.1.1  =》  CDH版本 使用Apache Kafka 0.10.0.0 源码编译打补丁 后的 CDH版Kafka  的版本号</span><br><span class="line">21   =》 打了几次补丁的意思</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102919561753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Kafka的版本的选择 要基于 Spark</span><br><span class="line">http://spark.apache.org/docs/latest/streaming-kafka-integration.html</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029200928752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这里我下载最新版本  </span><br><span class="line">el7 和 el6 有什么区别？</span><br><span class="line">el代表 centos的 </span><br><span class="line"> 7 -》 7.x  </span><br><span class="line"> 6--》6.x  </span><br><span class="line"></span><br><span class="line">解压之后 lib里</span><br><span class="line">kafka_2.11-2.2.1-kafka-4.1.0.jar</span><br><span class="line">2.11 scala 版本</span><br><span class="line">2.2.1 apache kafka版本</span><br><span class="line">4.1.0 cdk版本</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029201428246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">kafka：</span><br><span class="line">metadata--&gt;zk </span><br><span class="line">log--&gt;当前机器的Linux磁盘上  (Kakfa存的数据)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">部署;</span><br><span class="line">https://blog.csdn.net/jeffleo/article/details/75736474</span><br><span class="line"></span><br><span class="line">broker.id=0</span><br><span class="line">host.name=hadoop101</span><br><span class="line">port=9092</span><br><span class="line">log.dirs=/home/double_happy/tmp/kafka-logs00</span><br><span class="line">zookeeper.connect=hadoop101:2181/kafka</span><br><span class="line"></span><br><span class="line">broker.id=1</span><br><span class="line">host.name=hadoop101</span><br><span class="line">port=9093</span><br><span class="line">log.dirs=/home/double_happy/tmp/kafka-logs01</span><br><span class="line">zookeeper.connect=hadoop101:2181/kafka</span><br><span class="line"></span><br><span class="line">broker.id=2</span><br><span class="line">host.name=hadoop101</span><br><span class="line">port=9094</span><br><span class="line">log.dirs=/home/double_happy/tmp/kafka-logs02</span><br><span class="line">zookeeper.connect=hadoop101:2181/kafka</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">启动：</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server.properties</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server-1.properties</span><br><span class="line">bin/kafka-server-start.sh -daemon config/server-2.properties</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029223326116.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>启动成功 </p>
<p>查看zk：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] ls /</span><br><span class="line">[zookeeper, kafka]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5]</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 kafka]$ jps</span><br><span class="line">17088 QuorumPeerMain</span><br><span class="line">20992 Kafka</span><br><span class="line">4289 NodeManager</span><br><span class="line">21810 Jps</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">21591 Kafka</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">20392 Kafka</span><br><span class="line">6633 HistoryServer</span><br><span class="line">3721 NameNode</span><br><span class="line">4186 ResourceManager</span><br><span class="line">3853 DataNode</span><br></pre></td></tr></table></figure></div>


<p>Kafka概念：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">4.几个概念 </span><br><span class="line">producer: 生产者 Flume Maxwell</span><br><span class="line">consumer：消费者 SS/SSS/Flink/Flume</span><br><span class="line">broker： 消息处理节点  kafka的启动进程</span><br><span class="line"></span><br><span class="line">topic: 主题 </span><br><span class="line">	oms订单系统mysql.oms --&gt;maxwell--&gt;kafka  topic: oms  /oms文件夹下</span><br><span class="line">	wms仓库系统mysql.wms --&gt;maxwell--&gt;kafka  topic: wms  /wms文件夹下</span><br><span class="line"></span><br><span class="line">	app log---flume--&gt;kafka topic： applog</span><br><span class="line">	systemlog---flume--&gt;kafka topic： systemlog</span><br><span class="line"></span><br><span class="line">partition：分区 topic物理上的分组 (对应的Linux文件夹)，</span><br><span class="line">           一个topic 可以分&gt;=1个p，每个p是一个有序的队列：数据落地到磁盘 是0拷贝+按顺序追加写</span><br><span class="line">	  这就是kafka高效的数据中间件的原因</span><br><span class="line">      按顺序追加写 ： 磁盘的那个指针按一个方向追加和读写的 </span><br><span class="line">           oms_0 oms_1 oms_2</span><br><span class="line"></span><br><span class="line">这个parition ：</span><br><span class="line">相当于HDFS的副本机制 eg：</span><br><span class="line">一个文件 打成 多个block块 一个block块 多少个副本 </span><br><span class="line"></span><br><span class="line">kafka里的分区就相当于 HDFS里的分块的概念 </span><br><span class="line"></span><br><span class="line">replication：副本数   和HDFS的副本数 一样的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">常用命令：</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$ ./kafka-topics.sh </span><br><span class="line">Create, delete, describe, or change a topic.</span><br><span class="line">Option                                   Description                            </span><br><span class="line">------                                   -----------                            </span><br><span class="line">--alter                                  Alter the number of partitions,        </span><br><span class="line">                                           replica assignment, and/or           </span><br><span class="line">                                           configuration for the topic.         </span><br><span class="line">--bootstrap-server &lt;String: server to    REQUIRED: The Kafka server to connect  </span><br><span class="line">  connect to&gt;                              to. In case of providing this, a     </span><br><span class="line">                                           direct Zookeeper connection won&apos;t be </span><br><span class="line">                                           required.                            </span><br><span class="line">--command-config &lt;String: command        Property file containing configs to be </span><br><span class="line">  config property file&gt;                    passed to Admin Client. This is used </span><br><span class="line">                                           only with --bootstrap-server option  </span><br><span class="line">                                           for describing and altering broker   </span><br><span class="line">                                           configs.                             </span><br><span class="line">--config &lt;String: name=value&gt;            A topic configuration override for the </span><br><span class="line">                                           topic being created or altered.The   </span><br><span class="line">                                           following is a list of valid         </span><br><span class="line">                                           configurations:                      </span><br><span class="line">                                                cleanup.policy                        </span><br><span class="line">                                                compression.type                      </span><br><span class="line">                                                delete.retention.ms                   </span><br><span class="line">                                                file.delete.delay.ms                  </span><br><span class="line">                                                flush.messages                        </span><br><span class="line">                                                flush.ms                              </span><br><span class="line">                                                follower.replication.throttled.       </span><br><span class="line">                                           replicas                             </span><br><span class="line">                                                index.interval.bytes                  </span><br><span class="line">                                                leader.replication.throttled.replicas </span><br><span class="line">                                                max.message.bytes                     </span><br><span class="line">                                                message.downconversion.enable         </span><br><span class="line">                                                message.format.version                </span><br><span class="line">                                                message.timestamp.difference.max.ms   </span><br><span class="line">                                                message.timestamp.type                </span><br><span class="line">                                                min.cleanable.dirty.ratio             </span><br><span class="line">                                                min.compaction.lag.ms                 </span><br><span class="line">                                                min.insync.replicas                   </span><br><span class="line">                                                preallocate                           </span><br><span class="line">                                                retention.bytes                       </span><br><span class="line">                                                retention.ms                          </span><br><span class="line">                                                segment.bytes                         </span><br><span class="line">                                                segment.index.bytes                   </span><br><span class="line">                                                segment.jitter.ms                     </span><br><span class="line">                                                segment.ms                            </span><br><span class="line">                                                unclean.leader.election.enable        </span><br><span class="line">                                         See the Kafka documentation for full   </span><br><span class="line">                                           details on the topic configs.It is   </span><br><span class="line">                                           supported only in combination with --</span><br><span class="line">                                           create if --bootstrap-server option  </span><br><span class="line">                                           is used.                             </span><br><span class="line">--create                                 Create a new topic.                    </span><br><span class="line">--delete                                 Delete a topic                         </span><br><span class="line">--delete-config &lt;String: name&gt;           A topic configuration override to be   </span><br><span class="line">                                           removed for an existing topic (see   </span><br><span class="line">                                           the list of configurations under the </span><br><span class="line">                                           --config option). Not supported with </span><br><span class="line">                                           the --bootstrap-server option.       </span><br><span class="line">--describe                               List details for the given topics.     </span><br><span class="line">--disable-rack-aware                     Disable rack aware replica assignment  </span><br><span class="line">--exclude-internal                       exclude internal topics when running   </span><br><span class="line">                                           list or describe command. The        </span><br><span class="line">                                           internal topics will be listed by    </span><br><span class="line">                                           default                              </span><br><span class="line">--force                                  Suppress console prompts               </span><br><span class="line">--help                                   Print usage information.               </span><br><span class="line">--if-exists                              if set when altering or deleting or    </span><br><span class="line">                                           describing topics, the action will   </span><br><span class="line">                                           only execute if the topic exists.    </span><br><span class="line">                                           Not supported with the --bootstrap-  </span><br><span class="line">                                           server option.                       </span><br><span class="line">--if-not-exists                          if set when creating topics, the       </span><br><span class="line">                                           action will only execute if the      </span><br><span class="line">                                           topic does not already exist. Not    </span><br><span class="line">                                           supported with the --bootstrap-      </span><br><span class="line">                                           server option.                       </span><br><span class="line">--list                                   List all available topics.             </span><br><span class="line">--partitions &lt;Integer: # of partitions&gt;  The number of partitions for the topic </span><br><span class="line">                                           being created or altered (WARNING:   </span><br><span class="line">                                           If partitions are increased for a    </span><br><span class="line">                                           topic that has a key, the partition  </span><br><span class="line">                                           logic or ordering of the messages    </span><br><span class="line">                                           will be affected                     </span><br><span class="line">--replica-assignment &lt;String:            A list of manual partition-to-broker   </span><br><span class="line">  broker_id_for_part1_replica1 :           assignments for the topic being      </span><br><span class="line">  broker_id_for_part1_replica2 ,           created or altered.                  </span><br><span class="line">  broker_id_for_part2_replica1 :                                                </span><br><span class="line">  broker_id_for_part2_replica2 , ...&gt;                                           </span><br><span class="line">--replication-factor &lt;Integer:           The replication factor for each        </span><br><span class="line">  replication factor&gt;                      partition in the topic being created.</span><br><span class="line">--topic &lt;String: topic&gt;                  The topic to create, alter, describe   </span><br><span class="line">                                           or delete. It also accepts a regular </span><br><span class="line">                                           expression, except for --create      </span><br><span class="line">                                           option. Put topic name in double     </span><br><span class="line">                                           quotes and use the &apos;\&apos; prefix to     </span><br><span class="line">                                           escape regular expression symbols; e.</span><br><span class="line">                                           g. &quot;test\.topic&quot;.                    </span><br><span class="line">--topics-with-overrides                  if set when describing topics, only    </span><br><span class="line">                                           show topics that have overridden     </span><br><span class="line">                                           configs                              </span><br><span class="line">--unavailable-partitions                 if set when describing topics, only    </span><br><span class="line">                                           show partitions whose leader is not  </span><br><span class="line">                                           available                            </span><br><span class="line">--under-replicated-partitions            if set when describing topics, only    </span><br><span class="line">                                           show under replicated partitions     </span><br><span class="line">--zookeeper &lt;String: hosts&gt;              DEPRECATED, The connection string for  </span><br><span class="line">                                           the zookeeper connection in the form </span><br><span class="line">                                           host:port. Multiple hosts can be     </span><br><span class="line">                                           given to allow fail-over.            </span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.创建topic</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --create \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka \</span><br><span class="line">&gt; --replication-factor 3 \</span><br><span class="line">&gt; --partitions 3 \</span><br><span class="line">&gt; --topic test</span><br><span class="line">Created topic test.</span><br><span class="line">[double_happy@hadoop101 kafka]$</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191030101603723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线—————————————————————————————————————————–<br><img src="https://img-blog.csdnimg.cn/20191030101625339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线—————————————————————————————————————————–<br><img src="https://img-blog.csdnimg.cn/2019103010164530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线—————————————————————————————————————————–<br><img src="https://img-blog.csdnimg.cn/20191030102637736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线—————————————————————————————————————————–</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1.会创建3个文件夹 </span><br><span class="line">test-0</span><br><span class="line">test-1</span><br><span class="line">test-2</span><br><span class="line"></span><br><span class="line">test : topic 的名字   012 是partition的 下标从0开始 </span><br><span class="line"></span><br><span class="line">--replication-factor 3   三个副本指的是  我们有三台机器 副本数要 &lt;= 3     你三台机器 不能创建多余机器数的副本数</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2.查看我们的topic  ： 查看我当前的kafka集群有多少个Topic</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --list \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka </span><br><span class="line">test</span><br><span class="line">[double_happy@hadoop101 kafka]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">3.查看 topic 明细   ***</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --describe \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka \</span><br><span class="line">&gt; --topic test</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: test     Partition: 2    Leader: 0       Replicas: 0,2,1 Isr: 0,2,1</span><br><span class="line">[double_happy@hadoop101 kafka]$ </span><br><span class="line"></span><br><span class="line">解释：因为我是单节点部署三台kafka 这样说不好理解 我就把他变成三台机器部署三台kfaka </span><br><span class="line"></span><br><span class="line">1.机器</span><br><span class="line">broker 0  -&gt; Hadoop101</span><br><span class="line">broker 1 -&gt; Hadoop102</span><br><span class="line">broker 2 -&gt; Hadoop103</span><br><span class="line"></span><br><span class="line">2.明细</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 1   Hadoop102      Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 2   Hadoop103   Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: test     Partition: 2    Leader: 0   Hadoop101    Replicas: 0,2,1 Isr: 0,2,1</span><br><span class="line"></span><br><span class="line">3.解释</span><br><span class="line">Leader、Replicas、Isr：里面的数字 代表  broker id</span><br><span class="line">Partition：数字 代表 paritition的下标</span><br><span class="line">Leader ：该分区 负责对外的 读写的节点 </span><br><span class="line"></span><br><span class="line">3.1    按 分区0 进行解释 </span><br><span class="line">对于test 这个主题来说，我们的数据完整性 是由三个分区 来组成的  012 partition</span><br><span class="line"></span><br><span class="line">就是说 我0分区 由分本数 三个副本 三个副本是分散在不同的机器上面的</span><br><span class="line">但是哪个机器进行对外读和写呢 ？是由Leader 的值来表现的 </span><br><span class="line"></span><br><span class="line">那么：</span><br><span class="line">0 分区的数据 是由 哪台机器 来进行对外读写 提供的(Leader 1) 说明是 broker 1 号机器 Hadoop102</span><br><span class="line"></span><br><span class="line">那么 ：</span><br><span class="line"> Replicas: 1,0,2   表示 复制分区的节点的列表 </span><br><span class="line"> 0,2 =》表示 Hadoop103、Hadoop101    这两台机器对parittion 0 进行数据的复制同步</span><br><span class="line"></span><br><span class="line">Isr：in-sync 表示正在同步的集群列表</span><br><span class="line"></span><br><span class="line">那么：</span><br><span class="line">isr: 1,0,2  </span><br><span class="line">	1表示对外的 leader</span><br><span class="line">	假设分区0 对应的leader 1 机器挂了 ，那么 0 将会作为leader 进行对外提供服务</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">关键点：</span><br><span class="line">1.分区  topic的数据完整性</span><br><span class="line">2.谁负责对外读写</span><br><span class="line">3.谁负责分区数据同步</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">测试：isr</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh --describe --zookeeper hadoop101:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: test     Partition: 2    Leader: 0       Replicas: 0,2,1 Isr: 0,2,1</span><br><span class="line">[double_happy@hadoop101 kafka]$ jps</span><br><span class="line">17088 QuorumPeerMain</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">24260 Kafka</span><br><span class="line">23653 Kafka</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">6633 HistoryServer</span><br><span class="line">3721 NameNode</span><br><span class="line">4186 ResourceManager</span><br><span class="line">3853 DataNode</span><br><span class="line">24862 Kafka</span><br><span class="line">31278 Jps</span><br><span class="line">[double_happy@hadoop101 kafka]$ kill -9 24260</span><br><span class="line">[double_happy@hadoop101 kafka]$ </span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh --describe --zookeeper hadoop101:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 0       Replicas: 1,0,2 Isr: 0,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 2       Replicas: 2,1,0 Isr: 2,0</span><br><span class="line">        Topic: test     Partition: 2    Leader: 0       Replicas: 0,2,1 Isr: 0,2</span><br><span class="line">[double_happy@hadoop101 kafka]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">很巧 瞎kill一个真的把 1 broker kill掉了 </span><br><span class="line"></span><br><span class="line">注意;</span><br><span class="line">	看paritition 0  是不是 1被干掉之后 0上位了 当成了leader</span><br><span class="line">2.你kill -9  是不是太暴力了 </span><br><span class="line">查看</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$ cat kafka-server-stop.sh</span><br><span class="line">#!/bin/sh</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">SIGNAL=$&#123;SIGNAL:-TERM&#125;</span><br><span class="line">PIDS=$(ps ax | grep -i &apos;kafka\.Kafka&apos; | grep java | grep -v grep | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$PIDS&quot; ]; then</span><br><span class="line">  echo &quot;No kafka server to stop&quot;</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  kill -s $SIGNAL $PIDS</span><br><span class="line">fi</span><br><span class="line">[double_happy@hadoop101 bin]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">官方使用 kill -s  接受一个信号    有兴趣了解一下 但是官方也用了 kill</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">重启 broker 1 ：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh --describe --zookeeper hadoop101:2181/kafka --topic test</span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 0       Replicas: 1,0,2 Isr: 0,2</span><br><span class="line">        Topic: test     Partition: 1    Leader: 2       Replicas: 2,1,0 Isr: 2,0</span><br><span class="line">        Topic: test     Partition: 2    Leader: 0       Replicas: 0,2,1 Isr: 0,2</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-server-start.sh -daemon config/server-1.properties</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh --describe --zookeeper hadoop101:2181/kafka --topic test</span><br><span class="line"> </span><br><span class="line">Topic:test      PartitionCount:3        ReplicationFactor:3     Configs:</span><br><span class="line">        Topic: test     Partition: 0    Leader: 0       Replicas: 1,0,2 Isr: 0,2,1</span><br><span class="line">        Topic: test     Partition: 1    Leader: 2       Replicas: 2,1,0 Isr: 2,0,1</span><br><span class="line">        Topic: test     Partition: 2    Leader: 0       Replicas: 0,2,1 Isr: 0,2,1</span><br><span class="line">[double_happy@hadoop101 kafka]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">4.删除 Topic</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --delete \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka \</span><br><span class="line">&gt; --topic test</span><br><span class="line">Topic test is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line">[double_happy@hadoop101 kafka]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Topic test is marked for deletion.   已经标识了删除  那么我们还能不能查看到呢？</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh --delete --zookeeper hadoop101:2181/kafka --topic test</span><br><span class="line">Error while executing topic command : Topics in [] does not exist</span><br><span class="line">[2019-10-30 11:06:04,714] ERROR java.lang.IllegalArgumentException: Topics in [] does not exist</span><br><span class="line">        at kafka.admin.TopicCommand$.kafka$admin$TopicCommand$$ensureTopicExists(TopicCommand.scala:416)</span><br><span class="line">        at kafka.admin.TopicCommand$ZookeeperTopicService.deleteTopic(TopicCommand.scala:377)</span><br><span class="line">        at kafka.admin.TopicCommand$.main(TopicCommand.scala:68)</span><br><span class="line">        at kafka.admin.TopicCommand.main(TopicCommand.scala)</span><br><span class="line"> (kafka.admin.TopicCommand$)</span><br><span class="line">[double_happy@hadoop101 kafka]$</span><br><span class="line"></span><br><span class="line">查看不到了 说明Topic 已经不存在了 </span><br><span class="line"></span><br><span class="line">去机器上 看一下目录</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191030110911261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">正常应该由 delete 标识的  等重启Kafak之后 会彻底删除 delete 标识的文件夹  </span><br><span class="line"></span><br><span class="line">我这里没有演示出来  </span><br><span class="line"></span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line">这个提示是 表示;</span><br><span class="line">delete.topic.enable  参数没有设值为 true  那么 删除不会立马起作用 </span><br><span class="line">(双重否定 表示 肯定   是true 就会立马删除 )</span><br><span class="line"></span><br><span class="line">去官网看看：</span><br><span class="line">delete.topic.enable 默认 就是 true  </span><br><span class="line">所以 提示这句话 就是 放屁 </span><br><span class="line">但是也要注意 生产上别随便删除 Topic 删了可能会导致 kafka起不来 真的 别没事找事做</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191030111503573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">我想总结的是这个 删除topic 可能会导致kafka故障？</span><br><span class="line">删除整理:</span><br><span class="line">1.⽣产上命名不要有标点符号的字符，就英⽂ 可以带数字 默认⼩写</span><br><span class="line">2.假如已经在运⾏的kafka 只有1个topic，你可以删除，0⻛险</span><br><span class="line">3.假如已经在运⾏的kafka 只有多个topic，忍，⻛险可能存在  (你把自己的topic 删完 kafka坏了 别人的topic也没法用了 )</span><br><span class="line">4.千万不要⼿贱或者强迫症，看topic名称不舒服，去删除topic，删除需谨慎！</span><br><span class="line">5.删除不可逆，细⼼操作删除命令！</span><br><span class="line"></span><br><span class="line">删除可能遇到的问题？当你真的要删除Topic的时候 </span><br><span class="line">       如果可以重启kafka 就重启kafka 不能重启看下面</span><br><span class="line">假如删除不⼲净:</span><br><span class="line">1.linux磁盘⽂件夹  手工删除 </span><br><span class="line">2.zk的/kafka的</span><br><span class="line">ls /kafka/brokers/topics</span><br><span class="line">ls /kafka/config/topics</span><br><span class="line">默认delete.topic.enable=true，执⾏删除命令后，⽆需关⼼。(因为会立马删除掉 )</span><br><span class="line"></span><br><span class="line">看一下刚刚删除完的 topic 在zk的元数据</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[zookeeper, kafka]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /kafka/brokers/topics</span><br><span class="line">[]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls /kafka/config/topics</span><br><span class="line">[]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] </span><br><span class="line"></span><br><span class="line">没有 说明删的 很干净</span><br></pre></td></tr></table></figure></div>



<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">5..修改topic</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --create \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka \</span><br><span class="line">&gt; --replication-factor 1 \</span><br><span class="line">&gt; --partitions 1 \</span><br><span class="line">&gt; --topic g7</span><br><span class="line">Created topic g7.</span><br><span class="line">[double_happy@hadoop101 kafka]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（1）修改分区数量 </span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --alter \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka \</span><br><span class="line">&gt; --topic g7 --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line">[double_happy@hadoop101 kafka]$</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019103011415749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>华丽的分割线————————————————————————————————————</p>
<p>添加分区之后：<br><img src="https://img-blog.csdnimg.cn/20191030114325796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">没有 g7-1 g7-2</span><br><span class="line">说明分区的副本只有一份   其他分区落不进来</span><br><span class="line"></span><br><span class="line">副本数 一般3分 </span><br><span class="line">broker 8台机器 qbs很高的 生产上 是这么用的   3，4万的qbs</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">查看明细：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-topics.sh \</span><br><span class="line">&gt; --describe \</span><br><span class="line">&gt; --zookeeper hadoop101:2181/kafka \</span><br><span class="line">&gt; --topic g7 </span><br><span class="line">Topic:g7        PartitionCount:3     ReplicationFactor:1      Configs:</span><br><span class="line">        Topic: g7       Partition: 0 Leader: 2        Replicas: 2     Isr: 2</span><br><span class="line">        Topic: g7       Partition: 1 Leader: 0        Replicas: 0     Isr: 0</span><br><span class="line">        Topic: g7       Partition: 2 Leader: 1        Replicas: 1     Isr: 1</span><br><span class="line">[double_happy@hadoop101 kafka]$ </span><br><span class="line"></span><br><span class="line">说明副本只有一个  isr 只有一个 挂了 就丢数据了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6.自动迁移数据到 新的节点 ：</span><br><span class="line"></span><br><span class="line">假如有一天 你的kafka需要添加机器 那么你之前的kafka数据 要进行迁移</span><br></pre></td></tr></table></figure></div>
<p><a href="http://kafka.apache.org/22/documentation.html#basic_ops_automigrate" target="_blank" rel="noopener">迁移官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1.  kafka-reassign-partitions.sh    使用这个脚本 </span><br><span class="line">重新分配我们的partition  这种情况下主要是针对 </span><br><span class="line">kafka的集群规模 数据量 扛不住了</span><br><span class="line">需要新加机器来平衡我的数据量 </span><br><span class="line"></span><br><span class="line">就三步：</span><br><span class="line">1.生成一个json的文件 </span><br><span class="line">bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list &quot;5,6&quot; --generate</span><br><span class="line"></span><br><span class="line">就这个命令 会生成 json文件 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--broker-list &quot;5,6&quot;     表示 1234 下架 只用56   如果你用1356  那么这个参数这么写呢 ？</span><br><span class="line">                    --broker-list &quot;1,3,5,6&quot;   明白了吗？</span><br><span class="line"></span><br><span class="line">2.json文件就是一个标准 后面根据它 进行重新分配    (就是根据json 去执行)</span><br><span class="line">bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute</span><br><span class="line"></span><br><span class="line">3. 验证第二步  (进行校验)</span><br><span class="line">bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191030115720114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">工作时候 用的很少 一般都是事先规划好的 机器</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">7.console案例   很简单 </span><br><span class="line"></span><br><span class="line">生产者：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-producer.sh \</span><br><span class="line">&gt; --broker-list hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic g7</span><br><span class="line">&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">消费者：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line">&gt; --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic g7 \</span><br><span class="line">&gt; --from-beginning</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这就模拟了 生产者 发送数据到 broker  然后消费者进行消费</span><br></pre></td></tr></table></figure></div>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">生产者：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-producer.sh \</span><br><span class="line">&gt; --broker-list hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic g7</span><br><span class="line">&gt;doubleHappy</span><br><span class="line">&gt;1</span><br><span class="line">&gt;2</span><br><span class="line">&gt;3</span><br><span class="line">&gt;4</span><br><span class="line">&gt;5</span><br><span class="line">&gt;6</span><br><span class="line">&gt;7</span><br><span class="line">&gt;8</span><br><span class="line">&gt;9</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">消费者：</span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh \</span><br><span class="line">&gt; --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 \</span><br><span class="line">&gt; --topic g7 \</span><br><span class="line">&gt; --from-beginning</span><br><span class="line">doubleHappy</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">那么我把消费者 打断 重新 起来 ：</span><br><span class="line"></span><br><span class="line">--from-beginning  表示 从头开始消费    表示消息从最初始的位置开始消费 </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop101:9092,hadoop101:9093,hadoop101:9094 --topic g7 --from-beginning</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">7</span><br><span class="line">2</span><br><span class="line">5</span><br><span class="line">8</span><br><span class="line">doubleHappy</span><br><span class="line">3</span><br><span class="line">6</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">问题：</span><br><span class="line">是不是乱序了   指的是全局乱序的   因为kafka 单分区 数据是有序的 (就是 每个分区内的数据 在各自当前分区是有序的 )</span><br><span class="line"></span><br><span class="line">数据在生产的时候 发送到kafka的时候</span><br><span class="line"> 数据是发往不同的分区里 </span><br><span class="line"></span><br><span class="line">不同分区里的数据应该是这样的：</span><br><span class="line">1 4 7     |    2 5 8  |  doubleHappy  3 6 9</span><br><span class="line"></span><br><span class="line">**数据在每个 分区内部是有序的    </span><br><span class="line"></span><br><span class="line">那么 如何保证数据 全局有序？</span><br><span class="line">    自己思考思考 我下篇文章写</span><br></pre></td></tr></table></figure></div>

<p>故障案例：</p>
<p>异构平台Kafka对接使⽤<br><a href="http://blog.itpub.net/30089851/viewspace-2152671/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2152671/</a></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Redis" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/10/Redis/">Redis</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/10/Redis/" class="article-date">
  <time datetime="2018-02-10T12:09:12.000Z" itemprop="datePublished">2018-02-10</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p>是NoSQL的<br><a href="https://redis.io/" target="_blank" rel="noopener">官网</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Redis:</span><br><span class="line">	1.NoSQL的数据库</span><br><span class="line">	2.key-value方式存储的  内存数据库</span><br><span class="line">	    key ：通常是String类型</span><br><span class="line">	    value：strings, hashes, lists, sets, sorted sets  等 选择哪一个根据业务来定的 </span><br><span class="line">redis 数据存在内存里的 万一挂掉了怎么办？</span><br><span class="line">虽然内部有机制  ： 定时的把内存数据 刷到磁盘上面 重启之后 可以把磁盘上的数据加载进来的</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">特性：</span><br><span class="line">	1.速度快 1s 十万次  读写非常好的 </span><br><span class="line">	2. 持久化   两种 RBD(快照的方式 ) 和 AOF(记录所有操作的 写到日志文件里去的)的  </span><br><span class="line">	    重启之后优先AOF  因为 数据更加完整  RBD是定时的快照 </span><br><span class="line">	    	eg：10s 做一次快照 8s的时候挂掉了  这8s的数据就丢了 </span><br><span class="line">	3.数据类型 </span><br><span class="line">	4.多语言的  java scala 都可以</span><br><span class="line">	5.其他功能 ： 发布订阅 事务 pipeline(指的是 多条指令 可以放在一起 最后 发一条指令    代替 而不是一条数据一个指令  网络传输少很多  工作当中批量的 一定要采用pipeline的方式 发送请求)</span><br><span class="line">	6.单线程的 </span><br><span class="line">	7.有集群 有主从复制    来保证分布式和高可用</span><br><span class="line"></span><br><span class="line">国内 微博 使用redis的多</span><br></pre></td></tr></table></figure></div>
<p><strong>操作：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 src]$ ls redis-server </span><br><span class="line">redis-server                服务端的 </span><br><span class="line">[double_happy@hadoop101 src]$ ls redis-cli</span><br><span class="line">redis-cli                         客户端的</span><br><span class="line">[double_happy@hadoop101 src]$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ make  install</span><br><span class="line">cd src &amp;&amp; make install</span><br><span class="line">make[1]: Entering directory `/home/double_happy/app/redis-5.0.5/src&apos;</span><br><span class="line">    CC Makefile.dep</span><br><span class="line">make[1]: Leaving directory `/home/double_happy/app/redis-5.0.5/src&apos;</span><br><span class="line">make[1]: Entering directory `/home/double_happy/app/redis-5.0.5/src&apos;</span><br><span class="line"></span><br><span class="line">Hint: It&apos;s a good idea to run &apos;make test&apos; ;)</span><br><span class="line"></span><br><span class="line">    INSTALL install</span><br><span class="line">install: cannot create regular file ‘/usr/local/bin/redis-server’: Permission denied</span><br><span class="line">make[1]: *** [install] Error 1</span><br><span class="line">make[1]: Leaving directory `/home/double_happy/app/redis-5.0.5/src&apos;</span><br><span class="line">make: *** [install] Error 2</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ sudo make install</span><br><span class="line">cd src &amp;&amp; make install</span><br><span class="line">make[1]: Entering directory `/home/double_happy/app/redis-5.0.5/src&apos;</span><br><span class="line"></span><br><span class="line">Hint: It&apos;s a good idea to run &apos;make test&apos; ;)</span><br><span class="line"></span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">    INSTALL install</span><br><span class="line">make[1]: Leaving directory `/home/double_happy/app/redis-5.0.5/src&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">你把redis编译好了 </span><br><span class="line">‘/usr/local/bin/redis-server 这个目录下也是有 redis-server的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">启动：</span><br><span class="line">[double_happy@hadoop101 ~]$ redis-server </span><br><span class="line">27132:C 29 Oct 2019 13:41:11.883 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">27132:C 29 Oct 2019 13:41:11.883 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=27132, just started</span><br><span class="line">27132:C 29 Oct 2019 13:41:11.883 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf</span><br><span class="line">                _._                                                  </span><br><span class="line">           _.-``__ &apos;&apos;-._                                             </span><br><span class="line">      _.-``    `.  `_.  &apos;&apos;-._           Redis 5.0.5 (00000000/0) 64 bit</span><br><span class="line">  .-`` .-```.  ```\/    _.,_ &apos;&apos;-._                                   </span><br><span class="line"> (    &apos;      ,       .-`  | `,    )     Running in standalone mode</span><br><span class="line"> |`-._`-...-` __...-.``-._|&apos;` _.-&apos;|     Port: 6379</span><br><span class="line"> |    `-._   `._    /     _.-&apos;    |     PID: 27132</span><br><span class="line">  `-._    `-._  `-./  _.-&apos;    _.-&apos;                                   </span><br><span class="line"> |`-._`-._    `-.__.-&apos;    _.-&apos;_.-&apos;|                                  </span><br><span class="line"> |    `-._`-._        _.-&apos;_.-&apos;    |           http://redis.io        </span><br><span class="line">  `-._    `-._`-.__.-&apos;_.-&apos;    _.-&apos;                                   </span><br><span class="line"> |`-._`-._    `-.__.-&apos;    _.-&apos;_.-&apos;|                                  </span><br><span class="line"> |    `-._`-._        _.-&apos;_.-&apos;    |                                  </span><br><span class="line">  `-._    `-._`-.__.-&apos;_.-&apos;    _.-&apos;                                   </span><br><span class="line">      `-._    `-.__.-&apos;    _.-&apos;                                       </span><br><span class="line">          `-._        _.-&apos;                                           </span><br><span class="line">              `-.__.-&apos;                                               </span><br><span class="line"></span><br><span class="line">27132:M 29 Oct 2019 13:41:11.884 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.</span><br><span class="line">27132:M 29 Oct 2019 13:41:11.884 # Server initialized</span><br><span class="line">27132:M 29 Oct 2019 13:41:11.884 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.</span><br><span class="line">27132:M 29 Oct 2019 13:41:11.884 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span><br><span class="line">27132:M 29 Oct 2019 13:41:11.884 * Ready to accept connections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">6379  redis经典端口  </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ ps -ef | grep redis</span><br><span class="line">double_+ 27132 16513  0 13:41 pts/1    00:00:00 redis-server *:6379</span><br><span class="line">double_+ 27148 15274  0 13:42 pts/0    00:00:00 grep --color=auto redis</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ </span><br><span class="line"></span><br><span class="line">说明redis  服务端启动了 </span><br><span class="line">再启动客户端 ：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 ~]$ redis-cli </span><br><span class="line">127.0.0.1:6379&gt; keys*</span><br><span class="line">(error) ERR unknown command `keys*`, with args beginning with: </span><br><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">(empty list or set)</span><br><span class="line">127.0.0.1:6379&gt; set name double_happy</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; keys *</span><br><span class="line">1) &quot;name&quot;</span><br><span class="line">127.0.0.1:6379&gt; get name</span><br><span class="line">&quot;double_happy&quot;</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">后台运行：需要对redis.conf 文件进行简单的配置 配置一下 logfile位置 </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ src/redis-server redis.conf              </span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ tail -200f ~/tmp/redislogs/redis.log </span><br><span class="line">27337:C 29 Oct 2019 13:53:20.629 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo</span><br><span class="line">27337:C 29 Oct 2019 13:53:20.629 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=27337, just started</span><br><span class="line">27337:C 29 Oct 2019 13:53:20.629 # Configuration loaded</span><br><span class="line">                _._                                                  </span><br><span class="line">           _.-``__ &apos;&apos;-._                                             </span><br><span class="line">      _.-``    `.  `_.  &apos;&apos;-._           Redis 5.0.5 (00000000/0) 64 bit</span><br><span class="line">  .-`` .-```.  ```\/    _.,_ &apos;&apos;-._                                   </span><br><span class="line"> (    &apos;      ,       .-`  | `,    )     Running in standalone mode</span><br><span class="line"> |`-._`-...-` __...-.``-._|&apos;` _.-&apos;|     Port: 6379</span><br><span class="line"> |    `-._   `._    /     _.-&apos;    |     PID: 27338</span><br><span class="line">  `-._    `-._  `-./  _.-&apos;    _.-&apos;                                   </span><br><span class="line"> |`-._`-._    `-.__.-&apos;    _.-&apos;_.-&apos;|                                  </span><br><span class="line"> |    `-._`-._        _.-&apos;_.-&apos;    |           http://redis.io        </span><br><span class="line">  `-._    `-._`-.__.-&apos;_.-&apos;    _.-&apos;                                   </span><br><span class="line"> |`-._`-._    `-.__.-&apos;    _.-&apos;_.-&apos;|                                  </span><br><span class="line"> |    `-._`-._        _.-&apos;_.-&apos;    |                                  </span><br><span class="line">  `-._    `-._`-.__.-&apos;_.-&apos;    _.-&apos;                                   </span><br><span class="line">      `-._    `-.__.-&apos;    _.-&apos;                                       </span><br><span class="line">          `-._        _.-&apos;                                           </span><br><span class="line">              `-.__.-&apos;                                               </span><br><span class="line"></span><br><span class="line">27338:M 29 Oct 2019 13:53:20.632 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.</span><br><span class="line">27338:M 29 Oct 2019 13:53:20.632 # Server initialized</span><br><span class="line">27338:M 29 Oct 2019 13:53:20.632 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect.</span><br><span class="line">27338:M 29 Oct 2019 13:53:20.632 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &apos;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&apos; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.</span><br><span class="line">27338:M 29 Oct 2019 13:53:20.632 * Ready to accept connections</span><br><span class="line">^C</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ ^C</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">客户端操作：  需要把redis.conf  里的 bind 去掉 或者 配置成 0.0.0.0</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ src/redis-cli -h hadoop101 -p 6379</span><br><span class="line">hadoop101:6379&gt; </span><br><span class="line"></span><br><span class="line">再黑窗口查看 redis还是比较麻烦的 </span><br><span class="line">https://www.jianshu.com/p/6895384d2b9e 安装一下</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029140404559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Redis多数据库特性：</span><br><span class="line">	1.默认有16个数据库   可以改的   redis.conf       </span><br><span class="line">		通常情况下 数据库之间是 隔离的</span><br><span class="line">	2.</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029140735994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Redis 切换库 黑窗口：</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; select 1</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379[1]&gt; select 0</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379&gt; select 16</span><br><span class="line">(error) ERR DB index is out of range</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029140944482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1.	通常情况下 数据库之间是 隔离的</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; select 1</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379[1]&gt; set  bigdata spark</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379[1]&gt; get  bigdata</span><br><span class="line">&quot;spark&quot;</span><br><span class="line">hadoop101:6379[1]&gt; select 0</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379&gt; get bigdata</span><br><span class="line">(nil)</span><br><span class="line">hadoop101:6379&gt; </span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">flushall   这个操作没有做隔离的 会把所有的库里的东西 全部干掉 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; FLUSHALL</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379&gt; select 1</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379[1]&gt; get bigdata</span><br><span class="line">(nil)</span><br><span class="line">hadoop101:6379[1]&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">redis里的基础命令：</span><br><span class="line">	1. keys  *    查你当前库里的所有的key</span><br><span class="line">	   这个keys 是能进行匹配的 </span><br><span class="line">eg：</span><br><span class="line">hadoop101:6379&gt; set name 1</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379&gt; set name2 xx</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379&gt; set name3 ll</span><br><span class="line">OK</span><br><span class="line">hadoop101:6379&gt; keys *</span><br><span class="line">1) &quot;name3&quot;</span><br><span class="line">2) &quot;name2&quot;</span><br><span class="line">3) &quot;name&quot;</span><br><span class="line">hadoop101:6379&gt; keys name*</span><br><span class="line">1) &quot;name3&quot;</span><br><span class="line">2) &quot;name2&quot;</span><br><span class="line">3) &quot;name&quot;</span><br><span class="line">hadoop101:6379&gt; keys name?</span><br><span class="line">1) &quot;name3&quot;</span><br><span class="line">2) &quot;name2&quot;</span><br><span class="line">hadoop101:6379&gt; keys name[0-9]</span><br><span class="line">1) &quot;name3&quot;</span><br><span class="line">2) &quot;name2&quot;</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2.判断某一个key 存不存在 </span><br><span class="line">exists  对应的key      返回1 表示存在 </span><br><span class="line">eg：</span><br><span class="line">hadoop101:6379&gt; EXISTS name</span><br><span class="line">(integer) 1</span><br><span class="line">hadoop101:6379&gt; EXISTS name10</span><br><span class="line">(integer) 0</span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">3.删除  </span><br><span class="line">del  key </span><br><span class="line">del key1 key2</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; DEL name </span><br><span class="line">(integer) 1</span><br><span class="line">hadoop101:6379&gt; del name1 name2</span><br><span class="line">(integer) 1</span><br><span class="line">hadoop101:6379&gt; </span><br><span class="line"></span><br><span class="line">或者 会到外面删除 </span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ src/redis-cli keys &quot;*&quot;</span><br><span class="line">1) &quot;name1&quot;</span><br><span class="line">2) &quot;name3&quot;</span><br><span class="line">3) &quot;name2&quot;</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ src/redis-cli del `redis-cli keys &quot;name*&quot;`</span><br><span class="line">(integer) 3</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$ src/redis-cli keys &quot;*&quot;</span><br><span class="line">(empty list or set)</span><br><span class="line">[double_happy@hadoop101 redis-5.0.5]$</span><br></pre></td></tr></table></figure></div>

<p>命令太多了 看官网<br><a href="https://redis.io/commands#generic" target="_blank" rel="noopener">Commands</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查看 key的类型：</span><br><span class="line">hadoop101:6379&gt; type name</span><br><span class="line">string</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">但是真的记不住怎么办？</span><br><span class="line">help</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt; help</span><br><span class="line">redis-cli 5.0.5</span><br><span class="line">To get help about Redis commands type:</span><br><span class="line">      &quot;help @&lt;group&gt;&quot; to get a list of commands in &lt;group&gt;</span><br><span class="line">      &quot;help &lt;command&gt;&quot; for help on &lt;command&gt;</span><br><span class="line">      &quot;help &lt;tab&gt;&quot; to get a list of possible help topics</span><br><span class="line">      &quot;quit&quot; to exit</span><br><span class="line"></span><br><span class="line">To set redis-cli preferences:</span><br><span class="line">      &quot;:set hints&quot; enable online hints</span><br><span class="line">      &quot;:set nohints&quot; disable online hints</span><br><span class="line">Set your preferences in ~/.redisclirc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">help  @&lt;group&gt;  用的最多</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">eg：</span><br><span class="line">hadoop101:6379&gt; help @string</span><br><span class="line"></span><br><span class="line">  APPEND key value</span><br><span class="line">  summary: Append a value to a key</span><br><span class="line">  since: 2.0.0</span><br><span class="line"></span><br><span class="line">  BITCOUNT key [start end]</span><br><span class="line">  summary: Count set bits in a string</span><br><span class="line">  since: 2.6.0</span><br><span class="line"></span><br><span class="line">  BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL]</span><br><span class="line">  summary: Perform arbitrary bitfield integer operations on strings</span><br><span class="line">  since: 3.2.0</span><br><span class="line"></span><br><span class="line">  BITOP operation destkey key [key ...]</span><br><span class="line">  summary: Perform bitwise operations between strings</span><br><span class="line">  since: 2.6.0</span><br><span class="line"></span><br><span class="line">  BITPOS key bit [start] [end]</span><br><span class="line">  summary: Find first bit set or clear in a string</span><br><span class="line">  since: 2.8.7</span><br><span class="line"></span><br><span class="line">  DECR key</span><br><span class="line">  summary: Decrement the integer value of a key by one</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  DECRBY key decrement</span><br><span class="line">  summary: Decrement the integer value of a key by the given number</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  GET key</span><br><span class="line">  summary: Get the value of a key</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  GETBIT key offset</span><br><span class="line">  summary: Returns the bit value at offset in the string value stored at key</span><br><span class="line">  since: 2.2.0</span><br><span class="line"></span><br><span class="line">  GETRANGE key start end</span><br><span class="line">  summary: Get a substring of the string stored at a key</span><br><span class="line">  since: 2.4.0</span><br><span class="line"></span><br><span class="line">  GETSET key value</span><br><span class="line">  summary: Set the string value of a key and return its old value</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  INCR key</span><br><span class="line">  summary: Increment the integer value of a key by one</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  INCRBY key increment</span><br><span class="line">  summary: Increment the integer value of a key by the given amount</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  INCRBYFLOAT key increment</span><br><span class="line">  summary: Increment the float value of a key by the given amount</span><br><span class="line">  since: 2.6.0</span><br><span class="line"></span><br><span class="line">  MGET key [key ...]</span><br><span class="line">  summary: Get the values of all the given keys</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  MSET key value [key value ...]</span><br><span class="line">  summary: Set multiple keys to multiple values</span><br><span class="line">  since: 1.0.1</span><br><span class="line"></span><br><span class="line">  MSETNX key value [key value ...]</span><br><span class="line">  summary: Set multiple keys to multiple values, only if none of the keys exist</span><br><span class="line">  since: 1.0.1</span><br><span class="line"></span><br><span class="line">  PSETEX key milliseconds value</span><br><span class="line">  summary: Set the value and expiration in milliseconds of a key</span><br><span class="line">  since: 2.6.0</span><br><span class="line"></span><br><span class="line">  SET key value [expiration EX seconds|PX milliseconds] [NX|XX]</span><br><span class="line">  summary: Set the string value of a key</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  SETBIT key offset value</span><br><span class="line">  summary: Sets or clears the bit at offset in the string value stored at key</span><br><span class="line">  since: 2.2.0</span><br><span class="line"></span><br><span class="line">  SETEX key seconds value</span><br><span class="line">  summary: Set the value and expiration of a key</span><br><span class="line">  since: 2.0.0</span><br><span class="line"></span><br><span class="line">  SETNX key value</span><br><span class="line">  summary: Set the value of a key, only if the key does not exist</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">  SETRANGE key offset value</span><br><span class="line">  summary: Overwrite part of a string at key starting at the specified offset</span><br><span class="line">  since: 2.2.0</span><br><span class="line"></span><br><span class="line">  STRLEN key</span><br><span class="line">  summary: Get the length of the value stored in a key</span><br><span class="line">  since: 2.2.0</span><br><span class="line"></span><br><span class="line">hadoop101:6379&gt;</span><br></pre></td></tr></table></figure></div>

<p><strong>数据类型：</strong><br>这个需要掌握的 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">String类型：</span><br><span class="line">setnx ：</span><br><span class="line">SETNX key value</span><br><span class="line">  summary: Set the value of a key, only if the key does not exist</span><br><span class="line">  since: 1.0.0</span><br><span class="line"></span><br><span class="line">很多 incr可以用来统计在线人数</span><br><span class="line"></span><br><span class="line">Hash类型： ***</span><br><span class="line">场景 多了一个字段  假如 100行数据只有一条记录  多了这个字段</span><br><span class="line">大数据里 偏移量使用它 来存储的 </span><br><span class="line"></span><br><span class="line">hmset:</span><br><span class="line">对已有的会修改 对没有的会加上去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102914564719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>idea开发</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">public class RedisApp &#123;</span><br><span class="line"></span><br><span class="line">    String host = &quot;hadoop101&quot;;</span><br><span class="line">    int port = 6379;</span><br><span class="line"></span><br><span class="line">    Jedis jedis ;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Test</span><br><span class="line">    public void test01() &#123;</span><br><span class="line">        jedis.set(&quot;info&quot;, &quot;double_happy&quot;);</span><br><span class="line"></span><br><span class="line">        Assert.assertEquals(&quot;double_happy&quot;,jedis.get(&quot;info&quot;));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Before</span><br><span class="line">    public void setUp() &#123;</span><br><span class="line">        jedis = new Jedis(host,port);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    public void tearDown() &#123;</span><br><span class="line">        jedis.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">查看结果： 绿了哈    注意命令和api 一一对应的 </span><br><span class="line"></span><br><span class="line">突然想到一个梗 ：</span><br><span class="line">	异地恋最重要的是什么？</span><br><span class="line">	最重要的是 相互信任  这样对四个人都好</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029151736313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SparkSQL03" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/SparkSQL03/">SparkSQL03</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/05/SparkSQL03/" class="article-date">
  <time datetime="2018-02-05T12:08:32.000Z" itemprop="datePublished">2018-02-05</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">在object LogApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local[4]&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    // ETL: 一定保留原有的数据   最完整</span><br><span class="line">    var inputDF = spark.read.json(&quot;data/data-test.json&quot;)</span><br><span class="line">   inputDF = inputDF.withColumn(&quot;province&quot;, MydataUDF.getProvince(inputDF.col(&quot;ip&quot;)))</span><br><span class="line">   inputDF = inputDF.withColumn(&quot;city&quot;, MydataUDF.getCity(inputDF.col(&quot;ip&quot;)))</span><br><span class="line"></span><br><span class="line">    // ETL==&gt;ODS</span><br><span class="line">  //  inputDF.coalesce(1).write.format(&quot;parquet&quot;)     //orc /parquet</span><br><span class="line"> //     .option(&quot;compression&quot;,&quot;snappy&quot;).save(&quot; path&quot;)    //别使用snappy 用lzo</span><br><span class="line"></span><br><span class="line">    inputDF.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line"></span><br><span class="line">    spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;,&quot;400&quot;) // --conf  这个东西不建议写在代码里 建议写在 spark-submit --conf 那块 明白吗？</span><br><span class="line">    </span><br><span class="line">    val areaSQL01 = &quot;select province,city, &quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode &gt;=1 then 1 else 0 end) origin_request,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode &gt;=2 then 1 else 0 end) valid_request,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode =3 then 1 else 0 end) ad_request,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and isbid=1 and adorderid!=0 then 1 else 0 end) bid_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 then 1 else 0 end) bid_success_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=2 and iseffective=1 then 1 else 0 end) ad_display_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=3 and processnode=1 then 1 else 0 end) ad_click_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=2 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_display_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=3 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_click_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*winprice/1000 else 0 end) ad_consumption,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*adpayment/1000 else 0 end) ad_cost &quot; +</span><br><span class="line">      &quot;from log group by province,city&quot;</span><br><span class="line">    spark.sql(areaSQL01).show(false)//.createOrReplaceTempView(&quot;area_tmp&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val areaSQL02 = &quot;select province,city, &quot; +</span><br><span class="line">      &quot;origin_request,&quot; +</span><br><span class="line">      &quot;valid_request,&quot; +</span><br><span class="line">      &quot;ad_request,&quot; +</span><br><span class="line">      &quot;bid_cnt,&quot; +</span><br><span class="line">      &quot;bid_success_cnt,&quot; +</span><br><span class="line">      &quot;bid_success_cnt/bid_cnt bid_success_rate,&quot; +</span><br><span class="line">      &quot;ad_display_cnt,&quot; +</span><br><span class="line">      &quot;ad_click_cnt,&quot; +</span><br><span class="line">      &quot;ad_click_cnt/ad_display_cnt ad_click_rate,&quot; +</span><br><span class="line">      &quot;ad_consumption,&quot; +</span><br><span class="line">      &quot;ad_cost from area_tmp &quot; +</span><br><span class="line">      &quot;where bid_cnt!=0 and ad_display_cnt!=0&quot;</span><br><span class="line"></span><br><span class="line">    Thread.sleep(10000)</span><br><span class="line"></span><br><span class="line">    spark.sql(areaSQL02).show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object MydataUDF &#123;</span><br><span class="line"></span><br><span class="line">  import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">  def getProvince = udf((ip:String)=&gt;&#123;</span><br><span class="line">    IPUtil.getInstance().getInfos(ip)(1)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  def getCity = udf((ip:String)=&gt;&#123;</span><br><span class="line">    IPUtil.getInstance().getInfos(ip)(2)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">问题：</span><br><span class="line">有什么问题？</span><br><span class="line">1.spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;,&quot;400&quot;) // --conf  </span><br><span class="line">这个东西不建议写在代码里 建议写在 spark-submit --conf 那块 明白吗？ 或者通过代码判断输入值 eg:400</span><br><span class="line"></span><br><span class="line">2. inputDF.coalesce(1).write.format(&quot;parquet&quot;)     //orc /parquet</span><br><span class="line">.option(&quot;compression&quot;,&quot;snappy&quot;).save(&quot; path&quot;)    //别使用snappy 用lzo</span><br><span class="line">spark默认是snappy 别用哈 看压缩篇</span><br><span class="line"></span><br><span class="line">coalesce(1) 这个值 看下面给的建议  处理小文件的</span><br></pre></td></tr></table></figure></div>

<p>演示上面代码可能的问题<br><img src="https://img-blog.csdnimg.cn/2019102822020751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028220302437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">这200 哪里来的 ？</span><br><span class="line"></span><br><span class="line">    官网 sparksql 调优章节 </span><br><span class="line">1.spark.sql.shuffle.partitions 参数 默认 200   这是sparksql里面 设置的shuffle参数 </span><br><span class="line"></span><br><span class="line">2.RDD里的 reduceByKey(，numPartitions）还有印象吗？rdd是在这里设置的 </span><br><span class="line"></span><br><span class="line">sparksql 默认200  生产上绝对是不够的 只要你数据量稍微大一点 200个 一定是扛不住的 </span><br><span class="line"></span><br><span class="line">这个参数 你可以理解为mapreduce里的reduce的数量 ，reduce数量如果大了 </span><br><span class="line">会导致上面问题？程序跑起来是快了 但是 小文件过多 </span><br><span class="line"></span><br><span class="line">那么这个值 该怎么设置呢？</span><br><span class="line">给你个思路 估计你读进来的数据量大小 + 你预估你每个task处理的数据量是多少 </span><br><span class="line">来设计 这个值</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">还有一点就是：</span><br><span class="line">加入这个值是400  </span><br><span class="line">400</span><br><span class="line">    大：小文件多点、</span><br><span class="line">    10exe * 2core = 20task   同一时间点 20个task</span><br><span class="line">               400/20=20轮</span><br><span class="line">               600/20=30轮</span><br></pre></td></tr></table></figure></div>


<p><strong>ETL</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ETL</span><br><span class="line">    input:json</span><br><span class="line">    清洗 ==&gt; ODS  大宽表  HDFS/Hive/SparkSQL</span><br><span class="line">    output: 列式存储  ORC/Parquet   这块一定是要落地的 </span><br><span class="line"></span><br><span class="line">    Stat</span><br><span class="line">        ==&gt;  一个非常简单的SQL搞定</span><br><span class="line">        ==&gt;  复杂：多个SQL 或者 一个复杂SQL搞定</span><br></pre></td></tr></table></figure></div>
<p><a href="https://developer.ibm.com/hadoop/2016/01/14/5-reasons-to-choose-parquet-for-spark-sql/" target="_blank" rel="noopener">Choose Parquet for Spark SQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">行式存储：MySQL</span><br><span class="line">    一条记录有多个列  一行数据是存储在一起的</span><br><span class="line">    优点：</span><br><span class="line">        你每次查询都使用到所有的列</span><br><span class="line">    缺点：</span><br><span class="line">        大宽表有N多列，但是我们仅仅使用其中几列</span><br><span class="line"> 	</span><br><span class="line"> 	因为我使用大宽表(有100列)的时候 假如只用到其中的3个列，</span><br><span class="line"> 	如果我使用 行式存储   加载数据的时候会把 你一行的所有列都加载出来 意味着浪费了97%资源</span><br><span class="line"> </span><br><span class="line"> 列式存储很好的解决这个问题</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">列式：Orc Parquet</span><br><span class="line">    特点：把每一列的数据存放在一起</span><br><span class="line">    优点：减少IO 需要哪几列就直接获取哪几列</span><br><span class="line">    缺点：如果你还是要获取每一行中的所有列，那么性能比行式的差</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102821264517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用行式存储 spark跑程序的时候官网也列举了很多问题 </span><br><span class="line">eg：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028213942626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">那么Most of these failures force Spark to re-try by re-queuing tasks：</span><br><span class="line">spark会重试跑失败的task </span><br><span class="line">注意：</span><br><span class="line">	重试 一般是跑不出来的  如果没有倾斜 和资源够 可能会跑出来</span><br><span class="line">	假设10个task 3个task挂掉了 那么重新起的task 你能确定 </span><br><span class="line">	重启来的task 会在 3个task之前挂掉的executor上面么？</span><br><span class="line">	不能确定 很可能起到别的executor上面 </span><br><span class="line">	（别的executor 可能现在也在跑 其余7个task中的某些task）</span><br><span class="line">	对于这个 executor压力更大 可能会导致你的应用程序被干掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">存储是结合 压缩来用的   eg：orc + lzo</span><br><span class="line">减少disk io</span><br></pre></td></tr></table></figure></div>

<h2 id="beeline-jdbc"><a href="#beeline-jdbc" class="headerlink" title="beeline/jdbc"></a>beeline/jdbc</h2><p>生产上是用的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hiveserver2  beeline/jdbc     Hive里的 </span><br><span class="line">thriftserver beeline/jdbc     spark里的 </span><br><span class="line"></span><br><span class="line">怎么用呢？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ ./start-thriftserver.sh --jars ~/software/mysql-connector-java-5.1.47.jar </span><br><span class="line">starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop101.out</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ tail -200f /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop101.out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Spark Command: /usr/java/java/bin/java -cp /home/double_happy/app/spark/conf/:/home/double_happy/app/spark/jars/*:/home/double_happy/app/hadoop/etc/hadoop/ -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server --jars /home/double_happy/software/mysql-connector-java-5.1.47.jar spark-internal</span><br><span class="line">========================================</span><br><span class="line">19/10/28 22:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/28 22:26:56 INFO HiveThriftServer2: Started daemon with process name: 2297@hadoop101</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for TERM</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for HUP</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for INT</span><br><span class="line">19/10/28 22:26:56 INFO HiveThriftServer2: Starting SparkContext</span><br><span class="line">19/10/28 22:26:56 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/10/28 22:26:56 INFO SparkContext: Submitted application: Thrift JDBC/ODBC Server</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/28 22:26:57 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 35237.</span><br><span class="line">19/10/28 22:26:57 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/10/28 22:26:57 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/10/28 22:26:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/10/28 22:26:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/10/28 22:26:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb4561a2-a2c1-42a6-a313-96e3ff47a7fb</span><br><span class="line">19/10/28 22:26:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/10/28 22:26:58 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/10/28 22:26:58 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/10/28 22:26:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/10/28 22:26:58 INFO SparkContext: Added JAR file:///home/double_happy/software/mysql-connector-java-5.1.47.jar at spark://hadoop101:35237/jars/mysql-connector-java-5.1.47.jar with timestamp 1572272818597</span><br><span class="line">19/10/28 22:26:58 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/10/28 22:26:59 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 33661.</span><br><span class="line">19/10/28 22:26:59 INFO NettyBlockTransferService: Server created on hadoop101:33661</span><br><span class="line">19/10/28 22:26:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:33661 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:27:01 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572272818750</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: loading hive config file: file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/conf/hive-site.xml</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: Setting hive.metastore.warehouse.dir (&apos;null&apos;) to the value of spark.sql.warehouse.dir (&apos;file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse&apos;).</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: Warehouse path is &apos;file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse&apos;.</span><br><span class="line">19/10/28 22:27:01 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</span><br><span class="line">19/10/28 22:27:03 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/28 22:27:03 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:03 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/28 22:27:03 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/28 22:27:05 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/28 22:27:07 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:07 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/28 22:27:08 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/28 22:27:08 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:08 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/28 22:27:08 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:09 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_functions: db=homework pat=*</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=homework pat=*</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_function: homework.add_prefix_new</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.add_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO HiveMetaStore: 0: get_function: homework.remove_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.remove_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created local directory: /tmp/41aaf1c8-5deb-45c7-9c03-ef172a6058a3_resources</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created local directory: /tmp/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3/_tmp_space.db</span><br><span class="line">19/10/28 22:27:10 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse</span><br><span class="line">19/10/28 22:27:10 INFO HiveMetaStore: 0: get_database: default</span><br><span class="line">19/10/28 22:27:10 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_database: default</span><br><span class="line">19/10/28 22:27:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">19/10/28 22:27:10 INFO HiveUtils: Initializing execution hive, version 1.2.1</span><br><span class="line">19/10/28 22:27:11 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/28 22:27:11 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:11 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/28 22:27:11 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/28 22:27:14 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/28 22:27:15 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:15 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</span><br><span class="line">19/10/28 22:27:17 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0</span><br><span class="line">19/10/28 22:27:17 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:18 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created local directory: /tmp/5fe8af31-b32b-4788-a587-4fbf6cab7b1a_resources</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created local directory: /tmp/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a/_tmp_space.db</span><br><span class="line">19/10/28 22:27:18 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: Operation log root directory is created: /tmp/double_happy/operation_logs</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread pool size: 100</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:OperationManager is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:SessionManager is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service: CLIService is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:ThriftBinaryCLIService is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service: HiveServer2 is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:OperationManager is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:SessionManager is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:CLIService is started.</span><br><span class="line">19/10/28 22:27:18 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:18 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/28 22:27:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</span><br><span class="line">19/10/28 22:27:18 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_databases: default</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_databases: default</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: Shutting down the object store...</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Shutting down the object store...</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: Metastore shutdown complete.</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Metastore shutdown complete.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:ThriftBinaryCLIService is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:HiveServer2 is started.</span><br><span class="line">19/10/28 22:27:18 INFO HiveThriftServer2: HiveThriftServer2 started</span><br><span class="line">19/10/28 22:27:18 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">说明 thriftserver 启动起来了 </span><br><span class="line"></span><br><span class="line">sparkui端口 参数</span><br><span class="line">spark.port.maxRetries 16 默认16 也就是同一时间点对一台机器 只能起16个spark-submit </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 software]$ jps</span><br><span class="line">2496 Jps</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">6633 HistoryServer</span><br><span class="line">2297 SparkSubmit</span><br><span class="line">3721 NameNode</span><br><span class="line">4186 ResourceManager</span><br><span class="line">3853 DataNode</span><br><span class="line">[double_happy@hadoop101 software]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">也就是这个 2297 SparkSubmit  最多16个</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028223316966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是 thriftserver 端起来了 说明服务端有了</span><br><span class="line">所以接下来要通过客户端 连接一下 </span><br><span class="line">客户端怎么链接呢？</span><br><span class="line">使用beeline     用法跟Hive里是一毛一样的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/ruozedata_g7 -n double_happy</span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/ruozedata_g7</span><br><span class="line">19/10/28 22:40:56 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:40:56 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:40:56 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/ruozedata_g7</span><br><span class="line">Error: Database &apos;ruozedata_g7&apos; not found; (state=,code=0)</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/ruozedata_g7 (closed)&gt; ^C^C[double_happy@hadoop101 bin]$ </span><br><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/ -n double_happy            </span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/</span><br><span class="line">19/10/28 22:42:15 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:42:15 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:42:16 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/</span><br><span class="line">Connected to: Spark SQL (version 2.4.4)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1.spark2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; show databases;</span><br><span class="line">+---------------+--+</span><br><span class="line">| databaseName  |</span><br><span class="line">+---------------+--+</span><br><span class="line">| default       |</span><br><span class="line">| homework      |</span><br><span class="line">+---------------+--+</span><br><span class="line">2 rows selected (1.206 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; use homework;</span><br><span class="line">+---------+--+</span><br><span class="line">| Result  |</span><br><span class="line">+---------+--+</span><br><span class="line">+---------+--+</span><br><span class="line">No rows selected (0.181 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; show tables;</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| database  |             tableName              | isTemporary  |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| homework  | access_wide                        | false        |</span><br><span class="line">| homework  | dwd_platform_stat_info             | false        |</span><br><span class="line">| homework  | jf_tmp                             | false        |</span><br><span class="line">| homework  | ods_domain_traffic_info            | false        |</span><br><span class="line">| homework  | ods_log_info                       | false        |</span><br><span class="line">| homework  | ods_uid_pid_compression_info       | false        |</span><br><span class="line">| homework  | ods_uid_pid_info                   | false        |</span><br><span class="line">| homework  | ods_uid_pid_info_compression_test  | false        |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">8 rows selected (0.202 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/homework -n double_happy</span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/homework</span><br><span class="line">19/10/28 22:43:14 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:43:14 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:43:14 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/homework</span><br><span class="line">Connected to: Spark SQL (version 2.4.4)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1.spark2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/homework&gt; show tables;</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| database  |             tableName              | isTemporary  |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| homework  | access_wide                        | false        |</span><br><span class="line">| homework  | dwd_platform_stat_info             | false        |</span><br><span class="line">| homework  | jf_tmp                             | false        |</span><br><span class="line">| homework  | ods_domain_traffic_info            | false        |</span><br><span class="line">| homework  | ods_log_info                       | false        |</span><br><span class="line">| homework  | ods_uid_pid_compression_info       | false        |</span><br><span class="line">| homework  | ods_uid_pid_info                   | false        |</span><br><span class="line">| homework  | ods_uid_pid_info_compression_test  | false        |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">8 rows selected (0.352 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/homework&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个东西适用在哪里呢？</span><br><span class="line"></span><br><span class="line">你的数据是通过UI去访问的：eg：HUE/Zeppelin  (他们后台都有一个服务的 )</span><br><span class="line">   </span><br><span class="line">   之后可以写一个 jdbc代码 (跟hive里是一模一样的  把你的sql 发到服务 服务给你返回结果 </span><br><span class="line">    通过你的ui界面 把数据结果渲染出来 )</span><br><span class="line">    </span><br><span class="line">    如果你发的SQL是一个计算/统计SQL：返回肯定是需要时间</span><br><span class="line">    只拿结果，不计算</span><br></pre></td></tr></table></figure></div>
<p>参考官网<a href="http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#distributed-sql-engine" target="_blank" rel="noopener">Distributed SQL Engine</a></p>
<p><strong>Spark On Yarn</strong><br><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Running Spark on YARN</a></p>
<p>There are two deploy modes that can be used to launch Spark applications on YARN. In <strong>cluster mode,</strong> the Spark <strong>driver runs inside an application master process</strong> which is managed by YARN on the cluster, and <strong>the client can go away</strong> after initiating the application. In <strong>client mode</strong>, <strong>the driver runs in the client process</strong>, and the application master is only used for requesting resources from YARN.</p>
<p><img src="https://img-blog.csdnimg.cn/20191029101322751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>client模式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191029103139260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在Spark on YARN中  是没有Worker的概念，是Standalone中的</span><br><span class="line"></span><br><span class="line">Spark on YARN client ：</span><br><span class="line">   1.executor是运行在container中的</span><br><span class="line">   2.driver是跑在本地的</span><br></pre></td></tr></table></figure></div>
<p><strong>cluster模式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191029103558943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">spark on yarn 总结：</span><br><span class="line">Spark：Driver + Executors</span><br><span class="line"></span><br><span class="line">spark on yarn</span><br><span class="line">    cluster</span><br><span class="line">        driver是运行在AM里面的</span><br><span class="line">        AM：AM + Driver   既当爹又当妈 就是既要给executor发task和代码 也要申请资源</span><br><span class="line">        客户端退出   ？作业是没事的 </span><br><span class="line">        日志 是在YARN上的 ***  本地是看不见的 </span><br><span class="line">            yarn logs -applicationId &lt;app ID&gt;</span><br><span class="line"></span><br><span class="line">    client</span><br><span class="line">        driver是运行在本地的      </span><br><span class="line">        客户端退出  作业就退出了</span><br><span class="line">        AM：负责从YARN上去申请资源</span><br><span class="line">        日志是在本地的 ***   方便查看 </span><br><span class="line"></span><br><span class="line">1.</span><br><span class="line">  但是 日志在本地会有一个场景 本地的进程是有一定的限制的  </span><br><span class="line">加入你提交多个作业 都是以yarn client模式 那么 进程可能扎堆出现 机器可能会挂掉 </span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">[double_happy@hadoop101 ~]$ jps</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">17719 CoarseGrainedExecutorBackend</span><br><span class="line">6633 HistoryServer</span><br><span class="line">3721 NameNode</span><br><span class="line">17689 CoarseGrainedExecutorBackend</span><br><span class="line">4186 ResourceManager</span><br><span class="line">17517 SparkSubmit</span><br><span class="line">17645 ExecutorLauncher</span><br><span class="line">3853 DataNode</span><br><span class="line">17966 Jps</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">这是在本地 client 就提交作业 CoarseGrainedExecutorBackend 扎堆出现 多了 机器可能会挂掉</span><br><span class="line"></span><br><span class="line">2.driver 和 executor是有通信的 client模式 下 可能会有一种场景存在</span><br><span class="line">driver可以在任意一台机器上面 但是如果这个机器 不是 集群里的机器 (跟yarn 没有关系哈 这里只讨论机器和集群)</span><br><span class="line">如果这机器是在 集群外 这台机器一定是有集群的 gateway权限的 </span><br><span class="line">driver 和 executor是有通信的 网络会用影响 </span><br><span class="line">工作中在集群外的 很少哈 这里只是说一下这个场景 </span><br><span class="line"></span><br><span class="line">集群内带宽 很高 上面的场景影响不大 </span><br><span class="line"></span><br><span class="line">3.就是client模式就一个弱点 就是 本地进程太多</span><br></pre></td></tr></table></figure></div>

<p><strong>测试：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --help</span><br><span class="line">Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 ~]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line"></span><br><span class="line">spark-shell --master yarn  默认不写  --deploy-mode 是 client模式</span><br></pre></td></tr></table></figure></div>
<p>client模式：测试</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --master yarn</span><br><span class="line">19/10/29 10:41:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;ERROR&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Spark context Web UI available at http://hadoop101:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = yarn, app id = application_1570934113711_0037).</span><br><span class="line">Spark session available as &apos;spark&apos;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4</span><br><span class="line">      /_/</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102910505030.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 写代码跟运行模式是没有关系的 </span><br><span class="line"> 2.  --num-executors   默认是2 个</span><br><span class="line"> 3. id 是application_xxx 开头的必然是 yarn 模式的 去historyserver看见这个开头的 就是yarn模式跑的任务</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029104917905.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-sql --jars ~/software/mysql-connector-java-5.1.47.jar --master yarn</span><br><span class="line">19/10/29 10:54:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/29 10:54:19 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/29 10:54:19 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/29 10:54:20 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/29 10:54:20 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/29 10:54:21 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/29 10:54:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/29 10:54:23 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/29 10:54:24 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_functions: db=homework pat=*</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=homework pat=*</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_function: homework.add_prefix_new</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.add_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO HiveMetaStore: 0: get_function: homework.remove_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.remove_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created local directory: /tmp/eed4bfce-e4a6-4683-81a4-9bda791d7822_resources</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created local directory: /tmp/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822/_tmp_space.db</span><br><span class="line">19/10/29 10:54:25 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/10/29 10:54:25 INFO SparkContext: Submitted application: SparkSQL::172.26.162.56</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/29 10:54:26 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 44153.</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/10/29 10:54:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/10/29 10:54:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/10/29 10:54:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73b7d763-b5a3-476a-a93c-d259c46eac97</span><br><span class="line">19/10/29 10:54:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/10/29 10:54:26 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/10/29 10:54:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/10/29 10:54:27 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">19/10/29 10:54:27 INFO Client: Requesting a new application from cluster with 1 NodeManagers</span><br><span class="line">19/10/29 10:54:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">19/10/29 10:54:27 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead</span><br><span class="line">19/10/29 10:54:27 INFO Client: Setting up container launch context for our AM</span><br><span class="line">19/10/29 10:54:27 INFO Client: Setting up the launch environment for our AM container</span><br><span class="line">19/10/29 10:54:27 INFO Client: Preparing resources for our AM container</span><br><span class="line">19/10/29 10:54:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">19/10/29 10:54:33 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_libs__4819705212614105474.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_libs__4819705212614105474.zip</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/home/double_happy/software/mysql-connector-java-5.1.47.jar -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/mysql-connector-java-5.1.47.jar</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_conf__8084381853282513804.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_conf__.zip</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/29 10:54:37 INFO Client: Submitting application application_1570934113711_0038 to ResourceManager</span><br><span class="line">19/10/29 10:54:37 INFO YarnClientImpl: Submitted application application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:37 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1570934113711_0038 and attemptId None</span><br><span class="line">19/10/29 10:54:38 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:38 INFO Client: </span><br><span class="line">         client token: N/A</span><br><span class="line">         diagnostics: N/A</span><br><span class="line">         ApplicationMaster host: N/A</span><br><span class="line">         ApplicationMaster RPC port: -1</span><br><span class="line">         queue: root.double_happy</span><br><span class="line">         start time: 1572317677273</span><br><span class="line">         final status: UNDEFINED</span><br><span class="line">         tracking URL: http://hadoop101:8088/proxy/application_1570934113711_0038/</span><br><span class="line">         user: double_happy</span><br><span class="line">19/10/29 10:54:39 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:40 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:41 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:42 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:43 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:44 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hadoop101, PROXY_URI_BASES -&gt; http://hadoop101:8088/proxy/application_1570934113711_0038), /proxy/application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:44 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.</span><br><span class="line">19/10/29 10:54:44 INFO Client: Application report for application_1570934113711_0038 (state: RUNNING)</span><br><span class="line">19/10/29 10:54:44 INFO Client: </span><br><span class="line">         client token: N/A</span><br><span class="line">         diagnostics: N/A</span><br><span class="line">         ApplicationMaster host: 172.26.162.56</span><br><span class="line">         ApplicationMaster RPC port: -1</span><br><span class="line">         queue: root.double_happy</span><br><span class="line">         start time: 1572317677273</span><br><span class="line">         final status: UNDEFINED</span><br><span class="line">         tracking URL: http://hadoop101:8088/proxy/application_1570934113711_0038/</span><br><span class="line">         user: double_happy</span><br><span class="line">19/10/29 10:54:44 INFO YarnClientSchedulerBackend: Application application_1570934113711_0038 has started running.</span><br><span class="line">19/10/29 10:54:44 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 38768.</span><br><span class="line">19/10/29 10:54:44 INFO NettyBlockTransferService: Server created on hadoop101:38768</span><br><span class="line">19/10/29 10:54:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:38768 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)</span><br><span class="line">19/10/29 10:54:45 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.</span><br><span class="line">19/10/29 10:54:45 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.162.56:59428) with ID 1</span><br><span class="line">19/10/29 10:54:50 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:36737 with 366.3 MB RAM, BlockManagerId(1, hadoop101, 36737, None)</span><br><span class="line">19/10/29 10:54:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.162.56:34236) with ID 2</span><br><span class="line">19/10/29 10:54:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: loading hive config file: file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/conf/hive-site.xml</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: Setting hive.metastore.warehouse.dir (&apos;null&apos;) to the value of spark.sql.warehouse.dir (&apos;file:/home/double_happy/spark-warehouse&apos;).</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: Warehouse path is &apos;file:/home/double_happy/spark-warehouse&apos;.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.</span><br><span class="line">19/10/29 10:54:52 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:36948 with 366.3 MB RAM, BlockManagerId(2, hadoop101, 36948, None)</span><br><span class="line">19/10/29 10:54:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</span><br><span class="line">19/10/29 10:54:52 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/spark-warehouse</span><br><span class="line">19/10/29 10:54:52 INFO metastore: Mestastore configuration hive.metastore.warehouse.dir changed from /user/hive/warehouse to file:/home/double_happy/spark-warehouse</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Shutting down the object store...</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Shutting down the object store...</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Metastore shutdown complete.</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Metastore shutdown complete.</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: get_database: default</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_database: default</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/29 10:54:52 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/29 10:54:52 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/29 10:54:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/29 10:54:52 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/29 10:54:53 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">Spark master: yarn, Application Id: application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:53 INFO SparkSQLCLIDriver: Spark master: yarn, Application Id: application_1570934113711_0038</span><br><span class="line">spark-sql (default)&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：日志里</span><br><span class="line">19/10/29 10:54:27 WARN Client:</span><br><span class="line"> Neither spark.yarn.jars nor spark.yarn.archive is set, </span><br><span class="line"> falling back to uploading libraries under SPARK_HOME.</span><br><span class="line"></span><br><span class="line">1. spark.yarn.jars nor spark.yarn.archive is set </span><br><span class="line">这个没有设置 会把SPARK_HOME相关的东西 全部传到hdfs上去 </span><br><span class="line">不信看日志 </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">19/10/29 10:54:33 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_libs__4819705212614105474.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_libs__4819705212614105474.zip</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/home/double_happy/software/mysql-connector-java-5.1.47.jar -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/mysql-connector-java-5.1.47.jar</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_conf__8084381853282513804.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_conf__.zip</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">打开这个地址看一眼：</span><br><span class="line"> hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038</span><br><span class="line"></span><br><span class="line">这是我又重启了一个 spark-sql  之前的关掉了 </span><br><span class="line">[double_happy@hadoop101 ~]$ hadoop fs -ls  hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 double_happy supergroup     211902 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/__spark_conf__.zip</span><br><span class="line">-rw-r--r--   1 double_happy supergroup  298846294 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/__spark_libs__2528141214285970680.zip</span><br><span class="line">-rw-r--r--   1 double_happy supergroup    1007502 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/mysql-connector-java-5.1.47.jar</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">__spark_conf__.zip</span><br><span class="line">mysql-connector-java-5.1.47.jar</span><br><span class="line">__spark_libs__2528141214285970680.zip    非常大这个 </span><br><span class="line">作业跑完会把这些自动删掉</span><br><span class="line"></span><br><span class="line">如果上面提到的两个参数没有设置 会把这些传到HDFS  上传是需要花费时间的</span><br><span class="line">这个不解决 你的每个作业 都要这样</span><br><span class="line"></span><br><span class="line">这就是一个调优点：</span><br><span class="line">尽可能的让Spark快速的再yarn上运行起来   该怎么做的呢？</span><br><span class="line"></span><br><span class="line">https://guguoyu.blog.csdn.net/article/details/102644376</span><br></pre></td></tr></table></figure></div>


<p><img src="https://img-blog.csdnimg.cn/20191029105551860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell 和spark-sql 都可以 这不是主要的 主要的是下面的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --master yarn --deploy-mode cluster</span><br><span class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells.</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:853)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:281)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:774)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">为什么Cluster deploy mode is not applicable to Spark shells.？</span><br><span class="line"></span><br><span class="line">因为 spark-shell driver是在本地的 是可以交互代码的  而 yarn-claster  driver是在am里的  明白吗？</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SparkSQL002" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/02/SparkSQL002/">SparkSQL002</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/02/SparkSQL002/" class="article-date">
  <time datetime="2018-02-02T12:07:44.000Z" itemprop="datePublished">2018-02-02</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="操作Hive"><a href="#操作Hive" class="headerlink" title="操作Hive"></a>操作Hive</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;double_happy&quot;)</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;show databases&quot;).show()</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;&quot;).write.saveAsTable(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;&quot;).write.insertInto(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Saves the content of the `DataFrame` as the specified table.</span><br><span class="line">   *</span><br><span class="line">   * In the case the table already exists, behavior of this function depends on the</span><br><span class="line">   * save mode, specified by the `mode` function (default to throwing an exception).</span><br><span class="line">   * When `mode` is `Overwrite`, the schema of the `DataFrame` does not need to be</span><br><span class="line">   * the same as that of the existing table.</span><br><span class="line">   *</span><br><span class="line">   * When `mode` is `Append`, if there is an existing table, we will use the format and options of</span><br><span class="line">   * the existing table. The column order in the schema of the `DataFrame` doesn&apos;t need to be same</span><br><span class="line">   * as that of the existing table. Unlike `insertInto`, `saveAsTable` will use the column names to</span><br><span class="line">   * find the correct column positions. For example:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *    scala&gt; Seq((1, 2)).toDF(&quot;i&quot;, &quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((3, 4)).toDF(&quot;j&quot;, &quot;i&quot;).write.mode(&quot;append&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  i|  j|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  1|  2|</span><br><span class="line">   *    |  4|  3|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * In this method, save mode is used to determine the behavior if the data source table exists in</span><br><span class="line">   * Spark catalog. We will always overwrite the underlying data of data source (e.g. a table in</span><br><span class="line">   * JDBC data source) if the table doesn&apos;t exist in Spark catalog, and will always append to the</span><br><span class="line">   * underlying data of data source if the table already exists.</span><br><span class="line">   *</span><br><span class="line">   * When the DataFrame is created from a non-partitioned `HadoopFsRelation` with a single input</span><br><span class="line">   * path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC</span><br><span class="line">   * and Parquet), the table is persisted in a Hive compatible format, which means other systems</span><br><span class="line">   * like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL</span><br><span class="line">   * specific format.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def saveAsTable(tableName: String): Unit = &#123;</span><br><span class="line">    saveAsTable(df.sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Inserts the content of the `DataFrame` to the specified table. It requires that</span><br><span class="line">   * the schema of the `DataFrame` is the same as the schema of the table.</span><br><span class="line">   *</span><br><span class="line">   * @note Unlike `saveAsTable`, `insertInto` ignores the column names and just uses position-based</span><br><span class="line">   * resolution. For example:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *    scala&gt; Seq((1, 2)).toDF(&quot;i&quot;, &quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((3, 4)).toDF(&quot;j&quot;, &quot;i&quot;).write.insertInto(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((5, 6)).toDF(&quot;a&quot;, &quot;b&quot;).write.insertInto(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  i|  j|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  5|  6|</span><br><span class="line">   *    |  3|  4|</span><br><span class="line">   *    |  1|  2|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Because it inserts data to an existing table, format or options will be ignored.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def insertInto(tableName: String): Unit = &#123;</span><br><span class="line">    insertInto(df.sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建表 也可以直接 spark.sql(&quot;create table xxx&quot;) </span><br><span class="line">但是 不建议这样  因为 表 一般都是提前创建好的  </span><br><span class="line">因为 真正生产上 创建表 是在 一个 web页面 创建表的  是有权限 的</span><br></pre></td></tr></table></figure></div>

<p><a href="http://spark.apache.org/docs/latest/sql-getting-started.html#untyped-dataset-operations-aka-dataframe-operations" target="_blank" rel="noopener">Spark操作Hive 代码</a></p>
<p>大部分人使用spark开发 Hive是使用 spark.sql(“  sql  “)<br>可以的 我不喜欢 我还是喜欢使用api的方式   各有所爱 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">全局排序：这是使用sql的方式写的 </span><br><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line">    </span><br><span class="line">    //需求1 ：统计 每个平台 省市下面 traffic的总和       order by  是全局排序的 </span><br><span class="line">    val sql = &quot;select platform, province, city, sum(traffic) as traffics from log group by platform, province, city order by traffics desc&quot;</span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|platform|province|city|traffics|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|     mac|    香港|    | 2879982|</span><br><span class="line">| windows|    香港|    | 2871537|</span><br><span class="line">| Andriod|    香港|    | 2722363|</span><br><span class="line">|   linux|    香港|    | 2696578|</span><br><span class="line">| Symbain|    香港|    | 2444806|</span><br><span class="line">| Andriod|    山西|忻州|  968255|</span><br><span class="line">|   linux|    台湾|    |  898404|</span><br><span class="line">| windows|    山西|忻州|  894966|</span><br><span class="line">| Andriod|    湖北|武汉|  865758|</span><br><span class="line">|     mac|    湖北|武汉|  848995|</span><br><span class="line">|     mac|    山西|忻州|  837873|</span><br><span class="line">| Symbain|    山西|忻州|  791524|</span><br><span class="line">|   linux|    湖北|武汉|  781347|</span><br><span class="line">| Andriod|    台湾|    |  776642|</span><br><span class="line">|     mac|    台湾|    |  775977|</span><br><span class="line">| Symbain|    台湾|    |  744858|</span><br><span class="line">| windows|    台湾|    |  744558|</span><br><span class="line">| windows|    湖北|武汉|  728412|</span><br><span class="line">| Symbain|    湖北|武汉|  728034|</span><br><span class="line">|   linux|    山西|忻州|  689405|</span><br><span class="line">+--------+--------+----+--------+</span><br></pre></td></tr></table></figure></div>
<p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">Window Functions in Spark SQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">全局排序 ： Api方式   我喜欢的 </span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line">   * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line">   *</span><br><span class="line">   * This is a variant of groupBy that can only group by existing columns using column names</span><br><span class="line">   * (i.e. cannot construct expressions).</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Compute the average for all numeric columns grouped by department.</span><br><span class="line">   *   ds.groupBy(&quot;department&quot;).avg()</span><br><span class="line">   *</span><br><span class="line">   *   // Compute the max age and average salary, grouped by department and gender.</span><br><span class="line">   *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line">   *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line">   *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line">   *   ))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123;</span><br><span class="line">    val colNames: Seq[String] = col1 +: cols</span><br><span class="line">    RelationalGroupedDataset(</span><br><span class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Compute aggregates by specifying a series of aggregate columns. Note that this function by</span><br><span class="line">   * default retains the grouping columns in its output. To not retain grouping columns, set</span><br><span class="line">   * `spark.sql.retainGroupColumns` to false.</span><br><span class="line">   *</span><br><span class="line">   * The available aggregate methods are defined in [[org.apache.spark.sql.functions]].</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Selects the age of the oldest employee and the aggregate expense for each department</span><br><span class="line">   *</span><br><span class="line">   *   // Scala:</span><br><span class="line">   *   import org.apache.spark.sql.functions._</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   // Java:</span><br><span class="line">   *   import static org.apache.spark.sql.functions.*;</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;));</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Note that before Spark 1.4, the default behavior is to NOT retain grouping columns. To change</span><br><span class="line">   * to that behavior, set config variable `spark.sql.retainGroupColumns` to `false`.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Scala, 1.3.x:</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg($&quot;department&quot;, max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   // Java, 1.3.x:</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(col(&quot;department&quot;), max(&quot;age&quot;), sum(&quot;expense&quot;));</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @since 1.3.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def agg(expr: Column, exprs: Column*): DataFrame = &#123;</span><br><span class="line">    toDF((expr +: exprs).map &#123;</span><br><span class="line">      case typed: TypedColumn[_, _] =&gt;</span><br><span class="line">        typed.withInputType(df.exprEnc, df.logicalPlan.output).expr</span><br><span class="line">      case c =&gt; c.expr</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">你要好好看看注释 就会明白下面的代码</span><br><span class="line"></span><br><span class="line">groupBy:Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line">agg : </span><br><span class="line">  * Compute aggregates by specifying a series of aggregate columns. Note that this function by</span><br><span class="line">   * default retains the grouping columns in its output. To not retain grouping columns, set</span><br><span class="line">   * `spark.sql.retainGroupColumns` to false.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Returns a new Dataset sorted by the given expressions. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.sort($&quot;col1&quot;, $&quot;col2&quot;.desc)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sort(sortExprs: Column*): Dataset[T] = &#123;</span><br><span class="line">    sortInternal(global = true, sortExprs)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line">    //需求1 ：统计 每个平台 省市下面 traffic的总和       order by  是全局排序的</span><br><span class="line">        import org.apache.spark.sql.functions._    //spark 内置的函数</span><br><span class="line">        df.groupBy(&quot;platform&quot;, &quot;province&quot;, &quot;city&quot;)</span><br><span class="line">            .agg(sum(&quot;traffic&quot;).as(&quot;traffics&quot;))</span><br><span class="line">            .sort(&apos;traffics.desc)</span><br><span class="line">            .show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Hive的函数 Spark里也是有的 spark自己内置的 </span><br><span class="line">import org.apache.spark.sql.functions._   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果是;</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|platform|province|city|traffics|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|     mac|    香港|    | 2879982|</span><br><span class="line">| windows|    香港|    | 2871537|</span><br><span class="line">| Andriod|    香港|    | 2722363|</span><br><span class="line">|   linux|    香港|    | 2696578|</span><br><span class="line">| Symbain|    香港|    | 2444806|</span><br><span class="line">| Andriod|    山西|忻州|  968255|</span><br><span class="line">|   linux|    台湾|    |  898404|</span><br><span class="line">| windows|    山西|忻州|  894966|</span><br><span class="line">| Andriod|    湖北|武汉|  865758|</span><br><span class="line">|     mac|    湖北|武汉|  848995|</span><br><span class="line">|     mac|    山西|忻州|  837873|</span><br><span class="line">| Symbain|    山西|忻州|  791524|</span><br><span class="line">|   linux|    湖北|武汉|  781347|</span><br><span class="line">| Andriod|    台湾|    |  776642|</span><br><span class="line">|     mac|    台湾|    |  775977|</span><br><span class="line">| Symbain|    台湾|    |  744858|</span><br><span class="line">| windows|    台湾|    |  744558|</span><br><span class="line">| windows|    湖北|武汉|  728412|</span><br><span class="line">| Symbain|    湖北|武汉|  728034|</span><br><span class="line">|   linux|    山西|忻州|  689405|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102813201459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">api方式 开发 你要注意的是：</span><br><span class="line">	Column 和 String  就是你传进去的 列名 要传入 string类型 还是 Column 类型 </span><br><span class="line"></span><br><span class="line">   我个人喜欢 ：</span><br><span class="line">   		 Column  ==》 &apos;列名</span><br><span class="line">   		 String    ==》 “列名”</span><br><span class="line">   毕竟有太多种写法 找一个自己喜欢的  跟找女朋友相反 找女朋友 找一个喜欢自己的  明白吗</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">分组:Top n </span><br><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line"></span><br><span class="line">    // 需求二 ：platform  组内  province 访问次数最多的TopN</span><br><span class="line">    val sql =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |</span><br><span class="line">        |select * from</span><br><span class="line">        |(</span><br><span class="line">        |select t.*, row_number() over(partition by platform order by cnt desc) as r</span><br><span class="line">        |from</span><br><span class="line">        |(select platform,province,count(1) cnt from log group by platform,province) t</span><br><span class="line">        |) a where a.r&lt;=3</span><br><span class="line">        |</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+---+---+</span><br><span class="line">|platform|province|cnt|  r|</span><br><span class="line">+--------+--------+---+---+</span><br><span class="line">|   linux|    香港|606|  1|</span><br><span class="line">|   linux|    广东|211|  2|</span><br><span class="line">|   linux|    台湾|173|  3|</span><br><span class="line">| Symbain|    香港|582|  1|</span><br><span class="line">| Symbain|    广东|222|  2|</span><br><span class="line">| Symbain|    福建|153|  3|</span><br><span class="line">| Andriod|    香港|607|  1|</span><br><span class="line">| Andriod|    广东|223|  2|</span><br><span class="line">| Andriod|    北京|150|  3|</span><br><span class="line">|     mac|    香港|646|  1|</span><br><span class="line">|     mac|    广东|213|  2|</span><br><span class="line">|     mac|    台湾|156|  3|</span><br><span class="line">| windows|    香港|657|  1|</span><br><span class="line">| windows|    广东|186|  2|</span><br><span class="line">| windows|    河北|151|  3|</span><br><span class="line">+--------+--------+---+---+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line">    val sql2 =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |</span><br><span class="line">        |select a.* from</span><br><span class="line">        |(</span><br><span class="line">        |select t.*,</span><br><span class="line">        |row_number() over(partition by platform order by cnt desc) as rn,</span><br><span class="line">        |rank() over(partition by platform order by cnt desc) as r,</span><br><span class="line">        |dense_rank() over(partition by platform order by cnt desc) as dn</span><br><span class="line">        |from</span><br><span class="line">        |(select platform,province,count(1) cnt from log group by platform,province) t</span><br><span class="line">        |) a</span><br><span class="line">        |</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    spark.sql(sql2).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">|platform|province|cnt| rn|  r| dn|</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">|   linux|    香港|606|  1|  1|  1|</span><br><span class="line">|   linux|    广东|211|  2|  2|  2|</span><br><span class="line">|   linux|    台湾|173|  3|  3|  3|</span><br><span class="line">|   linux|    福建|147|  4|  4|  4|</span><br><span class="line">|   linux|    北京|134|  5|  5|  5|</span><br><span class="line">|   linux|    河北|128|  6|  6|  6|</span><br><span class="line">|   linux|    湖北|115|  7|  7|  7|</span><br><span class="line">|   linux|    山西|107|  8|  8|  8|</span><br><span class="line">|   linux|    江西|104|  9|  9|  9|</span><br><span class="line">|   linux|    上海|101| 10| 10| 10|</span><br><span class="line">|   linux|    山东| 26| 11| 11| 11|</span><br><span class="line">| Symbain|    香港|582|  1|  1|  1|</span><br><span class="line">| Symbain|    广东|222|  2|  2|  2|</span><br><span class="line">| Symbain|    福建|153|  3|  3|  3|</span><br><span class="line">| Symbain|    河北|151|  4|  4|  4|</span><br><span class="line">| Symbain|    台湾|146|  5|  5|  5|</span><br><span class="line">| Symbain|    北京|133|  6|  6|  6|</span><br><span class="line">| Symbain|    山西|121|  7|  7|  7|</span><br><span class="line">| Symbain|    湖北|120|  8|  8|  8|</span><br><span class="line">| Symbain|    上海|109|  9|  9|  9|</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">那么： 他们有什么区别？</span><br><span class="line">	row_number   123456  排序的  即使有的值相等 也往下排序</span><br><span class="line">	rank        1233567 排序的  有相同的值  排序号相等 之后会跳过重复的占位 这里就没有4</span><br><span class="line">	dense_rank   12334567 排序的 有相同的值  排序号相等  之后不会跳过重复的占位 这里紧接着4</span><br></pre></td></tr></table></figure></div>

<p><strong>Catalog</strong><br>非常非常重要 spark2.0之后才有的 我开发了一个csv入Hive 就用到了它</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你Hive的元数据存在 MySQl里面的 </span><br><span class="line">如果要代码中使用到元数据 要通过JDBC来取 </span><br><span class="line"></span><br><span class="line">但是2.0版本之后 Spark 提供 Catalog 可以拿到 Hive的元数据</span><br></pre></td></tr></table></figure></div>
<p>开启spark-shell –jars  MySQL驱动</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val catalog = spark.catalog</span><br><span class="line">catalog: org.apache.spark.sql.catalog.Catalog = org.apache.spark.sql.internal.CatalogImpl@672c4e24</span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listDatabases().show</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line">|    name|         description|         locationUri|</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line">| default|Default Hive data...|hdfs://hadoop101:...|</span><br><span class="line">|homework|                    |hdfs://hadoop101:...|</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listDatabases().show(false)</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line">|name    |description          |locationUri                                          |</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line">|default |Default Hive database|hdfs://hadoop101:8020/user/hive/warehouse            |</span><br><span class="line">|homework|                     |hdfs://hadoop101:8020/user/hive/warehouse/homework.db|</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listTables(&quot;homework&quot;).show(false)</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line">|name                             |database|description|tableType|isTemporary|</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line">|access_wide                      |homework|null       |EXTERNAL |false      |</span><br><span class="line">|dwd_platform_stat_info           |homework|null       |MANAGED  |false      |</span><br><span class="line">|jf_tmp                           |homework|null       |MANAGED  |false      |</span><br><span class="line">|ods_domain_traffic_info          |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_log_info                     |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_uid_pid_compression_info     |homework|null       |MANAGED  |false      |</span><br><span class="line">|ods_uid_pid_info                 |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_uid_pid_info_compression_test|homework|null       |EXTERNAL |false      |</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listFunctions().show(5,false)</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">|name|database|description|className                                           |isTemporary|</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">|!   |null    |null       |org.apache.spark.sql.catalyst.expressions.Not       |true       |</span><br><span class="line">|%   |null    |null       |org.apache.spark.sql.catalyst.expressions.Remainder |true       |</span><br><span class="line">|&amp;   |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseAnd|true       |</span><br><span class="line">|*   |null    |null       |org.apache.spark.sql.catalyst.expressions.Multiply  |true       |</span><br><span class="line">|+   |null    |null       |org.apache.spark.sql.catalyst.expressions.Add       |true       |</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listColumns(&quot;homework.dwd_platform_stat_info&quot;).show(false)</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line">|name    |description|dataType|nullable|isPartition|isBucket|</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line">|platform|null       |string  |true    |false      |false   |</span><br><span class="line">|cnt     |null       |int     |true    |false      |false   |</span><br><span class="line">|d       |null       |string  |true    |false      |false   |</span><br><span class="line">|day     |null       |string  |true    |true       |false   |</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">不仅仅这多多哈 catalog 几乎所有的元数据 信息都能搞到 </span><br><span class="line"></span><br><span class="line">但是这些值的返回值 都是 DataSet 接下来 讲讲DataSet</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028140024184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>给你一个使用上面catalog的场景<br>做一个页面：</p>
<p><img src="https://img-blog.csdnimg.cn/20191028140640930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>DataSet</strong><br>这个东西很简单的<br>Untyped Dataset = Row </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">DataSet就是你可以把它当作rdd来操作 </span><br><span class="line"></span><br><span class="line">object DSApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;DSApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;inferSchema&quot;,&quot;true&quot;).csv(&quot;file:///C:/IdeaProjects/spark/data/sale.csv&quot;)</span><br><span class="line">    val ds = df.as[Sales]</span><br><span class="line"></span><br><span class="line">    ds.printSchema()</span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    // ROW  DF弱类型</span><br><span class="line">    //    df.select(&quot;transactionId&quot;).show(false)</span><br><span class="line"></span><br><span class="line">    ds.map(columns =&gt;&#123;</span><br><span class="line">        columns.transactionId match &#123;</span><br><span class="line">        case 111 =&gt;Sales(columns.transactionId,columns.customerId,columns.itemId,columns.amountPaid+200)</span><br><span class="line">        case _ =&gt; Sales(columns.transactionId,columns.customerId,columns.itemId,columns.amountPaid)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Sales(transactionId:Int,customerId:Int,itemId:Int,amountPaid:Double)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- transactionId: integer (nullable = true)</span><br><span class="line"> |-- customerId: integer (nullable = true)</span><br><span class="line"> |-- itemId: integer (nullable = true)</span><br><span class="line"> |-- amountPaid: double (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|transactionId|customerId|itemId|amountPaid|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|          111|         1|     1|     100.0|</span><br><span class="line">|          112|         2|     2|     500.0|</span><br><span class="line">|          113|         3|     3|     400.0|</span><br><span class="line">|          114|         1|     4|     300.0|</span><br><span class="line">|          115|         1|     1|     200.0|</span><br><span class="line">|          116|         1|     2|     700.0|</span><br><span class="line">|          117|         4|     3|     800.0|</span><br><span class="line">|          118|         5|     1|     200.0|</span><br><span class="line">|          119|         3|     4|     200.0|</span><br><span class="line">|          120|         1|     1|     300.0|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|transactionId|customerId|itemId|amountPaid|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|111          |1         |1     |300.0     |</span><br><span class="line">|112          |2         |2     |500.0     |</span><br><span class="line">|113          |3         |3     |400.0     |</span><br><span class="line">|114          |1         |4     |300.0     |</span><br><span class="line">|115          |1         |1     |200.0     |</span><br><span class="line">|116          |1         |2     |700.0     |</span><br><span class="line">|117          |4         |3     |800.0     |</span><br><span class="line">|118          |5         |1     |200.0     |</span><br><span class="line">|119          |3         |4     |200.0     |</span><br><span class="line">|120          |1         |1     |300.0     |</span><br><span class="line">+-------------+----------+------+----------+</span><br></pre></td></tr></table></figure></div>


<p><strong>Interoperating with RDDs</strong><br><a href="http://spark.apache.org/docs/latest/sql-getting-started.html#interoperating-with-rdds" target="_blank" rel="noopener">Interoperating with RDDs</a><br>和RDD的交互操作</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DS  --》 DF   通过 DS.toDF(&quot;列名。。&quot;)</span><br><span class="line">DF--》DS   通过 样例类    df.as[样例类]</span><br><span class="line"></span><br><span class="line">RDD ---》 DF   两种 </span><br><span class="line">  Spark SQL supports two different methods for converting existing RDDs into Datasets.</span><br><span class="line">	1.反射   就是使用case  class  你的case class 定义的就是 table的信息</span><br><span class="line">	2.The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD.</span><br><span class="line">	编程的方式 ：</span><br><span class="line">		就是你的哪个字段什么类型指定好就可以了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1.反射的方式   RDD -》 DF</span><br><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // RDD ==&gt; DF/DS</span><br><span class="line">        val peopleDF = spark.sparkContext</span><br><span class="line">          .textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">          .map(_.split(&quot;,&quot;))</span><br><span class="line">          .map(x =&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">          .toDF()</span><br><span class="line"></span><br><span class="line">        peopleDF.show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+------------+---+</span><br><span class="line">|name        |age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy|25 |</span><br><span class="line">|Kairis      |25 |</span><br><span class="line">|Kite        |32 |</span><br><span class="line">+------------+---+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">2.编程的方式</span><br><span class="line">When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.</span><br><span class="line">  1.Create an RDD of Rows from the original RDD;</span><br><span class="line">  2.Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step </span><br><span class="line">  3.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.</span><br><span class="line"></span><br><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">    val fields: Array[StructField] = schemaString.split(&quot; &quot;).map(fieldName =&gt; &#123;</span><br><span class="line">      StructField(fieldName, StringType)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    val rowRDD: RDD[Row] = peopleRDD.map(_.split(&quot;,&quot;)).map(x=&gt;Row(x(0),x(1).trim))</span><br><span class="line"></span><br><span class="line">    val peopleDF: DataFrame = spark.createDataFrame(rowRDD,schema)</span><br><span class="line"></span><br><span class="line">    //TODO... 业务逻辑</span><br><span class="line">    peopleDF.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+------------+---+</span><br><span class="line">|        name|age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy| 25|</span><br><span class="line">|      Kairis| 25|</span><br><span class="line">|        Kite| 32|</span><br><span class="line">+------------+---+</span><br><span class="line"></span><br><span class="line">官网给的例子 不是很好  难道你们生产上全是 String 类型的么？ 我只能说还真是 我上一家公司就是 </span><br><span class="line">建议哈 统计字段 还是采用标准的int 或者 double类型  </span><br><span class="line">我之前统计的时候 全是String 的 就会出现 指标不准的问题  </span><br><span class="line">我遇到过 同事说用String 很爽 我只能说 是的是的 </span><br><span class="line">在他们眼里 spark不就是写sql吗？ </span><br><span class="line">emmm 统计指标可以的 但是 如果让你做 基础架构开发 呢？ </span><br><span class="line">不要仅仅局限于指标需求哈 那么你这大数据工程师 就是 sql怪</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"> </span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schema = StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;,StringType),</span><br><span class="line">      StructField(&quot;age&quot;,IntegerType)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1).trim.toInt))</span><br><span class="line">      </span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"> case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	Row里面的数据类型 一定要和 schema里的数据类型匹配上</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+------------+---+</span><br><span class="line">|        name|age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy| 25|</span><br><span class="line">|      Kairis| 25|</span><br><span class="line">|        Kite| 32|</span><br><span class="line">+------------+---+</span><br></pre></td></tr></table></figure></div>
<p><strong>UDF</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object UDFApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    /**</span><br><span class="line">      * step1： 定义 注册</span><br><span class="line">      * step2： 使用</span><br><span class="line">      */</span><br><span class="line">    spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/udf.txt&quot;)</span><br><span class="line">      .map(_.split(&quot; &quot;))</span><br><span class="line">      .map(x =&gt; FootballTeam(x(0), x(1)))</span><br><span class="line">      .toDF().createOrReplaceTempView(&quot;teams&quot;)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(&quot;teams_length&quot;,(input:String)=&gt;&#123;</span><br><span class="line">      input.split(&quot;，&quot;).length</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    //统计一个人喜欢的球队的个数</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;select name,teams,teams_length(teams) from teams&quot;).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  case class FootballTeam(name:String, teams:String)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line">|  name|             teams|UDF:teams_length(teams)|</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line">|苍老师 |      喵喵喵，红魔  |                      2|</span><br><span class="line">|    pk|小破车，国足，宅团   |                      3|</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	  spark.udf.register(&quot;teams_length&quot;,(input:String)=&gt;&#123;</span><br><span class="line">      input.split(&quot;，&quot;).length</span><br><span class="line">    &#125;)</span><br><span class="line">def register[RT: TypeTag, A1: TypeTag](name: String, func: Function1[A1, RT]): UserDefinedFunction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Function 就是传进去一个 函数 </span><br><span class="line"></span><br><span class="line">还有一种UDF函数的使用就是 api的方式 </span><br><span class="line"></span><br><span class="line">functions里面 有个 udf 方法 传进去一个函数    再 结合 withColumns 方法 使用 也是一样的</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SparkSQL01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/01/SparkSQL01/">SparkSQL01</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/01/SparkSQL01/" class="article-date">
  <time datetime="2018-02-01T12:06:54.000Z" itemprop="datePublished">2018-02-01</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p>工作当中几乎全用SparkSQL ，RDD用的很少(面试多)<br><strong>SparkSQL误区</strong></p>
<p>Spark SQL is Apache Spark’s module for working with structured data.<br>不要把SparkSQL认为就是处理SQl的 或者认为就是写SQL<br><a href="http://spark.apache.org/sql/" target="_blank" rel="noopener">SparkSQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">误区：</span><br><span class="line">    1）Spark SQL是处理结构化数据</span><br><span class="line">        并不是仅仅能够处理SQL</span><br><span class="line">        SQL仅仅是Spark SQL这个模块的一小部分应用</span><br><span class="line">        API/ExtDS</span><br><span class="line">    2）Uniform Data Access  外部数据源(*****)</span><br><span class="line">        Spark SQL是能够处理多种不同的数据源的数据</span><br><span class="line">            text、json、parquet、orc、hive、jdbc    数据的格式 </span><br><span class="line">            HDFS/S3(a/n)/OSS/COS                数据的存储系统</span><br><span class="line">        不同的数据格式压缩的不压缩的 sparksql都是兼容的 </span><br><span class="line">        你访问不同的数据源SparkSQl都是用统一的访问方式  这就是外部数据源</span><br><span class="line"></span><br><span class="line">SparkSQL能面试的东西 就是两个 ：</span><br><span class="line">	DataFrame 、 外部数据源、catelist </span><br><span class="line"></span><br><span class="line">2.能集成Hive</span><br><span class="line">你的数仓以前是基于Hive来做的 都是Hive的脚本 </span><br><span class="line"> 现在 如果想使用SparkSQL访问Hive的数据 SparkSQL能连接到MetaStore才可以</span><br><span class="line"> (把Hive-site.xml  拷贝到Sparkconf目录下就可以了)</span><br><span class="line"> 因为MetaStore 是 on Hadoop的核心所在 </span><br><span class="line"></span><br><span class="line">所以你要把Hive迁移到Spark上来 成本是很低的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027131424394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3.Standard Connectivity</span><br><span class="line">Hive能通过HiveServer2提供一个服务 大家去查，那么 spark里面有个thriftServer </span><br><span class="line">他们底层都是用thrift协议的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027131724269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">误区3：</span><br><span class="line">MR==&gt;Hive==&gt;  Hive底层当时是MR 慢 所以出来Spark </span><br><span class="line">     Spark==&gt; AMPLab Shark(为了将Hive SQL跑在Spark上)  1.x  配套一个打了补丁的Hive</span><br><span class="line">        Spark1.0  Shark不维护</span><br><span class="line">            ==&gt; Spark SQL 是在Spark里面的</span><br><span class="line">            ==&gt; Hive on Spark 是在Hive里面的      是Hive的引擎是Spark</span><br><span class="line"></span><br><span class="line">误区3）</span><br><span class="line">    Hive on Spark不是Spark SQL</span><br><span class="line">        Hive刚开始时底层执行引擎只有一个：MR</span><br><span class="line">        后期：Tez Spark</span><br><span class="line">        set hive.execution.engine=spark;    就可以 Hive on Spark</span><br><span class="line"></span><br><span class="line">    SparkSQL on Hive  X</span><br></pre></td></tr></table></figure></div>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started" target="_blank" rel="noopener">Hive On Spark</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Time taken: 6.86 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; set hive.execution.engine;</span><br><span class="line">hive.execution.engine=mr</span><br><span class="line">hive (default)&gt; set hive.execution.engine=spark;</span><br><span class="line">hive (default)&gt; set hive.execution.engine;</span><br><span class="line">hive.execution.engine=spark</span><br><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">default</span><br><span class="line">homework</span><br><span class="line">Time taken: 0.008 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个东西了解即可 Hive On Spark 真正生产上用的很少的 </span><br><span class="line">这个东西不是很成熟的</span><br></pre></td></tr></table></figure></div>

<h2 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">出来的时间：</span><br><span class="line"></span><br><span class="line">Spark SQL</span><br><span class="line">    1.0     </span><br><span class="line">    SchemaRDD  ==&gt; Table     RDD(存数据) + schema = Table</span><br><span class="line">    ==&gt; DataFrame  1.2/3     由SchemaRDD  变为DataFrame 原因是 更加 OO</span><br><span class="line">    ==&gt; Dataset    1.6    由DataFrame  变为Dataset 因为 compile-time type safety</span><br></pre></td></tr></table></figure></div>
<p><strong>DataFrame</strong><br>A Dataset is a distributed collection of data.<br>A DataFrame is a Dataset organized into named columns.<br>DataFrame = Dataset[Row]<br>In Scala and Java, a DataFrame is represented by a Dataset of Rows. </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataFrame ：</span><br><span class="line">	1.named columns    就是一个表  包含 列的名字 + 列的类型 </span><br><span class="line">	</span><br><span class="line">Row ： 可以理解为  一行数据 没有scheme的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SparkSession是Spark编程的入口点</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027173652956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Api：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SparkSession：</span><br><span class="line">  /**</span><br><span class="line">   * Executes a SQL query using Spark, returning the result as a `DataFrame`.</span><br><span class="line">   * The dialect that is used for SQL parsing can be configured with &apos;spark.sql.dialect&apos;.</span><br><span class="line">   *</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def sql(sqlText: String): DataFrame = &#123;</span><br><span class="line">    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">	1. returning the result as a `DataFrame`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Dataset：</span><br><span class="line">  /**</span><br><span class="line">   * Displays the top 20 rows of Dataset in a tabular form.</span><br><span class="line">   *</span><br><span class="line">   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</span><br><span class="line">   *                 be truncated and all cells will be aligned right</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def show(truncate: Boolean): Unit = show(20, truncate)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;show tables&quot;).show</span><br><span class="line"></span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| default|  student|      false|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	启动spark-shell的时候  指定MySQL驱动  </span><br><span class="line">	个人建议使用 --jars 指定MySQL驱动 </span><br><span class="line">	不建议把MySQL驱动 直接丢在Spark jar路径里</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">查看Hive里元数据：</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from DBS;</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">| DB_ID | DESC                  | DB_LOCATION_URI                                       | NAME     | OWNER_NAME   | OWNER_TYPE |</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">|     1 | Default Hive database | hdfs://hadoop101:8020/user/hive/warehouse             | default  | public       | ROLE       |</span><br><span class="line">|     6 | NULL                  | hdfs://hadoop101:8020/user/hive/warehouse/homework.db | homework | double_happy | USER       |</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from TBLS;</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER        | RETENTION | SD_ID | TBL_NAME                          | TBL_TYPE       | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT |</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">|      1 |  1568615059 |     1 |                0 | double_happy |         0 |     1 | student                           | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|      8 |  1568616039 |     6 |                0 | double_happy |         0 |     8 | ods_domain_traffic_info           | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|      9 |  1568620410 |     6 |                0 | double_happy |         0 |     9 | ods_uid_pid_info                  | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     17 |  1568860945 |     6 |                0 | double_happy |         0 |    17 | jf_tmp                            | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     21 |  1569056727 |     6 |                0 | double_happy |         0 |    21 | access_wide                       | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     26 |  1569209493 |     6 |                0 | double_happy |         0 |    31 | ods_uid_pid_info_compression_test | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     27 |  1569209946 |     6 |                0 | double_happy |         0 |    32 | ods_uid_pid_compression_info      | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     31 |  1569224142 |     6 |                0 | double_happy |         0 |    36 | dwd_platform_stat_info            | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     53 |  1570957119 |     6 |                0 | double_happy |         0 |    63 | ods_log_info                      | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">9 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell查询Hive里的表：</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select * from homework.dwd_platform_stat_info&quot;).show</span><br><span class="line">+--------+---+--------+--------+                                                </span><br><span class="line">|platform|cnt|       d|     day|</span><br><span class="line">+--------+---+--------+--------+</span><br><span class="line">| Andriod|658|20190921|20190921|</span><br><span class="line">| Symbain|683|20190921|20190921|</span><br><span class="line">|   linux|639|20190921|20190921|</span><br><span class="line">|     mac|652|20190921|20190921|</span><br><span class="line">| windows|640|20190921|20190921|</span><br><span class="line">+--------+---+--------+--------+</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用sparksql 在spark-shell交互 还得写 spark.sql</span><br><span class="line">在spark里 有个 spark-sql  用法和 spark-shell 是一样的</span><br></pre></td></tr></table></figure></div>

<h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a><strong>编程</strong></h2><p><a href="http://spark.apache.org/docs/latest/sql-getting-started.html" target="_blank" rel="noopener">sparksql编程</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1.SparkSession构建</span><br><span class="line"></span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">当然 你spark一些参数如何传进去呢？</span><br><span class="line">提供config传进去</span><br><span class="line">eg ： 你要设置多少个分区呀 等</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027180607177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Data Sources</strong></p>
<p><strong>1.读文本数据</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">1.读文本数据</span><br><span class="line"></span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+---------------+</span><br><span class="line">|          value|</span><br><span class="line">+---------------+</span><br><span class="line">|double_happy,25|</span><br><span class="line">|      Kairis,25|</span><br><span class="line">|        Kite,32|</span><br><span class="line">+---------------+</span><br><span class="line"></span><br><span class="line">1. 但是有一个问题 读取进来的数据   把所有内容</span><br><span class="line">都放到 value这个列 里面去了 </span><br><span class="line">该怎么办？</span><br><span class="line"></span><br><span class="line">2. 上面那种写法读进来的是DF</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def text(spark: SparkSession) = &#123;</span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">        ds.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">读进来的是DS</span><br><span class="line">结果是一样的：</span><br><span class="line">+---------------+</span><br><span class="line">|          value|</span><br><span class="line">+---------------+</span><br><span class="line">|double_happy,25|</span><br><span class="line">|      Kairis,25|</span><br><span class="line">|        Kite,32|</span><br><span class="line">+---------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Loads text files and returns a [[Dataset]] of String. See the documentation on the</span><br><span class="line">   * other overloaded `textFile()` method for more details.</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def textFile(path: String): Dataset[String] = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    textFile(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">可以传入多个路径的    textFile(Seq(path): _*)</span><br></pre></td></tr></table></figure></div>

<p><strong>取出第一列输出出去  注意df 和ds的区别</strong> </p>
<p>df：<br><img src="https://img-blog.csdnimg.cn/20191028083903382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028084355225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028084118930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. df.map  里面是row     ds.map  里面是String</span><br><span class="line"> 2. ds 可以map 里面 x.split </span><br><span class="line">   df 就不可以 </span><br><span class="line"> 那我要取出第一列使用df 该这么办？</span><br><span class="line">这就是 df 和 ds 编程的 最本质的区别   df = ds[Row]</span><br><span class="line"></span><br><span class="line">所以 df 得使用  df.rdd.map  </span><br><span class="line">而且他的返回值是 rdd</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028084634122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val result: RDD[(String, String)] = df.rdd.map(x =&gt; &#123;</span><br><span class="line">      val tmp: String = x.getString(0)</span><br><span class="line">      val splits: Array[String] = tmp.split(&quot;,&quot;)</span><br><span class="line">      (splits(0), splits(1))</span><br><span class="line">    &#125;)</span><br><span class="line">    result.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(double_happy,25)</span><br><span class="line">(Kairis,25)</span><br><span class="line">(Kite,32)</span><br><span class="line"></span><br><span class="line">这个结果不是我们想要的 ，我要的是 把结果写出去 </span><br><span class="line">上面这种是 df的 那么 ds该怎么操作呢？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS: Dataset[(String, String)] = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      (splits(0), splits(1))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028090741224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">那么我们只输出一列 ：</span><br><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      splits(0)</span><br><span class="line">    &#125;)</span><br><span class="line">    resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果 ：ok</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028091025124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">但是 有个问题的 文本格式是非常常用的格式  你只支持  一列输出 有个鬼用</span><br><span class="line"></span><br><span class="line">这个问题该这么解决呢？</span><br><span class="line">  这个问题很重要 前面的 不同类型日志输出  一定是多列的  </span><br><span class="line">  下面讲到压缩  给你一个场景 </span><br><span class="line">  andriod 的 bzip的  ios gzip 的   windos bz2  你该这么办？  这都是常见的需求</span><br></pre></td></tr></table></figure></div>
<p>上面的问题之后再解决 </p>
<p>那么 这个输出的数据也是可以用压缩的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      (splits(0))</span><br><span class="line">    &#125;)</span><br><span class="line">   resultDS.write.option(&quot;compression&quot;,&quot;gzip&quot;).mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102809233182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>注意：<br><img src="https://img-blog.csdnimg.cn/20191028092452531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说这个压缩 codec 是有限制的 </span><br><span class="line">问题：让是输出使用lzo 压缩该怎么办呢？</span><br></pre></td></tr></table></figure></div>
<p><strong>2.读json数据</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191028102115717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    json(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- _corrupt_record: string (nullable = true)</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- branch: string (nullable = true)</span><br><span class="line"> |-- camera_id: string (nullable = true)</span><br><span class="line"> |-- camera_ip: string (nullable = true)</span><br><span class="line"> |-- client_time: struct (nullable = true)</span><br><span class="line"> |    |-- enter_time: long (nullable = true)</span><br><span class="line"> |    |-- exit_time: long (nullable = true)</span><br><span class="line"> |    |-- first_time: long (nullable = true)</span><br><span class="line"> |    |-- last_time: long (nullable = true)</span><br><span class="line"> |-- events: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- host_time: long (nullable = true)</span><br><span class="line"> |    |    |-- name: string (nullable = true)</span><br><span class="line"> |    |    |-- osd_time: long (nullable = true)</span><br><span class="line"> |-- face_id: string (nullable = true)</span><br><span class="line"> |-- gender: long (nullable = true)</span><br><span class="line"> |-- is_new_user: boolean (nullable = true)</span><br><span class="line"> |-- mall_id: string (nullable = true)</span><br><span class="line"> |-- match_photo_index: long (nullable = true)</span><br><span class="line"> |-- match_score: long (nullable = true)</span><br><span class="line"> |-- package_index: long (nullable = true)</span><br><span class="line"> |-- photos: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- frame_time: long (nullable = true)</span><br><span class="line"> |    |    |-- quality: double (nullable = true)</span><br><span class="line"> |    |    |-- url: string (nullable = true)</span><br><span class="line"> |-- process_context: struct (nullable = true)</span><br><span class="line"> |    |-- history_res: string (nullable = true)</span><br><span class="line"> |    |-- temp_res: string (nullable = true)</span><br><span class="line"> |-- process_end_time: long (nullable = true)</span><br><span class="line"> |-- process_start_time: long (nullable = true)</span><br><span class="line"> |-- product_id: string (nullable = true)</span><br><span class="line"> |-- project_id: string (nullable = true)</span><br><span class="line"> |-- race: long (nullable = true)</span><br><span class="line"> |-- request_id: string (nullable = true)</span><br><span class="line"> |-- request_time: long (nullable = true)</span><br><span class="line"> |-- site_id: string (nullable = true)</span><br><span class="line"> |-- status: long (nullable = true)</span><br><span class="line"> |-- temp_id: string (nullable = true)</span><br><span class="line"> |-- tracks: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- box: struct (nullable = true)</span><br><span class="line"> |    |    |    |-- angle: long (nullable = true)</span><br><span class="line"> |    |    |    |-- height: long (nullable = true)</span><br><span class="line"> |    |    |    |-- left: long (nullable = true)</span><br><span class="line"> |    |    |    |-- top: long (nullable = true)</span><br><span class="line"> |    |    |    |-- width: long (nullable = true)</span><br><span class="line"> |    |    |-- host_time: long (nullable = true)</span><br><span class="line"> |    |    |-- index: long (nullable = true)</span><br><span class="line"> |    |    |-- video_time: long (nullable = true)</span><br><span class="line"> |-- user_id: string (nullable = true)</span><br><span class="line"></span><br><span class="line">19/10/28 10:17:49 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &apos;spark.debug.maxToStringFields&apos; in SparkEnv.conf.</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">|_corrupt_record|age|              branch|           camera_id|  camera_ip|         client_time|              events|             face_id|gender|is_new_user|           mall_id|match_photo_index|match_score|package_index|              photos|process_context|process_end_time|process_start_time| product_id|          project_id|race|          request_id| request_time|           site_id|status|temp_id|              tracks|user_id|</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">|           null|  0|low_quality_faceI...|afu-hanghai-yxhqg...|172.16.10.2|[1555054289000, 1...|[[1555073266644, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                0|          0|            9|[[1555054284000, ...|   [null, null]|   1555073288125|     1555073288099|trafficfull|AFU_shanghai_yxhq...|   0|f0cbcac5-60aa-498...|1555073288097|AFU_shanghai_yxhqg|    -1|       |[[[-21, 62, 884, ...|       |</span><br><span class="line">|           null| 23|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055539...|[[1555073289722, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           -1|[[1555055540000, ...|   [null, null]|   1555073292530|     1555073292487|trafficfull|AFU_beijing_xhm_t...|   0|b264f039-431e-4c9...|1555073292487|   AFU_beijing_xhm|     4|       |[[[0, 74, 1656, 1...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            3|[[1555054311000, ...|   [null, null]|   1555073297234|     1555073297137|trafficfull|AFU_shanghai_yxhq...|   1|9e9b0963-96d9-44b...|1555073297136|AFU_shanghai_yxhqg|    -1|       |[[[12, 88, 674, 4...|       |</span><br><span class="line">|           null| 22|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055539...|[[1555073289893, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|            1|[[1555055543000, ...|   [null, null]|   1555073298078|     1555073298034|trafficfull|AFU_beijing_xhm_t...|   0|8703d95e-8ca2-4a8...|1555073298034|   AFU_beijing_xhm|    -1|       |[[[7, 72, 171, 56...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            5|[[1555054311000, ...|   [null, null]|   1555073300572|     1555073300471|trafficfull|AFU_shanghai_yxhq...|   1|163a8256-d832-427...|1555073300471|AFU_shanghai_yxhqg|    -1|       |[[[19, 96, 625, 3...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-cytj-...|172.16.10.2|[0, 0, 1555068460...|[[1555073300056, ...|PROJAFU_beijing_c...|     0|      false|  AFU_beijing_cytj|                0|          0|           -1|                null|           [, ]|   1555073300572|     1555073300572|trafficfull|AFU_beijing_cytj_...|   0|5499d569-4067-42d...|1555073300494|  AFU_beijing_cytj|     4|       |[[[26, 55, 1341, ...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           14|[[1555055520000, ...|   [null, null]|   1555073301353|     1555073301300|trafficfull|AFU_beijing_xhm_t...|   1|80f35edb-e3c9-48f...|1555073301299|   AFU_beijing_xhm|    -1|       |[[[25, 110, 1269,...|       |</span><br><span class="line">|           null| 29|  low_quality_faceId|afu-beijing-cytj-...|172.16.10.2|[0, 0, 1555068461...|[[1555073300728, ...|PROJAFU_beijing_c...|     0|       true|  AFU_beijing_cytj|                0|          0|           -1|[[1555068461000, ...|   [null, null]|   1555073302108|     1555073302059|trafficfull|AFU_beijing_cytj_...|   1|ddbacad2-cd35-4a1...|1555073302058|  AFU_beijing_cytj|     4|       |[[[1, 108, 825, 1...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            6|[[1555054311000, ...|   [null, null]|   1555073302221|     1555073302126|trafficfull|AFU_shanghai_yxhq...|   1|3b0353b3-d5ec-492...|1555073302125|AFU_shanghai_yxhqg|    -1|       |[[[4, 85, 647, 39...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055554...|[[1555073303646, ...|PROJAFU_beijing_x...|     1|       true|   AFU_beijing_xhm|                0|          0|           -1|[[1555055554000, ...|   [null, null]|   1555073305191|     1555073305148|trafficfull|AFU_beijing_xhm_t...|   0|3bd7e125-ac80-4ff...|1555073305148|   AFU_beijing_xhm|     4|       |[[[11, 63, 925, 1...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           17|[[1555055520000, ...|   [null, null]|   1555073306338|     1555073306297|trafficfull|AFU_beijing_xhm_t...|   1|26383dcd-47a4-410...|1555073306297|   AFU_beijing_xhm|    -1|       |[[[11, 101, 1254,...|       |</span><br><span class="line">|           null| 26|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055563...|[[1555073312893, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                1|          0|           -1|[[1555055564000, ...|   [null, null]|   1555073314733|     1555073314663|trafficfull|AFU_beijing_xhm_t...|   0|e8a517a4-bf72-46f...|1555073314663|   AFU_beijing_xhm|     4|       |[[[0, 102, 554, 2...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055566...|[[1555073315646, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073315801|     1555073315801|trafficfull|AFU_beijing_xhm_t...|   0|1256cd65-3100-448...|1555073315797|   AFU_beijing_xhm|     4|       |[[[-8, 79, 1638, ...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           23|[[1555055520000, ...|   [null, null]|   1555073316495|     1555073316453|trafficfull|AFU_beijing_xhm_t...|   1|292ea3d7-cc3b-452...|1555073316453|   AFU_beijing_xhm|    -1|       |[[[27, 99, 1243, ...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055567...|[[1555073316558, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073316856|     1555073316856|trafficfull|AFU_beijing_xhm_t...|   0|caf82eb1-8f49-485...|1555073316856|   AFU_beijing_xhm|     4|       |[[[0, 68, 1695, 3...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055567...|[[1555073316313, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073317218|     1555073317218|trafficfull|AFU_beijing_xhm_t...|   0|868a5c14-903e-461...|1555073317129|   AFU_beijing_xhm|     4|       |[[[9, 129, 993, 3...|       |</span><br><span class="line">|           null| 31|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055564000, 1...|[[1555073302556, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                2|          0|            5|[[1555055558000, ...|   [null, null]|   1555073317592|     1555073317503|trafficfull|AFU_beijing_xhm_t...|   0|bb3b4831-f1db-4f9...|1555073317503|   AFU_beijing_xhm|    -1|       |[[[6, 91, 172, 56...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055569...|[[1555073318311, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073318529|     1555073318529|trafficfull|AFU_beijing_xhm_t...|   0|312c43a4-a247-464...|1555073318529|   AFU_beijing_xhm|     4|       |[[[2, 73, 1024, 1...|       |</span><br><span class="line">|           null| 31|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055564000, 1...|[[1555073302556, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                2|          0|            7|[[1555055558000, ...|   [null, null]|   1555073320823|     1555073320721|trafficfull|AFU_beijing_xhm_t...|   0|da438ef2-daf6-472...|1555073320721|   AFU_beijing_xhm|    -1|       |[[[18, 105, 186, ...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|          -18|[[1555054311000, ...|   [null, null]|   1555073321796|     1555073321700|trafficfull|AFU_shanghai_yxhq...|   1|d16a278f-3ae9-44a...|1555073321700|AFU_shanghai_yxhqg|     4|       |[[[10, 58, 598, 3...|       |</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&quot;is_new_user = true &quot;).show(10)</span><br><span class="line">    </span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).where(&quot;is_new_user = true&quot;).show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">结果是一样的哈 </span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Filters rows using the given SQL expression.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   peopleDs.where(&quot;age &gt; 15&quot;)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def where(conditionExpr: String): Dataset[T] = &#123;</span><br><span class="line">    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">where 底层调用的是 filter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter 和where 里面 有好多中写法 ：</span><br><span class="line">个人喜欢使用 &apos;列名 +判断条件</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028103234302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是报错：  加一个隐式转换</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028103316281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&apos;is_new_user === &quot;true&quot;).show(10)</span><br><span class="line">  &#125;</span><br><span class="line">结果是：</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191028103611462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">写法很多 ：</span><br><span class="line"> def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(df.col(&quot;is_new_user&quot;) === &quot;true&quot;).show(10)</span><br><span class="line">  &#125;</span><br><span class="line">结果是一样的</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">我个人是喜欢 </span><br><span class="line">import spark.implicits._</span><br><span class="line">select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;)  +  .filter(&apos;is_new_user === &quot;true&quot;)    </span><br><span class="line"></span><br><span class="line">这样写代码量少一些</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDF: Dataset[Row] = df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&apos;is_new_user === &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">    resultDF.write</span><br><span class="line">      .mode(SaveMode.Overwrite)</span><br><span class="line">      .format(&quot;json&quot;)</span><br><span class="line">      .save(&quot;file:///C:/IdeaProjects/spark/out-sparksql-json&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    json(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028104324951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">解析json 嵌套 + Sturct类型的 你会么？  给个思路 就是  exploded +打点</span><br></pre></td></tr></table></figure></div>
<p><strong>3.读csv数据</strong><br>csv文件打开是execel能看见的<br><img src="https://img-blog.csdnimg.cn/20191028104719825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- _c0: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line">|                 _c0|</span><br><span class="line">+--------------------+</span><br><span class="line">|pid	pid_type	stor...|</span><br><span class="line">|2637034	GLOBAL	30...|</span><br><span class="line">|127599	GLOBAL	303...|</span><br><span class="line">|2626026	GLOBAL	30...|</span><br><span class="line">|2643291	GLOBAL	30...|</span><br><span class="line">|182310	GLOBAL	303...|</span><br><span class="line">|182310	GLOBAL	303...|</span><br><span class="line">|856248	GLOBAL	303...|</span><br><span class="line">|29052	GLOBAL	3039...|</span><br><span class="line">|29052	GLOBAL	3039...|</span><br><span class="line">+--------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">所以这种处理结果不是我们想要的 </span><br><span class="line">所以处理 csv 文件的时候 需要一些 option 需要我们添加的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- pid	pid_type	store_id	store_name	floor	start_time	end_time	event_type	label_version	channel: string (nullable = true)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">|pid	pid_type	store_id	store_name	floor	start_time	end_time	event_type	label_version	channel|</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">|                                                                       2637034	GLOBAL	30...|</span><br><span class="line">|                                                                       127599	GLOBAL	303...|</span><br><span class="line">|                                                                       2626026	GLOBAL	30...|</span><br><span class="line">|                                                                       2643291	GLOBAL	30...|</span><br><span class="line">|                                                                       182310	GLOBAL	303...|</span><br><span class="line">|                                                                       182310	GLOBAL	303...|</span><br><span class="line">|                                                                       856248	GLOBAL	303...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">表 头出来了  但是不是我们想要的</span><br><span class="line">这个头 就一列  没有分开  所以 还得加option  把头拆开</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;\t&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- pid: string (nullable = true)</span><br><span class="line"> |-- pid_type: string (nullable = true)</span><br><span class="line"> |-- store_id: string (nullable = true)</span><br><span class="line"> |-- store_name: string (nullable = true)</span><br><span class="line"> |-- floor: string (nullable = true)</span><br><span class="line"> |-- start_time: string (nullable = true)</span><br><span class="line"> |-- end_time: string (nullable = true)</span><br><span class="line"> |-- event_type: string (nullable = true)</span><br><span class="line"> |-- label_version: string (nullable = true)</span><br><span class="line"> |-- channel: string (nullable = true)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">|    pid|pid_type|store_id|    store_name|floor|start_time|end_time|event_type|       label_version|channel|</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">|2637034|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  17:38:44|17:39:32|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 127599|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  20:09:26|20:18:03|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|2626026|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  11:38:21|11:38:50|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|2643291|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  21:07:31|21:09:01|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 182310|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  10:41:34|10:41:55|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 182310|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  10:42:02|10:57:19|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 856248|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:49:23|14:56:18|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  13:12:00|13:13:57|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:14:28|14:14:55|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:30:38|14:30:52|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;\t&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    df.select(&quot;pid&quot;,&quot;store_name&quot;).filter($&quot;store_id&quot; === &quot;3039A&quot;)</span><br><span class="line">      .write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;)</span><br><span class="line">      .save(&quot;file:///C:/IdeaProjects/spark/out-sparksql-csv&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028110000707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些 option 参数 我是怎么知道的 ？去源码里找 </span><br><span class="line">CSVOptions 类下面</span><br></pre></td></tr></table></figure></div>
<p><strong>4.读jdbc数据</strong><br>MySQL中的数据是这样的<br><img src="https://img-blog.csdnimg.cn/20191028111043452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def jdbc(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val jdbcDF = spark.read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">        jdbcDF.show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是:</span><br><span class="line">+---------------+-----+---+</span><br><span class="line">|         domain|  url|cnt|</span><br><span class="line">+---------------+-----+---+</span><br><span class="line">|  www.baidu.com| url5|  5|</span><br><span class="line">|  www.baidu.com| url2|  2|</span><br><span class="line">|  www.baidu.com| url4|  4|</span><br><span class="line">|  www.baidu.com| url1|  1|</span><br><span class="line">|  www.baidu.com| url3|  3|</span><br><span class="line">|www.twitter.com| url6|  1|</span><br><span class="line">|www.twitter.com|url10| 11|</span><br><span class="line">|www.twitter.com| url9|  6|</span><br><span class="line">| www.google.com| url2|  2|</span><br><span class="line">| www.google.com| url6|  7|</span><br><span class="line">| www.google.com| url1|  1|</span><br><span class="line">| www.google.com| url8|  7|</span><br><span class="line">+---------------+-----+---+</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank" rel="noopener">JDBC To Other Databases</a><br>官网有好多写法 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line">  def jdbc(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val jdbcDF = spark.read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">        jdbcDF.show()</span><br><span class="line"></span><br><span class="line">    jdbcDF.filter(&apos;domain === &quot;www.google.com&quot;)</span><br><span class="line">      .write.format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn_2&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .save()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：写回MySQL</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show tables;</span><br><span class="line">+--------------------+</span><br><span class="line">| Tables_in_hive_dwd |</span><br><span class="line">+--------------------+</span><br><span class="line">| stat               |</span><br><span class="line">| topn               |</span><br><span class="line">| topn_2             |</span><br><span class="line">+--------------------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from topn_2;</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| domain         | url  | cnt  |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| www.google.com | url2 |    2 |</span><br><span class="line">| www.google.com | url6 |    7 |</span><br><span class="line">| www.google.com | url1 |    1 |</span><br><span class="line">| www.google.com | url8 |    7 |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">但是按照上面写 是不是太恶心了 参数 全都写死的 </span><br><span class="line">通过读取配置文件的方式  ：有很多种写法  这里列出一个</span><br><span class="line"></span><br><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">    def jdbc2(spark: SparkSession) = &#123;</span><br><span class="line">      import spark.implicits._</span><br><span class="line"></span><br><span class="line">      val config = ConfigFactory.load()</span><br><span class="line">      val url = config.getString(&quot;db.default.url&quot;)</span><br><span class="line">      val user = config.getString(&quot;db.default.user&quot;)</span><br><span class="line">      val password = config.getString(&quot;db.default.password&quot;)</span><br><span class="line">      val srcTable = config.getString(&quot;db.default.srctable&quot;)</span><br><span class="line">      val targetTable = config.getString(&quot;db.default.targettable&quot;)</span><br><span class="line"></span><br><span class="line">      val jdbcDF = spark.read</span><br><span class="line">        .format(&quot;jdbc&quot;)</span><br><span class="line">        .option(&quot;url&quot;, url)</span><br><span class="line">        .option(&quot;dbtable&quot;, srcTable)</span><br><span class="line">        .option(&quot;user&quot;, user)</span><br><span class="line">        .option(&quot;password&quot;, password)</span><br><span class="line">        .load()</span><br><span class="line"></span><br><span class="line">          jdbcDF.show()</span><br><span class="line"></span><br><span class="line">      jdbcDF.filter(&apos;domain === &quot;www.google.com&quot;)</span><br><span class="line">        .write.format(&quot;jdbc&quot;)</span><br><span class="line">        .option(&quot;url&quot;, url)</span><br><span class="line">        .option(&quot;dbtable&quot;, targetTable)</span><br><span class="line">        .option(&quot;user&quot;, user)</span><br><span class="line">        .option(&quot;password&quot;, password)</span><br><span class="line">        .save()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc2(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028113030955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from topn_3;</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| domain         | url  | cnt  |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| www.google.com | url2 |    2 |</span><br><span class="line">| www.google.com | url6 |    7 |</span><br><span class="line">| www.google.com | url1 |    1 |</span><br><span class="line">| www.google.com | url8 |    7 |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>


</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/27/k8s-Spark-doublehappy/">k8s-Spark-doublehappy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Kudu-Impala%E6%95%85%E9%9A%9C%E6%A1%88%E4%BE%8B01-double-happy/">Kudu+Impala故障案例01--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink04-double-happy/">Flink04--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink03-double-happy/">Flink03--double_happy</a>
          </li>
        
          <li>
            <a href="/2020/01/05/Flink02-double-happy/">Flink02--double_happy</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://liverrrr.fun/archives" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2020 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/archives" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>