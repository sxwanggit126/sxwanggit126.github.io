<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;2&#x2F;index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/default-avatar.jpg">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlight.css">
</head>

<body>
  <div id="fullpage" class="mobile-nav-right">
    
      <div id="wrapper" title="图片来自网络">
    
    
      <header id="header">
  <div id="nav-toggle" class="nav-toggle"></div>
  <div class="head-box global-width">
    <nav class="nav-box nav-right">
      
        <a class="nav-item" href="/" title
        
        >首页</a>
      
        <a class="nav-item" href="/archives" title
        
        >归档</a>
      
    </nav>
  </div>
</header>
      <div id="middlecontent" title class="global-width sidebar-right">
        <section id="main">
  
    <article id="post-SparkSQL03" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/SparkSQL03/">SparkSQL03</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/05/SparkSQL03/" class="article-date">
  <time datetime="2018-02-05T12:08:32.000Z" itemprop="datePublished">2018-02-05</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">在object LogApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local[4]&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    // ETL: 一定保留原有的数据   最完整</span><br><span class="line">    var inputDF = spark.read.json(&quot;data/data-test.json&quot;)</span><br><span class="line">   inputDF = inputDF.withColumn(&quot;province&quot;, MydataUDF.getProvince(inputDF.col(&quot;ip&quot;)))</span><br><span class="line">   inputDF = inputDF.withColumn(&quot;city&quot;, MydataUDF.getCity(inputDF.col(&quot;ip&quot;)))</span><br><span class="line"></span><br><span class="line">    // ETL==&gt;ODS</span><br><span class="line">  //  inputDF.coalesce(1).write.format(&quot;parquet&quot;)     //orc /parquet</span><br><span class="line"> //     .option(&quot;compression&quot;,&quot;snappy&quot;).save(&quot; path&quot;)    //别使用snappy 用lzo</span><br><span class="line"></span><br><span class="line">    inputDF.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line"></span><br><span class="line">    spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;,&quot;400&quot;) // --conf  这个东西不建议写在代码里 建议写在 spark-submit --conf 那块 明白吗？</span><br><span class="line">    </span><br><span class="line">    val areaSQL01 = &quot;select province,city, &quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode &gt;=1 then 1 else 0 end) origin_request,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode &gt;=2 then 1 else 0 end) valid_request,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=1 and processnode =3 then 1 else 0 end) ad_request,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and isbid=1 and adorderid!=0 then 1 else 0 end) bid_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 then 1 else 0 end) bid_success_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=2 and iseffective=1 then 1 else 0 end) ad_display_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=3 and processnode=1 then 1 else 0 end) ad_click_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=2 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_display_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when requestmode=3 and iseffective=1 and isbilling=1 then 1 else 0 end) medium_click_cnt,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*winprice/1000 else 0 end) ad_consumption,&quot; +</span><br><span class="line">      &quot;sum(case when adplatformproviderid&gt;=100000 and iseffective=1 and isbilling=1 and iswin=1 and adorderid&gt;20000  then 1*adpayment/1000 else 0 end) ad_cost &quot; +</span><br><span class="line">      &quot;from log group by province,city&quot;</span><br><span class="line">    spark.sql(areaSQL01).show(false)//.createOrReplaceTempView(&quot;area_tmp&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val areaSQL02 = &quot;select province,city, &quot; +</span><br><span class="line">      &quot;origin_request,&quot; +</span><br><span class="line">      &quot;valid_request,&quot; +</span><br><span class="line">      &quot;ad_request,&quot; +</span><br><span class="line">      &quot;bid_cnt,&quot; +</span><br><span class="line">      &quot;bid_success_cnt,&quot; +</span><br><span class="line">      &quot;bid_success_cnt/bid_cnt bid_success_rate,&quot; +</span><br><span class="line">      &quot;ad_display_cnt,&quot; +</span><br><span class="line">      &quot;ad_click_cnt,&quot; +</span><br><span class="line">      &quot;ad_click_cnt/ad_display_cnt ad_click_rate,&quot; +</span><br><span class="line">      &quot;ad_consumption,&quot; +</span><br><span class="line">      &quot;ad_cost from area_tmp &quot; +</span><br><span class="line">      &quot;where bid_cnt!=0 and ad_display_cnt!=0&quot;</span><br><span class="line"></span><br><span class="line">    Thread.sleep(10000)</span><br><span class="line"></span><br><span class="line">    spark.sql(areaSQL02).show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object MydataUDF &#123;</span><br><span class="line"></span><br><span class="line">  import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">  def getProvince = udf((ip:String)=&gt;&#123;</span><br><span class="line">    IPUtil.getInstance().getInfos(ip)(1)</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  def getCity = udf((ip:String)=&gt;&#123;</span><br><span class="line">    IPUtil.getInstance().getInfos(ip)(2)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">问题：</span><br><span class="line">有什么问题？</span><br><span class="line">1.spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;,&quot;400&quot;) // --conf  </span><br><span class="line">这个东西不建议写在代码里 建议写在 spark-submit --conf 那块 明白吗？ 或者通过代码判断输入值 eg:400</span><br><span class="line"></span><br><span class="line">2. inputDF.coalesce(1).write.format(&quot;parquet&quot;)     //orc /parquet</span><br><span class="line">.option(&quot;compression&quot;,&quot;snappy&quot;).save(&quot; path&quot;)    //别使用snappy 用lzo</span><br><span class="line">spark默认是snappy 别用哈 看压缩篇</span><br><span class="line"></span><br><span class="line">coalesce(1) 这个值 看下面给的建议  处理小文件的</span><br></pre></td></tr></table></figure></div>

<p>演示上面代码可能的问题<br><img src="https://img-blog.csdnimg.cn/2019102822020751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028220302437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">这200 哪里来的 ？</span><br><span class="line"></span><br><span class="line">    官网 sparksql 调优章节 </span><br><span class="line">1.spark.sql.shuffle.partitions 参数 默认 200   这是sparksql里面 设置的shuffle参数 </span><br><span class="line"></span><br><span class="line">2.RDD里的 reduceByKey(，numPartitions）还有印象吗？rdd是在这里设置的 </span><br><span class="line"></span><br><span class="line">sparksql 默认200  生产上绝对是不够的 只要你数据量稍微大一点 200个 一定是扛不住的 </span><br><span class="line"></span><br><span class="line">这个参数 你可以理解为mapreduce里的reduce的数量 ，reduce数量如果大了 </span><br><span class="line">会导致上面问题？程序跑起来是快了 但是 小文件过多 </span><br><span class="line"></span><br><span class="line">那么这个值 该怎么设置呢？</span><br><span class="line">给你个思路 估计你读进来的数据量大小 + 你预估你每个task处理的数据量是多少 </span><br><span class="line">来设计 这个值</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">还有一点就是：</span><br><span class="line">加入这个值是400  </span><br><span class="line">400</span><br><span class="line">    大：小文件多点、</span><br><span class="line">    10exe * 2core = 20task   同一时间点 20个task</span><br><span class="line">               400/20=20轮</span><br><span class="line">               600/20=30轮</span><br></pre></td></tr></table></figure></div>


<p><strong>ETL</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ETL</span><br><span class="line">    input:json</span><br><span class="line">    清洗 ==&gt; ODS  大宽表  HDFS/Hive/SparkSQL</span><br><span class="line">    output: 列式存储  ORC/Parquet   这块一定是要落地的 </span><br><span class="line"></span><br><span class="line">    Stat</span><br><span class="line">        ==&gt;  一个非常简单的SQL搞定</span><br><span class="line">        ==&gt;  复杂：多个SQL 或者 一个复杂SQL搞定</span><br></pre></td></tr></table></figure></div>
<p><a href="https://developer.ibm.com/hadoop/2016/01/14/5-reasons-to-choose-parquet-for-spark-sql/" target="_blank" rel="noopener">Choose Parquet for Spark SQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">行式存储：MySQL</span><br><span class="line">    一条记录有多个列  一行数据是存储在一起的</span><br><span class="line">    优点：</span><br><span class="line">        你每次查询都使用到所有的列</span><br><span class="line">    缺点：</span><br><span class="line">        大宽表有N多列，但是我们仅仅使用其中几列</span><br><span class="line"> 	</span><br><span class="line"> 	因为我使用大宽表(有100列)的时候 假如只用到其中的3个列，</span><br><span class="line"> 	如果我使用 行式存储   加载数据的时候会把 你一行的所有列都加载出来 意味着浪费了97%资源</span><br><span class="line"> </span><br><span class="line"> 列式存储很好的解决这个问题</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">列式：Orc Parquet</span><br><span class="line">    特点：把每一列的数据存放在一起</span><br><span class="line">    优点：减少IO 需要哪几列就直接获取哪几列</span><br><span class="line">    缺点：如果你还是要获取每一行中的所有列，那么性能比行式的差</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102821264517.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用行式存储 spark跑程序的时候官网也列举了很多问题 </span><br><span class="line">eg：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028213942626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">那么Most of these failures force Spark to re-try by re-queuing tasks：</span><br><span class="line">spark会重试跑失败的task </span><br><span class="line">注意：</span><br><span class="line">	重试 一般是跑不出来的  如果没有倾斜 和资源够 可能会跑出来</span><br><span class="line">	假设10个task 3个task挂掉了 那么重新起的task 你能确定 </span><br><span class="line">	重启来的task 会在 3个task之前挂掉的executor上面么？</span><br><span class="line">	不能确定 很可能起到别的executor上面 </span><br><span class="line">	（别的executor 可能现在也在跑 其余7个task中的某些task）</span><br><span class="line">	对于这个 executor压力更大 可能会导致你的应用程序被干掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">存储是结合 压缩来用的   eg：orc + lzo</span><br><span class="line">减少disk io</span><br></pre></td></tr></table></figure></div>

<h2 id="beeline-jdbc"><a href="#beeline-jdbc" class="headerlink" title="beeline/jdbc"></a>beeline/jdbc</h2><p>生产上是用的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hiveserver2  beeline/jdbc     Hive里的 </span><br><span class="line">thriftserver beeline/jdbc     spark里的 </span><br><span class="line"></span><br><span class="line">怎么用呢？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ ./start-thriftserver.sh --jars ~/software/mysql-connector-java-5.1.47.jar </span><br><span class="line">starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop101.out</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ tail -200f /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-hadoop101.out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Spark Command: /usr/java/java/bin/java -cp /home/double_happy/app/spark/conf/:/home/double_happy/app/spark/jars/*:/home/double_happy/app/hadoop/etc/hadoop/ -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server --jars /home/double_happy/software/mysql-connector-java-5.1.47.jar spark-internal</span><br><span class="line">========================================</span><br><span class="line">19/10/28 22:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/28 22:26:56 INFO HiveThriftServer2: Started daemon with process name: 2297@hadoop101</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for TERM</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for HUP</span><br><span class="line">19/10/28 22:26:56 INFO SignalUtils: Registered signal handler for INT</span><br><span class="line">19/10/28 22:26:56 INFO HiveThriftServer2: Starting SparkContext</span><br><span class="line">19/10/28 22:26:56 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/10/28 22:26:56 INFO SparkContext: Submitted application: Thrift JDBC/ODBC Server</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/28 22:26:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/28 22:26:57 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 35237.</span><br><span class="line">19/10/28 22:26:57 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/10/28 22:26:57 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/10/28 22:26:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/10/28 22:26:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/10/28 22:26:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb4561a2-a2c1-42a6-a313-96e3ff47a7fb</span><br><span class="line">19/10/28 22:26:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/10/28 22:26:58 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/10/28 22:26:58 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/10/28 22:26:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/10/28 22:26:58 INFO SparkContext: Added JAR file:///home/double_happy/software/mysql-connector-java-5.1.47.jar at spark://hadoop101:35237/jars/mysql-connector-java-5.1.47.jar with timestamp 1572272818597</span><br><span class="line">19/10/28 22:26:58 INFO Executor: Starting executor ID driver on host localhost</span><br><span class="line">19/10/28 22:26:59 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 33661.</span><br><span class="line">19/10/28 22:26:59 INFO NettyBlockTransferService: Server created on hadoop101:33661</span><br><span class="line">19/10/28 22:26:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:33661 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:26:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 33661, None)</span><br><span class="line">19/10/28 22:27:01 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/local-1572272818750</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: loading hive config file: file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/conf/hive-site.xml</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: Setting hive.metastore.warehouse.dir (&apos;null&apos;) to the value of spark.sql.warehouse.dir (&apos;file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse&apos;).</span><br><span class="line">19/10/28 22:27:01 INFO SharedState: Warehouse path is &apos;file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse&apos;.</span><br><span class="line">19/10/28 22:27:01 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</span><br><span class="line">19/10/28 22:27:03 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/28 22:27:03 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:03 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/28 22:27:03 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/28 22:27:05 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/28 22:27:07 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:07 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:08 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/28 22:27:08 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/28 22:27:08 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:08 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/28 22:27:08 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:09 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_functions: db=homework pat=*</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=homework pat=*</span><br><span class="line">19/10/28 22:27:09 INFO HiveMetaStore: 0: get_function: homework.add_prefix_new</span><br><span class="line">19/10/28 22:27:09 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.add_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO HiveMetaStore: 0: get_function: homework.remove_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.remove_prefix_new</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created local directory: /tmp/41aaf1c8-5deb-45c7-9c03-ef172a6058a3_resources</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created local directory: /tmp/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3</span><br><span class="line">19/10/28 22:27:10 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/41aaf1c8-5deb-45c7-9c03-ef172a6058a3/_tmp_space.db</span><br><span class="line">19/10/28 22:27:10 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse</span><br><span class="line">19/10/28 22:27:10 INFO HiveMetaStore: 0: get_database: default</span><br><span class="line">19/10/28 22:27:10 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_database: default</span><br><span class="line">19/10/28 22:27:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">19/10/28 22:27:10 INFO HiveUtils: Initializing execution hive, version 1.2.1</span><br><span class="line">19/10/28 22:27:11 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/28 22:27:11 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:11 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/28 22:27:11 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/28 22:27:14 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/28 22:27:15 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:15 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</span><br><span class="line">19/10/28 22:27:17 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0</span><br><span class="line">19/10/28 22:27:17 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/28 22:27:18 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created local directory: /tmp/5fe8af31-b32b-4788-a587-4fbf6cab7b1a_resources</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created local directory: /tmp/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a</span><br><span class="line">19/10/28 22:27:18 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/5fe8af31-b32b-4788-a587-4fbf6cab7b1a/_tmp_space.db</span><br><span class="line">19/10/28 22:27:18 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/sbin/spark-warehouse</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: Operation log root directory is created: /tmp/double_happy/operation_logs</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread pool size: 100</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100</span><br><span class="line">19/10/28 22:27:18 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:OperationManager is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:SessionManager is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service: CLIService is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:ThriftBinaryCLIService is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service: HiveServer2 is inited.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:OperationManager is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:SessionManager is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:CLIService is started.</span><br><span class="line">19/10/28 22:27:18 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/28 22:27:18 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/28 22:27:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY</span><br><span class="line">19/10/28 22:27:18 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: get_databases: default</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_databases: default</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: Shutting down the object store...</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Shutting down the object store...</span><br><span class="line">19/10/28 22:27:18 INFO HiveMetaStore: 0: Metastore shutdown complete.</span><br><span class="line">19/10/28 22:27:18 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Metastore shutdown complete.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:ThriftBinaryCLIService is started.</span><br><span class="line">19/10/28 22:27:18 INFO AbstractService: Service:HiveServer2 is started.</span><br><span class="line">19/10/28 22:27:18 INFO HiveThriftServer2: HiveThriftServer2 started</span><br><span class="line">19/10/28 22:27:18 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">说明 thriftserver 启动起来了 </span><br><span class="line"></span><br><span class="line">sparkui端口 参数</span><br><span class="line">spark.port.maxRetries 16 默认16 也就是同一时间点对一台机器 只能起16个spark-submit </span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 software]$ jps</span><br><span class="line">2496 Jps</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">6633 HistoryServer</span><br><span class="line">2297 SparkSubmit</span><br><span class="line">3721 NameNode</span><br><span class="line">4186 ResourceManager</span><br><span class="line">3853 DataNode</span><br><span class="line">[double_happy@hadoop101 software]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">也就是这个 2297 SparkSubmit  最多16个</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028223316966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">这是 thriftserver 端起来了 说明服务端有了</span><br><span class="line">所以接下来要通过客户端 连接一下 </span><br><span class="line">客户端怎么链接呢？</span><br><span class="line">使用beeline     用法跟Hive里是一毛一样的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/ruozedata_g7 -n double_happy</span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/ruozedata_g7</span><br><span class="line">19/10/28 22:40:56 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:40:56 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:40:56 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/ruozedata_g7</span><br><span class="line">Error: Database &apos;ruozedata_g7&apos; not found; (state=,code=0)</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/ruozedata_g7 (closed)&gt; ^C^C[double_happy@hadoop101 bin]$ </span><br><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/ -n double_happy            </span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/</span><br><span class="line">19/10/28 22:42:15 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:42:15 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:42:16 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/</span><br><span class="line">Connected to: Spark SQL (version 2.4.4)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1.spark2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; show databases;</span><br><span class="line">+---------------+--+</span><br><span class="line">| databaseName  |</span><br><span class="line">+---------------+--+</span><br><span class="line">| default       |</span><br><span class="line">| homework      |</span><br><span class="line">+---------------+--+</span><br><span class="line">2 rows selected (1.206 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; use homework;</span><br><span class="line">+---------+--+</span><br><span class="line">| Result  |</span><br><span class="line">+---------+--+</span><br><span class="line">+---------+--+</span><br><span class="line">No rows selected (0.181 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt; show tables;</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| database  |             tableName              | isTemporary  |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| homework  | access_wide                        | false        |</span><br><span class="line">| homework  | dwd_platform_stat_info             | false        |</span><br><span class="line">| homework  | jf_tmp                             | false        |</span><br><span class="line">| homework  | ods_domain_traffic_info            | false        |</span><br><span class="line">| homework  | ods_log_info                       | false        |</span><br><span class="line">| homework  | ods_uid_pid_compression_info       | false        |</span><br><span class="line">| homework  | ods_uid_pid_info                   | false        |</span><br><span class="line">| homework  | ods_uid_pid_info_compression_test  | false        |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">8 rows selected (0.202 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./beeline -u jdbc:hive2://hadoop101:10000/homework -n double_happy</span><br><span class="line">Connecting to jdbc:hive2://hadoop101:10000/homework</span><br><span class="line">19/10/28 22:43:14 INFO Utils: Supplied authorities: hadoop101:10000</span><br><span class="line">19/10/28 22:43:14 INFO Utils: Resolved authority: hadoop101:10000</span><br><span class="line">19/10/28 22:43:14 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://hadoop101:10000/homework</span><br><span class="line">Connected to: Spark SQL (version 2.4.4)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1.spark2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 1.2.1.spark2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/homework&gt; show tables;</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| database  |             tableName              | isTemporary  |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">| homework  | access_wide                        | false        |</span><br><span class="line">| homework  | dwd_platform_stat_info             | false        |</span><br><span class="line">| homework  | jf_tmp                             | false        |</span><br><span class="line">| homework  | ods_domain_traffic_info            | false        |</span><br><span class="line">| homework  | ods_log_info                       | false        |</span><br><span class="line">| homework  | ods_uid_pid_compression_info       | false        |</span><br><span class="line">| homework  | ods_uid_pid_info                   | false        |</span><br><span class="line">| homework  | ods_uid_pid_info_compression_test  | false        |</span><br><span class="line">+-----------+------------------------------------+--------------+--+</span><br><span class="line">8 rows selected (0.352 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop101:10000/homework&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">这个东西适用在哪里呢？</span><br><span class="line"></span><br><span class="line">你的数据是通过UI去访问的：eg：HUE/Zeppelin  (他们后台都有一个服务的 )</span><br><span class="line">   </span><br><span class="line">   之后可以写一个 jdbc代码 (跟hive里是一模一样的  把你的sql 发到服务 服务给你返回结果 </span><br><span class="line">    通过你的ui界面 把数据结果渲染出来 )</span><br><span class="line">    </span><br><span class="line">    如果你发的SQL是一个计算/统计SQL：返回肯定是需要时间</span><br><span class="line">    只拿结果，不计算</span><br></pre></td></tr></table></figure></div>
<p>参考官网<a href="http://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#distributed-sql-engine" target="_blank" rel="noopener">Distributed SQL Engine</a></p>
<p><strong>Spark On Yarn</strong><br><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Running Spark on YARN</a></p>
<p>There are two deploy modes that can be used to launch Spark applications on YARN. In <strong>cluster mode,</strong> the Spark <strong>driver runs inside an application master process</strong> which is managed by YARN on the cluster, and <strong>the client can go away</strong> after initiating the application. In <strong>client mode</strong>, <strong>the driver runs in the client process</strong>, and the application master is only used for requesting resources from YARN.</p>
<p><img src="https://img-blog.csdnimg.cn/20191029101322751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>client模式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191029103139260.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在Spark on YARN中  是没有Worker的概念，是Standalone中的</span><br><span class="line"></span><br><span class="line">Spark on YARN client ：</span><br><span class="line">   1.executor是运行在container中的</span><br><span class="line">   2.driver是跑在本地的</span><br></pre></td></tr></table></figure></div>
<p><strong>cluster模式：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191029103558943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">spark on yarn 总结：</span><br><span class="line">Spark：Driver + Executors</span><br><span class="line"></span><br><span class="line">spark on yarn</span><br><span class="line">    cluster</span><br><span class="line">        driver是运行在AM里面的</span><br><span class="line">        AM：AM + Driver   既当爹又当妈 就是既要给executor发task和代码 也要申请资源</span><br><span class="line">        客户端退出   ？作业是没事的 </span><br><span class="line">        日志 是在YARN上的 ***  本地是看不见的 </span><br><span class="line">            yarn logs -applicationId &lt;app ID&gt;</span><br><span class="line"></span><br><span class="line">    client</span><br><span class="line">        driver是运行在本地的      </span><br><span class="line">        客户端退出  作业就退出了</span><br><span class="line">        AM：负责从YARN上去申请资源</span><br><span class="line">        日志是在本地的 ***   方便查看 </span><br><span class="line"></span><br><span class="line">1.</span><br><span class="line">  但是 日志在本地会有一个场景 本地的进程是有一定的限制的  </span><br><span class="line">加入你提交多个作业 都是以yarn client模式 那么 进程可能扎堆出现 机器可能会挂掉 </span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">[double_happy@hadoop101 ~]$ jps</span><br><span class="line">4289 NodeManager</span><br><span class="line">4019 SecondaryNameNode</span><br><span class="line">14999 AzkabanSingleServer</span><br><span class="line">17719 CoarseGrainedExecutorBackend</span><br><span class="line">6633 HistoryServer</span><br><span class="line">3721 NameNode</span><br><span class="line">17689 CoarseGrainedExecutorBackend</span><br><span class="line">4186 ResourceManager</span><br><span class="line">17517 SparkSubmit</span><br><span class="line">17645 ExecutorLauncher</span><br><span class="line">3853 DataNode</span><br><span class="line">17966 Jps</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">这是在本地 client 就提交作业 CoarseGrainedExecutorBackend 扎堆出现 多了 机器可能会挂掉</span><br><span class="line"></span><br><span class="line">2.driver 和 executor是有通信的 client模式 下 可能会有一种场景存在</span><br><span class="line">driver可以在任意一台机器上面 但是如果这个机器 不是 集群里的机器 (跟yarn 没有关系哈 这里只讨论机器和集群)</span><br><span class="line">如果这机器是在 集群外 这台机器一定是有集群的 gateway权限的 </span><br><span class="line">driver 和 executor是有通信的 网络会用影响 </span><br><span class="line">工作中在集群外的 很少哈 这里只是说一下这个场景 </span><br><span class="line"></span><br><span class="line">集群内带宽 很高 上面的场景影响不大 </span><br><span class="line"></span><br><span class="line">3.就是client模式就一个弱点 就是 本地进程太多</span><br></pre></td></tr></table></figure></div>

<p><strong>测试：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --help</span><br><span class="line">Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 ~]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line"></span><br><span class="line">spark-shell --master yarn  默认不写  --deploy-mode 是 client模式</span><br></pre></td></tr></table></figure></div>
<p>client模式：测试</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --master yarn</span><br><span class="line">19/10/29 10:41:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;ERROR&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Spark context Web UI available at http://hadoop101:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = yarn, app id = application_1570934113711_0037).</span><br><span class="line">Spark session available as &apos;spark&apos;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4</span><br><span class="line">      /_/</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102910505030.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1. 写代码跟运行模式是没有关系的 </span><br><span class="line"> 2.  --num-executors   默认是2 个</span><br><span class="line"> 3. id 是application_xxx 开头的必然是 yarn 模式的 去historyserver看见这个开头的 就是yarn模式跑的任务</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191029104917905.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-sql --jars ~/software/mysql-connector-java-5.1.47.jar --master yarn</span><br><span class="line">19/10/29 10:54:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/29 10:54:19 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/29 10:54:19 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/29 10:54:20 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored</span><br><span class="line">19/10/29 10:54:20 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored</span><br><span class="line">19/10/29 10:54:21 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:23 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/29 10:54:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/29 10:54:23 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: Added admin role in metastore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: Added public role in metastore</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: No user is added in admin role, since config is empty</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_all_databases</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_all_databases</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_functions: db=default pat=*</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=default pat=*</span><br><span class="line">19/10/29 10:54:24 INFO Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_functions: db=homework pat=*</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_functions: db=homework pat=*</span><br><span class="line">19/10/29 10:54:24 INFO HiveMetaStore: 0: get_function: homework.add_prefix_new</span><br><span class="line">19/10/29 10:54:24 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.add_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO HiveMetaStore: 0: get_function: homework.remove_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_function: homework.remove_prefix_new</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created local directory: /tmp/eed4bfce-e4a6-4683-81a4-9bda791d7822_resources</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created local directory: /tmp/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822</span><br><span class="line">19/10/29 10:54:25 INFO SessionState: Created HDFS directory: /tmp/hive/double_happy/eed4bfce-e4a6-4683-81a4-9bda791d7822/_tmp_space.db</span><br><span class="line">19/10/29 10:54:25 INFO SparkContext: Running Spark version 2.4.4</span><br><span class="line">19/10/29 10:54:25 INFO SparkContext: Submitted application: SparkSQL::172.26.162.56</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/29 10:54:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/29 10:54:26 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 44153.</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line">19/10/29 10:54:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span><br><span class="line">19/10/29 10:54:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span><br><span class="line">19/10/29 10:54:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73b7d763-b5a3-476a-a93c-d259c46eac97</span><br><span class="line">19/10/29 10:54:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB</span><br><span class="line">19/10/29 10:54:26 INFO SparkEnv: Registering OutputCommitCoordinator</span><br><span class="line">19/10/29 10:54:26 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.</span><br><span class="line">19/10/29 10:54:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop101:4040</span><br><span class="line">19/10/29 10:54:27 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">19/10/29 10:54:27 INFO Client: Requesting a new application from cluster with 1 NodeManagers</span><br><span class="line">19/10/29 10:54:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)</span><br><span class="line">19/10/29 10:54:27 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead</span><br><span class="line">19/10/29 10:54:27 INFO Client: Setting up container launch context for our AM</span><br><span class="line">19/10/29 10:54:27 INFO Client: Setting up the launch environment for our AM container</span><br><span class="line">19/10/29 10:54:27 INFO Client: Preparing resources for our AM container</span><br><span class="line">19/10/29 10:54:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">19/10/29 10:54:33 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_libs__4819705212614105474.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_libs__4819705212614105474.zip</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/home/double_happy/software/mysql-connector-java-5.1.47.jar -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/mysql-connector-java-5.1.47.jar</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_conf__8084381853282513804.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_conf__.zip</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/29 10:54:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/29 10:54:37 INFO Client: Submitting application application_1570934113711_0038 to ResourceManager</span><br><span class="line">19/10/29 10:54:37 INFO YarnClientImpl: Submitted application application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:37 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1570934113711_0038 and attemptId None</span><br><span class="line">19/10/29 10:54:38 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:38 INFO Client: </span><br><span class="line">         client token: N/A</span><br><span class="line">         diagnostics: N/A</span><br><span class="line">         ApplicationMaster host: N/A</span><br><span class="line">         ApplicationMaster RPC port: -1</span><br><span class="line">         queue: root.double_happy</span><br><span class="line">         start time: 1572317677273</span><br><span class="line">         final status: UNDEFINED</span><br><span class="line">         tracking URL: http://hadoop101:8088/proxy/application_1570934113711_0038/</span><br><span class="line">         user: double_happy</span><br><span class="line">19/10/29 10:54:39 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:40 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:41 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:42 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:43 INFO Client: Application report for application_1570934113711_0038 (state: ACCEPTED)</span><br><span class="line">19/10/29 10:54:44 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hadoop101, PROXY_URI_BASES -&gt; http://hadoop101:8088/proxy/application_1570934113711_0038), /proxy/application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:44 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.</span><br><span class="line">19/10/29 10:54:44 INFO Client: Application report for application_1570934113711_0038 (state: RUNNING)</span><br><span class="line">19/10/29 10:54:44 INFO Client: </span><br><span class="line">         client token: N/A</span><br><span class="line">         diagnostics: N/A</span><br><span class="line">         ApplicationMaster host: 172.26.162.56</span><br><span class="line">         ApplicationMaster RPC port: -1</span><br><span class="line">         queue: root.double_happy</span><br><span class="line">         start time: 1572317677273</span><br><span class="line">         final status: UNDEFINED</span><br><span class="line">         tracking URL: http://hadoop101:8088/proxy/application_1570934113711_0038/</span><br><span class="line">         user: double_happy</span><br><span class="line">19/10/29 10:54:44 INFO YarnClientSchedulerBackend: Application application_1570934113711_0038 has started running.</span><br><span class="line">19/10/29 10:54:44 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 38768.</span><br><span class="line">19/10/29 10:54:44 INFO NettyBlockTransferService: Server created on hadoop101:38768</span><br><span class="line">19/10/29 10:54:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:38768 with 366.3 MB RAM, BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop101, 38768, None)</span><br><span class="line">19/10/29 10:54:44 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)</span><br><span class="line">19/10/29 10:54:45 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.</span><br><span class="line">19/10/29 10:54:45 INFO EventLoggingListener: Logging events to hdfs://hadoop101:8020/spark_directory/application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:50 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.162.56:59428) with ID 1</span><br><span class="line">19/10/29 10:54:50 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:36737 with 366.3 MB RAM, BlockManagerId(1, hadoop101, 36737, None)</span><br><span class="line">19/10/29 10:54:52 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.26.162.56:34236) with ID 2</span><br><span class="line">19/10/29 10:54:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: loading hive config file: file:/home/double_happy/app/spark-2.4.4-bin-2.6.0-cdh5.15.1/conf/hive-site.xml</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: Setting hive.metastore.warehouse.dir (&apos;null&apos;) to the value of spark.sql.warehouse.dir (&apos;file:/home/double_happy/spark-warehouse&apos;).</span><br><span class="line">19/10/29 10:54:52 INFO SharedState: Warehouse path is &apos;file:/home/double_happy/spark-warehouse&apos;.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.</span><br><span class="line">19/10/29 10:54:52 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.</span><br><span class="line">19/10/29 10:54:52 INFO BlockManagerMasterEndpoint: Registering block manager hadoop101:36948 with 366.3 MB RAM, BlockManagerId(2, hadoop101, 36948, None)</span><br><span class="line">19/10/29 10:54:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.</span><br><span class="line">19/10/29 10:54:52 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/double_happy/spark-warehouse</span><br><span class="line">19/10/29 10:54:52 INFO metastore: Mestastore configuration hive.metastore.warehouse.dir changed from /user/hive/warehouse to file:/home/double_happy/spark-warehouse</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Shutting down the object store...</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Shutting down the object store...</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Metastore shutdown complete.</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=Metastore shutdown complete.</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: get_database: default</span><br><span class="line">19/10/29 10:54:52 INFO audit: ugi=double_happy  ip=unknown-ip-addr      cmd=get_database: default</span><br><span class="line">19/10/29 10:54:52 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore</span><br><span class="line">19/10/29 10:54:52 INFO ObjectStore: ObjectStore, initialize called</span><br><span class="line">19/10/29 10:54:52 INFO Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing</span><br><span class="line">19/10/29 10:54:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL</span><br><span class="line">19/10/29 10:54:52 INFO ObjectStore: Initialized ObjectStore</span><br><span class="line">19/10/29 10:54:53 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint</span><br><span class="line">Spark master: yarn, Application Id: application_1570934113711_0038</span><br><span class="line">19/10/29 10:54:53 INFO SparkSQLCLIDriver: Spark master: yarn, Application Id: application_1570934113711_0038</span><br><span class="line">spark-sql (default)&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：日志里</span><br><span class="line">19/10/29 10:54:27 WARN Client:</span><br><span class="line"> Neither spark.yarn.jars nor spark.yarn.archive is set, </span><br><span class="line"> falling back to uploading libraries under SPARK_HOME.</span><br><span class="line"></span><br><span class="line">1. spark.yarn.jars nor spark.yarn.archive is set </span><br><span class="line">这个没有设置 会把SPARK_HOME相关的东西 全部传到hdfs上去 </span><br><span class="line">不信看日志 </span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">19/10/29 10:54:33 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_libs__4819705212614105474.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_libs__4819705212614105474.zip</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/home/double_happy/software/mysql-connector-java-5.1.47.jar -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/mysql-connector-java-5.1.47.jar</span><br><span class="line">19/10/29 10:54:35 INFO Client: Uploading resource file:/tmp/spark-84f5cd2c-d3ab-4e82-8520-7f45d7422e8c/__spark_conf__8084381853282513804.zip -&gt; hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038/__spark_conf__.zip</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">打开这个地址看一眼：</span><br><span class="line"> hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0038</span><br><span class="line"></span><br><span class="line">这是我又重启了一个 spark-sql  之前的关掉了 </span><br><span class="line">[double_happy@hadoop101 ~]$ hadoop fs -ls  hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   1 double_happy supergroup     211902 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/__spark_conf__.zip</span><br><span class="line">-rw-r--r--   1 double_happy supergroup  298846294 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/__spark_libs__2528141214285970680.zip</span><br><span class="line">-rw-r--r--   1 double_happy supergroup    1007502 2019-10-29 11:18 hdfs://hadoop101:8020/user/double_happy/.sparkStaging/application_1570934113711_0041/mysql-connector-java-5.1.47.jar</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">__spark_conf__.zip</span><br><span class="line">mysql-connector-java-5.1.47.jar</span><br><span class="line">__spark_libs__2528141214285970680.zip    非常大这个 </span><br><span class="line">作业跑完会把这些自动删掉</span><br><span class="line"></span><br><span class="line">如果上面提到的两个参数没有设置 会把这些传到HDFS  上传是需要花费时间的</span><br><span class="line">这个不解决 你的每个作业 都要这样</span><br><span class="line"></span><br><span class="line">这就是一个调优点：</span><br><span class="line">尽可能的让Spark快速的再yarn上运行起来   该怎么做的呢？</span><br><span class="line"></span><br><span class="line">https://guguoyu.blog.csdn.net/article/details/102644376</span><br></pre></td></tr></table></figure></div>


<p><img src="https://img-blog.csdnimg.cn/20191029105551860.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell 和spark-sql 都可以 这不是主要的 主要的是下面的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 ~]$ spark-shell --master yarn --deploy-mode cluster</span><br><span class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells.</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:853)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:281)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:774)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)</span><br><span class="line">        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">[double_happy@hadoop101 ~]$ </span><br><span class="line"></span><br><span class="line">为什么Cluster deploy mode is not applicable to Spark shells.？</span><br><span class="line"></span><br><span class="line">因为 spark-shell driver是在本地的 是可以交互代码的  而 yarn-claster  driver是在am里的  明白吗？</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SparkSQL002" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/02/SparkSQL002/">SparkSQL002</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/02/SparkSQL002/" class="article-date">
  <time datetime="2018-02-02T12:07:44.000Z" itemprop="datePublished">2018-02-02</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="操作Hive"><a href="#操作Hive" class="headerlink" title="操作Hive"></a>操作Hive</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;double_happy&quot;)</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">        .enableHiveSupport()</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;show databases&quot;).show()</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;&quot;).write.saveAsTable(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;&quot;).write.insertInto(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Saves the content of the `DataFrame` as the specified table.</span><br><span class="line">   *</span><br><span class="line">   * In the case the table already exists, behavior of this function depends on the</span><br><span class="line">   * save mode, specified by the `mode` function (default to throwing an exception).</span><br><span class="line">   * When `mode` is `Overwrite`, the schema of the `DataFrame` does not need to be</span><br><span class="line">   * the same as that of the existing table.</span><br><span class="line">   *</span><br><span class="line">   * When `mode` is `Append`, if there is an existing table, we will use the format and options of</span><br><span class="line">   * the existing table. The column order in the schema of the `DataFrame` doesn&apos;t need to be same</span><br><span class="line">   * as that of the existing table. Unlike `insertInto`, `saveAsTable` will use the column names to</span><br><span class="line">   * find the correct column positions. For example:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *    scala&gt; Seq((1, 2)).toDF(&quot;i&quot;, &quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((3, 4)).toDF(&quot;j&quot;, &quot;i&quot;).write.mode(&quot;append&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  i|  j|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  1|  2|</span><br><span class="line">   *    |  4|  3|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * In this method, save mode is used to determine the behavior if the data source table exists in</span><br><span class="line">   * Spark catalog. We will always overwrite the underlying data of data source (e.g. a table in</span><br><span class="line">   * JDBC data source) if the table doesn&apos;t exist in Spark catalog, and will always append to the</span><br><span class="line">   * underlying data of data source if the table already exists.</span><br><span class="line">   *</span><br><span class="line">   * When the DataFrame is created from a non-partitioned `HadoopFsRelation` with a single input</span><br><span class="line">   * path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC</span><br><span class="line">   * and Parquet), the table is persisted in a Hive compatible format, which means other systems</span><br><span class="line">   * like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL</span><br><span class="line">   * specific format.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def saveAsTable(tableName: String): Unit = &#123;</span><br><span class="line">    saveAsTable(df.sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Inserts the content of the `DataFrame` to the specified table. It requires that</span><br><span class="line">   * the schema of the `DataFrame` is the same as the schema of the table.</span><br><span class="line">   *</span><br><span class="line">   * @note Unlike `saveAsTable`, `insertInto` ignores the column names and just uses position-based</span><br><span class="line">   * resolution. For example:</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *    scala&gt; Seq((1, 2)).toDF(&quot;i&quot;, &quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((3, 4)).toDF(&quot;j&quot;, &quot;i&quot;).write.insertInto(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; Seq((5, 6)).toDF(&quot;a&quot;, &quot;b&quot;).write.insertInto(&quot;t1&quot;)</span><br><span class="line">   *    scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  i|  j|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   *    |  5|  6|</span><br><span class="line">   *    |  3|  4|</span><br><span class="line">   *    |  1|  2|</span><br><span class="line">   *    +---+---+</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Because it inserts data to an existing table, format or options will be ignored.</span><br><span class="line">   *</span><br><span class="line">   * @since 1.4.0</span><br><span class="line">   */</span><br><span class="line">  def insertInto(tableName: String): Unit = &#123;</span><br><span class="line">    insertInto(df.sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建表 也可以直接 spark.sql(&quot;create table xxx&quot;) </span><br><span class="line">但是 不建议这样  因为 表 一般都是提前创建好的  </span><br><span class="line">因为 真正生产上 创建表 是在 一个 web页面 创建表的  是有权限 的</span><br></pre></td></tr></table></figure></div>

<p><a href="http://spark.apache.org/docs/latest/sql-getting-started.html#untyped-dataset-operations-aka-dataframe-operations" target="_blank" rel="noopener">Spark操作Hive 代码</a></p>
<p>大部分人使用spark开发 Hive是使用 spark.sql(“  sql  “)<br>可以的 我不喜欢 我还是喜欢使用api的方式   各有所爱 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">全局排序：这是使用sql的方式写的 </span><br><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line">    </span><br><span class="line">    //需求1 ：统计 每个平台 省市下面 traffic的总和       order by  是全局排序的 </span><br><span class="line">    val sql = &quot;select platform, province, city, sum(traffic) as traffics from log group by platform, province, city order by traffics desc&quot;</span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|platform|province|city|traffics|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|     mac|    香港|    | 2879982|</span><br><span class="line">| windows|    香港|    | 2871537|</span><br><span class="line">| Andriod|    香港|    | 2722363|</span><br><span class="line">|   linux|    香港|    | 2696578|</span><br><span class="line">| Symbain|    香港|    | 2444806|</span><br><span class="line">| Andriod|    山西|忻州|  968255|</span><br><span class="line">|   linux|    台湾|    |  898404|</span><br><span class="line">| windows|    山西|忻州|  894966|</span><br><span class="line">| Andriod|    湖北|武汉|  865758|</span><br><span class="line">|     mac|    湖北|武汉|  848995|</span><br><span class="line">|     mac|    山西|忻州|  837873|</span><br><span class="line">| Symbain|    山西|忻州|  791524|</span><br><span class="line">|   linux|    湖北|武汉|  781347|</span><br><span class="line">| Andriod|    台湾|    |  776642|</span><br><span class="line">|     mac|    台湾|    |  775977|</span><br><span class="line">| Symbain|    台湾|    |  744858|</span><br><span class="line">| windows|    台湾|    |  744558|</span><br><span class="line">| windows|    湖北|武汉|  728412|</span><br><span class="line">| Symbain|    湖北|武汉|  728034|</span><br><span class="line">|   linux|    山西|忻州|  689405|</span><br><span class="line">+--------+--------+----+--------+</span><br></pre></td></tr></table></figure></div>
<p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">Window Functions in Spark SQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">全局排序 ： Api方式   我喜欢的 </span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line">   * See [[RelationalGroupedDataset]] for all the available aggregate functions.</span><br><span class="line">   *</span><br><span class="line">   * This is a variant of groupBy that can only group by existing columns using column names</span><br><span class="line">   * (i.e. cannot construct expressions).</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Compute the average for all numeric columns grouped by department.</span><br><span class="line">   *   ds.groupBy(&quot;department&quot;).avg()</span><br><span class="line">   *</span><br><span class="line">   *   // Compute the max age and average salary, grouped by department and gender.</span><br><span class="line">   *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</span><br><span class="line">   *     &quot;salary&quot; -&gt; &quot;avg&quot;,</span><br><span class="line">   *     &quot;age&quot; -&gt; &quot;max&quot;</span><br><span class="line">   *   ))</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   * @group untypedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123;</span><br><span class="line">    val colNames: Seq[String] = col1 +: cols</span><br><span class="line">    RelationalGroupedDataset(</span><br><span class="line">      toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Compute aggregates by specifying a series of aggregate columns. Note that this function by</span><br><span class="line">   * default retains the grouping columns in its output. To not retain grouping columns, set</span><br><span class="line">   * `spark.sql.retainGroupColumns` to false.</span><br><span class="line">   *</span><br><span class="line">   * The available aggregate methods are defined in [[org.apache.spark.sql.functions]].</span><br><span class="line">   *</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Selects the age of the oldest employee and the aggregate expense for each department</span><br><span class="line">   *</span><br><span class="line">   *   // Scala:</span><br><span class="line">   *   import org.apache.spark.sql.functions._</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   // Java:</span><br><span class="line">   *   import static org.apache.spark.sql.functions.*;</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;));</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * Note that before Spark 1.4, the default behavior is to NOT retain grouping columns. To change</span><br><span class="line">   * to that behavior, set config variable `spark.sql.retainGroupColumns` to `false`.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   // Scala, 1.3.x:</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg($&quot;department&quot;, max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line">   *</span><br><span class="line">   *   // Java, 1.3.x:</span><br><span class="line">   *   df.groupBy(&quot;department&quot;).agg(col(&quot;department&quot;), max(&quot;age&quot;), sum(&quot;expense&quot;));</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @since 1.3.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def agg(expr: Column, exprs: Column*): DataFrame = &#123;</span><br><span class="line">    toDF((expr +: exprs).map &#123;</span><br><span class="line">      case typed: TypedColumn[_, _] =&gt;</span><br><span class="line">        typed.withInputType(df.exprEnc, df.logicalPlan.output).expr</span><br><span class="line">      case c =&gt; c.expr</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">你要好好看看注释 就会明白下面的代码</span><br><span class="line"></span><br><span class="line">groupBy:Groups the Dataset using the specified columns, so that we can run aggregation on them.</span><br><span class="line">agg : </span><br><span class="line">  * Compute aggregates by specifying a series of aggregate columns. Note that this function by</span><br><span class="line">   * default retains the grouping columns in its output. To not retain grouping columns, set</span><br><span class="line">   * `spark.sql.retainGroupColumns` to false.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Returns a new Dataset sorted by the given expressions. For example:</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   ds.sort($&quot;col1&quot;, $&quot;col2&quot;.desc)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  @scala.annotation.varargs</span><br><span class="line">  def sort(sortExprs: Column*): Dataset[T] = &#123;</span><br><span class="line">    sortInternal(global = true, sortExprs)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line">    //需求1 ：统计 每个平台 省市下面 traffic的总和       order by  是全局排序的</span><br><span class="line">        import org.apache.spark.sql.functions._    //spark 内置的函数</span><br><span class="line">        df.groupBy(&quot;platform&quot;, &quot;province&quot;, &quot;city&quot;)</span><br><span class="line">            .agg(sum(&quot;traffic&quot;).as(&quot;traffics&quot;))</span><br><span class="line">            .sort(&apos;traffics.desc)</span><br><span class="line">            .show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Hive的函数 Spark里也是有的 spark自己内置的 </span><br><span class="line">import org.apache.spark.sql.functions._   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果是;</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|platform|province|city|traffics|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">|     mac|    香港|    | 2879982|</span><br><span class="line">| windows|    香港|    | 2871537|</span><br><span class="line">| Andriod|    香港|    | 2722363|</span><br><span class="line">|   linux|    香港|    | 2696578|</span><br><span class="line">| Symbain|    香港|    | 2444806|</span><br><span class="line">| Andriod|    山西|忻州|  968255|</span><br><span class="line">|   linux|    台湾|    |  898404|</span><br><span class="line">| windows|    山西|忻州|  894966|</span><br><span class="line">| Andriod|    湖北|武汉|  865758|</span><br><span class="line">|     mac|    湖北|武汉|  848995|</span><br><span class="line">|     mac|    山西|忻州|  837873|</span><br><span class="line">| Symbain|    山西|忻州|  791524|</span><br><span class="line">|   linux|    湖北|武汉|  781347|</span><br><span class="line">| Andriod|    台湾|    |  776642|</span><br><span class="line">|     mac|    台湾|    |  775977|</span><br><span class="line">| Symbain|    台湾|    |  744858|</span><br><span class="line">| windows|    台湾|    |  744558|</span><br><span class="line">| windows|    湖北|武汉|  728412|</span><br><span class="line">| Symbain|    湖北|武汉|  728034|</span><br><span class="line">|   linux|    山西|忻州|  689405|</span><br><span class="line">+--------+--------+----+--------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102813201459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">api方式 开发 你要注意的是：</span><br><span class="line">	Column 和 String  就是你传进去的 列名 要传入 string类型 还是 Column 类型 </span><br><span class="line"></span><br><span class="line">   我个人喜欢 ：</span><br><span class="line">   		 Column  ==》 &apos;列名</span><br><span class="line">   		 String    ==》 “列名”</span><br><span class="line">   毕竟有太多种写法 找一个自己喜欢的  跟找女朋友相反 找女朋友 找一个喜欢自己的  明白吗</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">分组:Top n </span><br><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line"></span><br><span class="line">    // 需求二 ：platform  组内  province 访问次数最多的TopN</span><br><span class="line">    val sql =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |</span><br><span class="line">        |select * from</span><br><span class="line">        |(</span><br><span class="line">        |select t.*, row_number() over(partition by platform order by cnt desc) as r</span><br><span class="line">        |from</span><br><span class="line">        |(select platform,province,count(1) cnt from log group by platform,province) t</span><br><span class="line">        |) a where a.r&lt;=3</span><br><span class="line">        |</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    spark.sql(sql).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+---+---+</span><br><span class="line">|platform|province|cnt|  r|</span><br><span class="line">+--------+--------+---+---+</span><br><span class="line">|   linux|    香港|606|  1|</span><br><span class="line">|   linux|    广东|211|  2|</span><br><span class="line">|   linux|    台湾|173|  3|</span><br><span class="line">| Symbain|    香港|582|  1|</span><br><span class="line">| Symbain|    广东|222|  2|</span><br><span class="line">| Symbain|    福建|153|  3|</span><br><span class="line">| Andriod|    香港|607|  1|</span><br><span class="line">| Andriod|    广东|223|  2|</span><br><span class="line">| Andriod|    北京|150|  3|</span><br><span class="line">|     mac|    香港|646|  1|</span><br><span class="line">|     mac|    广东|213|  2|</span><br><span class="line">|     mac|    台湾|156|  3|</span><br><span class="line">| windows|    香港|657|  1|</span><br><span class="line">| windows|    广东|186|  2|</span><br><span class="line">| windows|    河北|151|  3|</span><br><span class="line">+--------+--------+---+---+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">object LogApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;LogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;\t&quot;)</span><br><span class="line">        val platform = splits(1)</span><br><span class="line">        val traffic = splits(6).toLong</span><br><span class="line">        val province = splits(8)</span><br><span class="line">        val city = splits(9)</span><br><span class="line">        val isp = splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;).toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)   //toDF的字段名</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图</span><br><span class="line">    df.createOrReplaceTempView(&quot;log&quot;)</span><br><span class="line">    val sql2 =</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        |</span><br><span class="line">        |select a.* from</span><br><span class="line">        |(</span><br><span class="line">        |select t.*,</span><br><span class="line">        |row_number() over(partition by platform order by cnt desc) as rn,</span><br><span class="line">        |rank() over(partition by platform order by cnt desc) as r,</span><br><span class="line">        |dense_rank() over(partition by platform order by cnt desc) as dn</span><br><span class="line">        |from</span><br><span class="line">        |(select platform,province,count(1) cnt from log group by platform,province) t</span><br><span class="line">        |) a</span><br><span class="line">        |</span><br><span class="line">      &quot;&quot;&quot;.stripMargin</span><br><span class="line"></span><br><span class="line">    spark.sql(sql2).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">|platform|province|cnt| rn|  r| dn|</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">|   linux|    香港|606|  1|  1|  1|</span><br><span class="line">|   linux|    广东|211|  2|  2|  2|</span><br><span class="line">|   linux|    台湾|173|  3|  3|  3|</span><br><span class="line">|   linux|    福建|147|  4|  4|  4|</span><br><span class="line">|   linux|    北京|134|  5|  5|  5|</span><br><span class="line">|   linux|    河北|128|  6|  6|  6|</span><br><span class="line">|   linux|    湖北|115|  7|  7|  7|</span><br><span class="line">|   linux|    山西|107|  8|  8|  8|</span><br><span class="line">|   linux|    江西|104|  9|  9|  9|</span><br><span class="line">|   linux|    上海|101| 10| 10| 10|</span><br><span class="line">|   linux|    山东| 26| 11| 11| 11|</span><br><span class="line">| Symbain|    香港|582|  1|  1|  1|</span><br><span class="line">| Symbain|    广东|222|  2|  2|  2|</span><br><span class="line">| Symbain|    福建|153|  3|  3|  3|</span><br><span class="line">| Symbain|    河北|151|  4|  4|  4|</span><br><span class="line">| Symbain|    台湾|146|  5|  5|  5|</span><br><span class="line">| Symbain|    北京|133|  6|  6|  6|</span><br><span class="line">| Symbain|    山西|121|  7|  7|  7|</span><br><span class="line">| Symbain|    湖北|120|  8|  8|  8|</span><br><span class="line">| Symbain|    上海|109|  9|  9|  9|</span><br><span class="line">+--------+--------+---+---+---+---+</span><br><span class="line">only showing top 20 rows</span><br><span class="line"></span><br><span class="line">那么： 他们有什么区别？</span><br><span class="line">	row_number   123456  排序的  即使有的值相等 也往下排序</span><br><span class="line">	rank        1233567 排序的  有相同的值  排序号相等 之后会跳过重复的占位 这里就没有4</span><br><span class="line">	dense_rank   12334567 排序的 有相同的值  排序号相等  之后不会跳过重复的占位 这里紧接着4</span><br></pre></td></tr></table></figure></div>

<p><strong>Catalog</strong><br>非常非常重要 spark2.0之后才有的 我开发了一个csv入Hive 就用到了它</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你Hive的元数据存在 MySQl里面的 </span><br><span class="line">如果要代码中使用到元数据 要通过JDBC来取 </span><br><span class="line"></span><br><span class="line">但是2.0版本之后 Spark 提供 Catalog 可以拿到 Hive的元数据</span><br></pre></td></tr></table></figure></div>
<p>开启spark-shell –jars  MySQL驱动</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val catalog = spark.catalog</span><br><span class="line">catalog: org.apache.spark.sql.catalog.Catalog = org.apache.spark.sql.internal.CatalogImpl@672c4e24</span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listDatabases().show</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line">|    name|         description|         locationUri|</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line">| default|Default Hive data...|hdfs://hadoop101:...|</span><br><span class="line">|homework|                    |hdfs://hadoop101:...|</span><br><span class="line">+--------+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listDatabases().show(false)</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line">|name    |description          |locationUri                                          |</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line">|default |Default Hive database|hdfs://hadoop101:8020/user/hive/warehouse            |</span><br><span class="line">|homework|                     |hdfs://hadoop101:8020/user/hive/warehouse/homework.db|</span><br><span class="line">+--------+---------------------+-----------------------------------------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listTables(&quot;homework&quot;).show(false)</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line">|name                             |database|description|tableType|isTemporary|</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line">|access_wide                      |homework|null       |EXTERNAL |false      |</span><br><span class="line">|dwd_platform_stat_info           |homework|null       |MANAGED  |false      |</span><br><span class="line">|jf_tmp                           |homework|null       |MANAGED  |false      |</span><br><span class="line">|ods_domain_traffic_info          |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_log_info                     |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_uid_pid_compression_info     |homework|null       |MANAGED  |false      |</span><br><span class="line">|ods_uid_pid_info                 |homework|null       |EXTERNAL |false      |</span><br><span class="line">|ods_uid_pid_info_compression_test|homework|null       |EXTERNAL |false      |</span><br><span class="line">+---------------------------------+--------+-----------+---------+-----------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listFunctions().show(5,false)</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">|name|database|description|className                                           |isTemporary|</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">|!   |null    |null       |org.apache.spark.sql.catalyst.expressions.Not       |true       |</span><br><span class="line">|%   |null    |null       |org.apache.spark.sql.catalyst.expressions.Remainder |true       |</span><br><span class="line">|&amp;   |null    |null       |org.apache.spark.sql.catalyst.expressions.BitwiseAnd|true       |</span><br><span class="line">|*   |null    |null       |org.apache.spark.sql.catalyst.expressions.Multiply  |true       |</span><br><span class="line">|+   |null    |null       |org.apache.spark.sql.catalyst.expressions.Add       |true       |</span><br><span class="line">+----+--------+-----------+----------------------------------------------------+-----------+</span><br><span class="line">only showing top 5 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; catalog.listColumns(&quot;homework.dwd_platform_stat_info&quot;).show(false)</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line">|name    |description|dataType|nullable|isPartition|isBucket|</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line">|platform|null       |string  |true    |false      |false   |</span><br><span class="line">|cnt     |null       |int     |true    |false      |false   |</span><br><span class="line">|d       |null       |string  |true    |false      |false   |</span><br><span class="line">|day     |null       |string  |true    |true       |false   |</span><br><span class="line">+--------+-----------+--------+--------+-----------+--------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">不仅仅这多多哈 catalog 几乎所有的元数据 信息都能搞到 </span><br><span class="line"></span><br><span class="line">但是这些值的返回值 都是 DataSet 接下来 讲讲DataSet</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028140024184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>给你一个使用上面catalog的场景<br>做一个页面：</p>
<p><img src="https://img-blog.csdnimg.cn/20191028140640930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong>DataSet</strong><br>这个东西很简单的<br>Untyped Dataset = Row </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">DataSet就是你可以把它当作rdd来操作 </span><br><span class="line"></span><br><span class="line">object DSApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;DSApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;inferSchema&quot;,&quot;true&quot;).csv(&quot;file:///C:/IdeaProjects/spark/data/sale.csv&quot;)</span><br><span class="line">    val ds = df.as[Sales]</span><br><span class="line"></span><br><span class="line">    ds.printSchema()</span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    // ROW  DF弱类型</span><br><span class="line">    //    df.select(&quot;transactionId&quot;).show(false)</span><br><span class="line"></span><br><span class="line">    ds.map(columns =&gt;&#123;</span><br><span class="line">        columns.transactionId match &#123;</span><br><span class="line">        case 111 =&gt;Sales(columns.transactionId,columns.customerId,columns.itemId,columns.amountPaid+200)</span><br><span class="line">        case _ =&gt; Sales(columns.transactionId,columns.customerId,columns.itemId,columns.amountPaid)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Sales(transactionId:Int,customerId:Int,itemId:Int,amountPaid:Double)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- transactionId: integer (nullable = true)</span><br><span class="line"> |-- customerId: integer (nullable = true)</span><br><span class="line"> |-- itemId: integer (nullable = true)</span><br><span class="line"> |-- amountPaid: double (nullable = true)</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|transactionId|customerId|itemId|amountPaid|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|          111|         1|     1|     100.0|</span><br><span class="line">|          112|         2|     2|     500.0|</span><br><span class="line">|          113|         3|     3|     400.0|</span><br><span class="line">|          114|         1|     4|     300.0|</span><br><span class="line">|          115|         1|     1|     200.0|</span><br><span class="line">|          116|         1|     2|     700.0|</span><br><span class="line">|          117|         4|     3|     800.0|</span><br><span class="line">|          118|         5|     1|     200.0|</span><br><span class="line">|          119|         3|     4|     200.0|</span><br><span class="line">|          120|         1|     1|     300.0|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line"></span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|transactionId|customerId|itemId|amountPaid|</span><br><span class="line">+-------------+----------+------+----------+</span><br><span class="line">|111          |1         |1     |300.0     |</span><br><span class="line">|112          |2         |2     |500.0     |</span><br><span class="line">|113          |3         |3     |400.0     |</span><br><span class="line">|114          |1         |4     |300.0     |</span><br><span class="line">|115          |1         |1     |200.0     |</span><br><span class="line">|116          |1         |2     |700.0     |</span><br><span class="line">|117          |4         |3     |800.0     |</span><br><span class="line">|118          |5         |1     |200.0     |</span><br><span class="line">|119          |3         |4     |200.0     |</span><br><span class="line">|120          |1         |1     |300.0     |</span><br><span class="line">+-------------+----------+------+----------+</span><br></pre></td></tr></table></figure></div>


<p><strong>Interoperating with RDDs</strong><br><a href="http://spark.apache.org/docs/latest/sql-getting-started.html#interoperating-with-rdds" target="_blank" rel="noopener">Interoperating with RDDs</a><br>和RDD的交互操作</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DS  --》 DF   通过 DS.toDF(&quot;列名。。&quot;)</span><br><span class="line">DF--》DS   通过 样例类    df.as[样例类]</span><br><span class="line"></span><br><span class="line">RDD ---》 DF   两种 </span><br><span class="line">  Spark SQL supports two different methods for converting existing RDDs into Datasets.</span><br><span class="line">	1.反射   就是使用case  class  你的case class 定义的就是 table的信息</span><br><span class="line">	2.The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD.</span><br><span class="line">	编程的方式 ：</span><br><span class="line">		就是你的哪个字段什么类型指定好就可以了</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1.反射的方式   RDD -》 DF</span><br><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // RDD ==&gt; DF/DS</span><br><span class="line">        val peopleDF = spark.sparkContext</span><br><span class="line">          .textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">          .map(_.split(&quot;,&quot;))</span><br><span class="line">          .map(x =&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">          .toDF()</span><br><span class="line"></span><br><span class="line">        peopleDF.show(false)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">+------------+---+</span><br><span class="line">|name        |age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy|25 |</span><br><span class="line">|Kairis      |25 |</span><br><span class="line">|Kite        |32 |</span><br><span class="line">+------------+---+</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">2.编程的方式</span><br><span class="line">When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.</span><br><span class="line">  1.Create an RDD of Rows from the original RDD;</span><br><span class="line">  2.Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step </span><br><span class="line">  3.Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.</span><br><span class="line"></span><br><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">    val fields: Array[StructField] = schemaString.split(&quot; &quot;).map(fieldName =&gt; &#123;</span><br><span class="line">      StructField(fieldName, StringType)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    val rowRDD: RDD[Row] = peopleRDD.map(_.split(&quot;,&quot;)).map(x=&gt;Row(x(0),x(1).trim))</span><br><span class="line"></span><br><span class="line">    val peopleDF: DataFrame = spark.createDataFrame(rowRDD,schema)</span><br><span class="line"></span><br><span class="line">    //TODO... 业务逻辑</span><br><span class="line">    peopleDF.show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+------------+---+</span><br><span class="line">|        name|age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy| 25|</span><br><span class="line">|      Kairis| 25|</span><br><span class="line">|        Kite| 32|</span><br><span class="line">+------------+---+</span><br><span class="line"></span><br><span class="line">官网给的例子 不是很好  难道你们生产上全是 String 类型的么？ 我只能说还真是 我上一家公司就是 </span><br><span class="line">建议哈 统计字段 还是采用标准的int 或者 double类型  </span><br><span class="line">我之前统计的时候 全是String 的 就会出现 指标不准的问题  </span><br><span class="line">我遇到过 同事说用String 很爽 我只能说 是的是的 </span><br><span class="line">在他们眼里 spark不就是写sql吗？ </span><br><span class="line">emmm 统计指标可以的 但是 如果让你做 基础架构开发 呢？ </span><br><span class="line">不要仅仅局限于指标需求哈 那么你这大数据工程师 就是 sql怪</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">object RDDApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;CatalogApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"> </span><br><span class="line">    val peopleRDD = spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val schema = StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;,StringType),</span><br><span class="line">      StructField(&quot;age&quot;,IntegerType)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1).trim.toInt))</span><br><span class="line">      </span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.show()</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"> case class Person(name:String,age:Int)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	Row里面的数据类型 一定要和 schema里的数据类型匹配上</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+------------+---+</span><br><span class="line">|        name|age|</span><br><span class="line">+------------+---+</span><br><span class="line">|double_happy| 25|</span><br><span class="line">|      Kairis| 25|</span><br><span class="line">|        Kite| 32|</span><br><span class="line">+------------+---+</span><br></pre></td></tr></table></figure></div>
<p><strong>UDF</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object UDFApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    /**</span><br><span class="line">      * step1： 定义 注册</span><br><span class="line">      * step2： 使用</span><br><span class="line">      */</span><br><span class="line">    spark.sparkContext.textFile(&quot;file:///C:/IdeaProjects/spark/data/udf.txt&quot;)</span><br><span class="line">      .map(_.split(&quot; &quot;))</span><br><span class="line">      .map(x =&gt; FootballTeam(x(0), x(1)))</span><br><span class="line">      .toDF().createOrReplaceTempView(&quot;teams&quot;)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(&quot;teams_length&quot;,(input:String)=&gt;&#123;</span><br><span class="line">      input.split(&quot;，&quot;).length</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    //统计一个人喜欢的球队的个数</span><br><span class="line"></span><br><span class="line">    spark.sql(&quot;select name,teams,teams_length(teams) from teams&quot;).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  case class FootballTeam(name:String, teams:String)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line">|  name|             teams|UDF:teams_length(teams)|</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line">|苍老师 |      喵喵喵，红魔  |                      2|</span><br><span class="line">|    pk|小破车，国足，宅团   |                      3|</span><br><span class="line">+------+------------------+-----------------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	  spark.udf.register(&quot;teams_length&quot;,(input:String)=&gt;&#123;</span><br><span class="line">      input.split(&quot;，&quot;).length</span><br><span class="line">    &#125;)</span><br><span class="line">def register[RT: TypeTag, A1: TypeTag](name: String, func: Function1[A1, RT]): UserDefinedFunction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Function 就是传进去一个 函数 </span><br><span class="line"></span><br><span class="line">还有一种UDF函数的使用就是 api的方式 </span><br><span class="line"></span><br><span class="line">functions里面 有个 udf 方法 传进去一个函数    再 结合 withColumns 方法 使用 也是一样的</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-SparkSQL01" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/01/SparkSQL01/">SparkSQL01</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/02/01/SparkSQL01/" class="article-date">
  <time datetime="2018-02-01T12:06:54.000Z" itemprop="datePublished">2018-02-01</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p>工作当中几乎全用SparkSQL ，RDD用的很少(面试多)<br><strong>SparkSQL误区</strong></p>
<p>Spark SQL is Apache Spark’s module for working with structured data.<br>不要把SparkSQL认为就是处理SQl的 或者认为就是写SQL<br><a href="http://spark.apache.org/sql/" target="_blank" rel="noopener">SparkSQL</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">误区：</span><br><span class="line">    1）Spark SQL是处理结构化数据</span><br><span class="line">        并不是仅仅能够处理SQL</span><br><span class="line">        SQL仅仅是Spark SQL这个模块的一小部分应用</span><br><span class="line">        API/ExtDS</span><br><span class="line">    2）Uniform Data Access  外部数据源(*****)</span><br><span class="line">        Spark SQL是能够处理多种不同的数据源的数据</span><br><span class="line">            text、json、parquet、orc、hive、jdbc    数据的格式 </span><br><span class="line">            HDFS/S3(a/n)/OSS/COS                数据的存储系统</span><br><span class="line">        不同的数据格式压缩的不压缩的 sparksql都是兼容的 </span><br><span class="line">        你访问不同的数据源SparkSQl都是用统一的访问方式  这就是外部数据源</span><br><span class="line"></span><br><span class="line">SparkSQL能面试的东西 就是两个 ：</span><br><span class="line">	DataFrame 、 外部数据源、catelist </span><br><span class="line"></span><br><span class="line">2.能集成Hive</span><br><span class="line">你的数仓以前是基于Hive来做的 都是Hive的脚本 </span><br><span class="line"> 现在 如果想使用SparkSQL访问Hive的数据 SparkSQL能连接到MetaStore才可以</span><br><span class="line"> (把Hive-site.xml  拷贝到Sparkconf目录下就可以了)</span><br><span class="line"> 因为MetaStore 是 on Hadoop的核心所在 </span><br><span class="line"></span><br><span class="line">所以你要把Hive迁移到Spark上来 成本是很低的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027131424394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3.Standard Connectivity</span><br><span class="line">Hive能通过HiveServer2提供一个服务 大家去查，那么 spark里面有个thriftServer </span><br><span class="line">他们底层都是用thrift协议的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027131724269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">误区3：</span><br><span class="line">MR==&gt;Hive==&gt;  Hive底层当时是MR 慢 所以出来Spark </span><br><span class="line">     Spark==&gt; AMPLab Shark(为了将Hive SQL跑在Spark上)  1.x  配套一个打了补丁的Hive</span><br><span class="line">        Spark1.0  Shark不维护</span><br><span class="line">            ==&gt; Spark SQL 是在Spark里面的</span><br><span class="line">            ==&gt; Hive on Spark 是在Hive里面的      是Hive的引擎是Spark</span><br><span class="line"></span><br><span class="line">误区3）</span><br><span class="line">    Hive on Spark不是Spark SQL</span><br><span class="line">        Hive刚开始时底层执行引擎只有一个：MR</span><br><span class="line">        后期：Tez Spark</span><br><span class="line">        set hive.execution.engine=spark;    就可以 Hive on Spark</span><br><span class="line"></span><br><span class="line">    SparkSQL on Hive  X</span><br></pre></td></tr></table></figure></div>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark:+Getting+Started" target="_blank" rel="noopener">Hive On Spark</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Time taken: 6.86 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt; set hive.execution.engine;</span><br><span class="line">hive.execution.engine=mr</span><br><span class="line">hive (default)&gt; set hive.execution.engine=spark;</span><br><span class="line">hive (default)&gt; set hive.execution.engine;</span><br><span class="line">hive.execution.engine=spark</span><br><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">default</span><br><span class="line">homework</span><br><span class="line">Time taken: 0.008 seconds, Fetched: 2 row(s)</span><br><span class="line">hive (default)&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个东西了解即可 Hive On Spark 真正生产上用的很少的 </span><br><span class="line">这个东西不是很成熟的</span><br></pre></td></tr></table></figure></div>

<h2 id="Datasets-and-DataFrames"><a href="#Datasets-and-DataFrames" class="headerlink" title="Datasets and DataFrames"></a>Datasets and DataFrames</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">出来的时间：</span><br><span class="line"></span><br><span class="line">Spark SQL</span><br><span class="line">    1.0     </span><br><span class="line">    SchemaRDD  ==&gt; Table     RDD(存数据) + schema = Table</span><br><span class="line">    ==&gt; DataFrame  1.2/3     由SchemaRDD  变为DataFrame 原因是 更加 OO</span><br><span class="line">    ==&gt; Dataset    1.6    由DataFrame  变为Dataset 因为 compile-time type safety</span><br></pre></td></tr></table></figure></div>
<p><strong>DataFrame</strong><br>A Dataset is a distributed collection of data.<br>A DataFrame is a Dataset organized into named columns.<br>DataFrame = Dataset[Row]<br>In Scala and Java, a DataFrame is represented by a Dataset of Rows. </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DataFrame ：</span><br><span class="line">	1.named columns    就是一个表  包含 列的名字 + 列的类型 </span><br><span class="line">	</span><br><span class="line">Row ： 可以理解为  一行数据 没有scheme的 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SparkSession是Spark编程的入口点</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027173652956.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Api：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SparkSession：</span><br><span class="line">  /**</span><br><span class="line">   * Executes a SQL query using Spark, returning the result as a `DataFrame`.</span><br><span class="line">   * The dialect that is used for SQL parsing can be configured with &apos;spark.sql.dialect&apos;.</span><br><span class="line">   *</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def sql(sqlText: String): DataFrame = &#123;</span><br><span class="line">    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">	1. returning the result as a `DataFrame`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Dataset：</span><br><span class="line">  /**</span><br><span class="line">   * Displays the top 20 rows of Dataset in a tabular form.</span><br><span class="line">   *</span><br><span class="line">   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will</span><br><span class="line">   *                 be truncated and all cells will be aligned right</span><br><span class="line">   *</span><br><span class="line">   * @group action</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def show(truncate: Boolean): Unit = show(20, truncate)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;show tables&quot;).show</span><br><span class="line"></span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| default|  student|      false|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	启动spark-shell的时候  指定MySQL驱动  </span><br><span class="line">	个人建议使用 --jars 指定MySQL驱动 </span><br><span class="line">	不建议把MySQL驱动 直接丢在Spark jar路径里</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">查看Hive里元数据：</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from DBS;</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">| DB_ID | DESC                  | DB_LOCATION_URI                                       | NAME     | OWNER_NAME   | OWNER_TYPE |</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">|     1 | Default Hive database | hdfs://hadoop101:8020/user/hive/warehouse             | default  | public       | ROLE       |</span><br><span class="line">|     6 | NULL                  | hdfs://hadoop101:8020/user/hive/warehouse/homework.db | homework | double_happy | USER       |</span><br><span class="line">+-------+-----------------------+-------------------------------------------------------+----------+--------------+------------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from TBLS;</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER        | RETENTION | SD_ID | TBL_NAME                          | TBL_TYPE       | VIEW_EXPANDED_TEXT | VIEW_ORIGINAL_TEXT |</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">|      1 |  1568615059 |     1 |                0 | double_happy |         0 |     1 | student                           | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|      8 |  1568616039 |     6 |                0 | double_happy |         0 |     8 | ods_domain_traffic_info           | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|      9 |  1568620410 |     6 |                0 | double_happy |         0 |     9 | ods_uid_pid_info                  | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     17 |  1568860945 |     6 |                0 | double_happy |         0 |    17 | jf_tmp                            | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     21 |  1569056727 |     6 |                0 | double_happy |         0 |    21 | access_wide                       | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     26 |  1569209493 |     6 |                0 | double_happy |         0 |    31 | ods_uid_pid_info_compression_test | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">|     27 |  1569209946 |     6 |                0 | double_happy |         0 |    32 | ods_uid_pid_compression_info      | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     31 |  1569224142 |     6 |                0 | double_happy |         0 |    36 | dwd_platform_stat_info            | MANAGED_TABLE  | NULL               | NULL               |</span><br><span class="line">|     53 |  1570957119 |     6 |                0 | double_happy |         0 |    63 | ods_log_info                      | EXTERNAL_TABLE | NULL               | NULL               |</span><br><span class="line">+--------+-------------+-------+------------------+--------------+-----------+-------+-----------------------------------+----------------+--------------------+--------------------+</span><br><span class="line">9 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell查询Hive里的表：</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(&quot;select * from homework.dwd_platform_stat_info&quot;).show</span><br><span class="line">+--------+---+--------+--------+                                                </span><br><span class="line">|platform|cnt|       d|     day|</span><br><span class="line">+--------+---+--------+--------+</span><br><span class="line">| Andriod|658|20190921|20190921|</span><br><span class="line">| Symbain|683|20190921|20190921|</span><br><span class="line">|   linux|639|20190921|20190921|</span><br><span class="line">|     mac|652|20190921|20190921|</span><br><span class="line">| windows|640|20190921|20190921|</span><br><span class="line">+--------+---+--------+--------+</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">使用sparksql 在spark-shell交互 还得写 spark.sql</span><br><span class="line">在spark里 有个 spark-sql  用法和 spark-shell 是一样的</span><br></pre></td></tr></table></figure></div>

<h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a><strong>编程</strong></h2><p><a href="http://spark.apache.org/docs/latest/sql-getting-started.html" target="_blank" rel="noopener">sparksql编程</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1.SparkSession构建</span><br><span class="line"></span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">当然 你spark一些参数如何传进去呢？</span><br><span class="line">提供config传进去</span><br><span class="line">eg ： 你要设置多少个分区呀 等</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191027180607177.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>Data Sources</strong></p>
<p><strong>1.读文本数据</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">1.读文本数据</span><br><span class="line"></span><br><span class="line">object SparkSessionApp &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+---------------+</span><br><span class="line">|          value|</span><br><span class="line">+---------------+</span><br><span class="line">|double_happy,25|</span><br><span class="line">|      Kairis,25|</span><br><span class="line">|        Kite,32|</span><br><span class="line">+---------------+</span><br><span class="line"></span><br><span class="line">1. 但是有一个问题 读取进来的数据   把所有内容</span><br><span class="line">都放到 value这个列 里面去了 </span><br><span class="line">该怎么办？</span><br><span class="line"></span><br><span class="line">2. 上面那种写法读进来的是DF</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def text(spark: SparkSession) = &#123;</span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line">        ds.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">读进来的是DS</span><br><span class="line">结果是一样的：</span><br><span class="line">+---------------+</span><br><span class="line">|          value|</span><br><span class="line">+---------------+</span><br><span class="line">|double_happy,25|</span><br><span class="line">|      Kairis,25|</span><br><span class="line">|        Kite,32|</span><br><span class="line">+---------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> /**</span><br><span class="line">   * Loads text files and returns a [[Dataset]] of String. See the documentation on the</span><br><span class="line">   * other overloaded `textFile()` method for more details.</span><br><span class="line">   * @since 2.0.0</span><br><span class="line">   */</span><br><span class="line">  def textFile(path: String): Dataset[String] = &#123;</span><br><span class="line">    // This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">    textFile(Seq(path): _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">可以传入多个路径的    textFile(Seq(path): _*)</span><br></pre></td></tr></table></figure></div>

<p><strong>取出第一列输出出去  注意df 和ds的区别</strong> </p>
<p>df：<br><img src="https://img-blog.csdnimg.cn/20191028083903382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028084355225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191028084118930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. df.map  里面是row     ds.map  里面是String</span><br><span class="line"> 2. ds 可以map 里面 x.split </span><br><span class="line">   df 就不可以 </span><br><span class="line"> 那我要取出第一列使用df 该这么办？</span><br><span class="line">这就是 df 和 ds 编程的 最本质的区别   df = ds[Row]</span><br><span class="line"></span><br><span class="line">所以 df 得使用  df.rdd.map  </span><br><span class="line">而且他的返回值是 rdd</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028084634122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    val result: RDD[(String, String)] = df.rdd.map(x =&gt; &#123;</span><br><span class="line">      val tmp: String = x.getString(0)</span><br><span class="line">      val splits: Array[String] = tmp.split(&quot;,&quot;)</span><br><span class="line">      (splits(0), splits(1))</span><br><span class="line">    &#125;)</span><br><span class="line">    result.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(double_happy,25)</span><br><span class="line">(Kairis,25)</span><br><span class="line">(Kite,32)</span><br><span class="line"></span><br><span class="line">这个结果不是我们想要的 ，我要的是 把结果写出去 </span><br><span class="line">上面这种是 df的 那么 ds该怎么操作呢？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS: Dataset[(String, String)] = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      (splits(0), splits(1))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028090741224.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">那么我们只输出一列 ：</span><br><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      splits(0)</span><br><span class="line">    &#125;)</span><br><span class="line">    resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">结果 ：ok</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028091025124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">但是 有个问题的 文本格式是非常常用的格式  你只支持  一列输出 有个鬼用</span><br><span class="line"></span><br><span class="line">这个问题该这么解决呢？</span><br><span class="line">  这个问题很重要 前面的 不同类型日志输出  一定是多列的  </span><br><span class="line">  下面讲到压缩  给你一个场景 </span><br><span class="line">  andriod 的 bzip的  ios gzip 的   windos bz2  你该这么办？  这都是常见的需求</span><br></pre></td></tr></table></figure></div>
<p>上面的问题之后再解决 </p>
<p>那么 这个输出的数据也是可以用压缩的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def text(spark: SparkSession) = &#123;</span><br><span class="line">    val ds: Dataset[String] = spark.read.textFile(&quot;file:///C:/IdeaProjects/spark/data/data.txt&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDS = ds.map(x =&gt; &#123;</span><br><span class="line">      val splits: Array[String] = x.split(&quot;,&quot;)</span><br><span class="line">      (splits(0))</span><br><span class="line">    &#125;)</span><br><span class="line">   resultDS.write.option(&quot;compression&quot;,&quot;gzip&quot;).mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;file:///C:/IdeaProjects/spark/out-sparksql&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    text(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102809233182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>注意：<br><img src="https://img-blog.csdnimg.cn/20191028092452531.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">也就是说这个压缩 codec 是有限制的 </span><br><span class="line">问题：让是输出使用lzo 压缩该怎么办呢？</span><br></pre></td></tr></table></figure></div>
<p><strong>2.读json数据</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20191028102115717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    json(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- _corrupt_record: string (nullable = true)</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- branch: string (nullable = true)</span><br><span class="line"> |-- camera_id: string (nullable = true)</span><br><span class="line"> |-- camera_ip: string (nullable = true)</span><br><span class="line"> |-- client_time: struct (nullable = true)</span><br><span class="line"> |    |-- enter_time: long (nullable = true)</span><br><span class="line"> |    |-- exit_time: long (nullable = true)</span><br><span class="line"> |    |-- first_time: long (nullable = true)</span><br><span class="line"> |    |-- last_time: long (nullable = true)</span><br><span class="line"> |-- events: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- host_time: long (nullable = true)</span><br><span class="line"> |    |    |-- name: string (nullable = true)</span><br><span class="line"> |    |    |-- osd_time: long (nullable = true)</span><br><span class="line"> |-- face_id: string (nullable = true)</span><br><span class="line"> |-- gender: long (nullable = true)</span><br><span class="line"> |-- is_new_user: boolean (nullable = true)</span><br><span class="line"> |-- mall_id: string (nullable = true)</span><br><span class="line"> |-- match_photo_index: long (nullable = true)</span><br><span class="line"> |-- match_score: long (nullable = true)</span><br><span class="line"> |-- package_index: long (nullable = true)</span><br><span class="line"> |-- photos: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- frame_time: long (nullable = true)</span><br><span class="line"> |    |    |-- quality: double (nullable = true)</span><br><span class="line"> |    |    |-- url: string (nullable = true)</span><br><span class="line"> |-- process_context: struct (nullable = true)</span><br><span class="line"> |    |-- history_res: string (nullable = true)</span><br><span class="line"> |    |-- temp_res: string (nullable = true)</span><br><span class="line"> |-- process_end_time: long (nullable = true)</span><br><span class="line"> |-- process_start_time: long (nullable = true)</span><br><span class="line"> |-- product_id: string (nullable = true)</span><br><span class="line"> |-- project_id: string (nullable = true)</span><br><span class="line"> |-- race: long (nullable = true)</span><br><span class="line"> |-- request_id: string (nullable = true)</span><br><span class="line"> |-- request_time: long (nullable = true)</span><br><span class="line"> |-- site_id: string (nullable = true)</span><br><span class="line"> |-- status: long (nullable = true)</span><br><span class="line"> |-- temp_id: string (nullable = true)</span><br><span class="line"> |-- tracks: array (nullable = true)</span><br><span class="line"> |    |-- element: struct (containsNull = true)</span><br><span class="line"> |    |    |-- box: struct (nullable = true)</span><br><span class="line"> |    |    |    |-- angle: long (nullable = true)</span><br><span class="line"> |    |    |    |-- height: long (nullable = true)</span><br><span class="line"> |    |    |    |-- left: long (nullable = true)</span><br><span class="line"> |    |    |    |-- top: long (nullable = true)</span><br><span class="line"> |    |    |    |-- width: long (nullable = true)</span><br><span class="line"> |    |    |-- host_time: long (nullable = true)</span><br><span class="line"> |    |    |-- index: long (nullable = true)</span><br><span class="line"> |    |    |-- video_time: long (nullable = true)</span><br><span class="line"> |-- user_id: string (nullable = true)</span><br><span class="line"></span><br><span class="line">19/10/28 10:17:49 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &apos;spark.debug.maxToStringFields&apos; in SparkEnv.conf.</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">|_corrupt_record|age|              branch|           camera_id|  camera_ip|         client_time|              events|             face_id|gender|is_new_user|           mall_id|match_photo_index|match_score|package_index|              photos|process_context|process_end_time|process_start_time| product_id|          project_id|race|          request_id| request_time|           site_id|status|temp_id|              tracks|user_id|</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">|           null|  0|low_quality_faceI...|afu-hanghai-yxhqg...|172.16.10.2|[1555054289000, 1...|[[1555073266644, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                0|          0|            9|[[1555054284000, ...|   [null, null]|   1555073288125|     1555073288099|trafficfull|AFU_shanghai_yxhq...|   0|f0cbcac5-60aa-498...|1555073288097|AFU_shanghai_yxhqg|    -1|       |[[[-21, 62, 884, ...|       |</span><br><span class="line">|           null| 23|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055539...|[[1555073289722, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           -1|[[1555055540000, ...|   [null, null]|   1555073292530|     1555073292487|trafficfull|AFU_beijing_xhm_t...|   0|b264f039-431e-4c9...|1555073292487|   AFU_beijing_xhm|     4|       |[[[0, 74, 1656, 1...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            3|[[1555054311000, ...|   [null, null]|   1555073297234|     1555073297137|trafficfull|AFU_shanghai_yxhq...|   1|9e9b0963-96d9-44b...|1555073297136|AFU_shanghai_yxhqg|    -1|       |[[[12, 88, 674, 4...|       |</span><br><span class="line">|           null| 22|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055539...|[[1555073289893, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|            1|[[1555055543000, ...|   [null, null]|   1555073298078|     1555073298034|trafficfull|AFU_beijing_xhm_t...|   0|8703d95e-8ca2-4a8...|1555073298034|   AFU_beijing_xhm|    -1|       |[[[7, 72, 171, 56...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            5|[[1555054311000, ...|   [null, null]|   1555073300572|     1555073300471|trafficfull|AFU_shanghai_yxhq...|   1|163a8256-d832-427...|1555073300471|AFU_shanghai_yxhqg|    -1|       |[[[19, 96, 625, 3...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-cytj-...|172.16.10.2|[0, 0, 1555068460...|[[1555073300056, ...|PROJAFU_beijing_c...|     0|      false|  AFU_beijing_cytj|                0|          0|           -1|                null|           [, ]|   1555073300572|     1555073300572|trafficfull|AFU_beijing_cytj_...|   0|5499d569-4067-42d...|1555073300494|  AFU_beijing_cytj|     4|       |[[[26, 55, 1341, ...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           14|[[1555055520000, ...|   [null, null]|   1555073301353|     1555073301300|trafficfull|AFU_beijing_xhm_t...|   1|80f35edb-e3c9-48f...|1555073301299|   AFU_beijing_xhm|    -1|       |[[[25, 110, 1269,...|       |</span><br><span class="line">|           null| 29|  low_quality_faceId|afu-beijing-cytj-...|172.16.10.2|[0, 0, 1555068461...|[[1555073300728, ...|PROJAFU_beijing_c...|     0|       true|  AFU_beijing_cytj|                0|          0|           -1|[[1555068461000, ...|   [null, null]|   1555073302108|     1555073302059|trafficfull|AFU_beijing_cytj_...|   1|ddbacad2-cd35-4a1...|1555073302058|  AFU_beijing_cytj|     4|       |[[[1, 108, 825, 1...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|            6|[[1555054311000, ...|   [null, null]|   1555073302221|     1555073302126|trafficfull|AFU_shanghai_yxhq...|   1|3b0353b3-d5ec-492...|1555073302125|AFU_shanghai_yxhqg|    -1|       |[[[4, 85, 647, 39...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055554...|[[1555073303646, ...|PROJAFU_beijing_x...|     1|       true|   AFU_beijing_xhm|                0|          0|           -1|[[1555055554000, ...|   [null, null]|   1555073305191|     1555073305148|trafficfull|AFU_beijing_xhm_t...|   0|3bd7e125-ac80-4ff...|1555073305148|   AFU_beijing_xhm|     4|       |[[[11, 63, 925, 1...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           17|[[1555055520000, ...|   [null, null]|   1555073306338|     1555073306297|trafficfull|AFU_beijing_xhm_t...|   1|26383dcd-47a4-410...|1555073306297|   AFU_beijing_xhm|    -1|       |[[[11, 101, 1254,...|       |</span><br><span class="line">|           null| 26|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055563...|[[1555073312893, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                1|          0|           -1|[[1555055564000, ...|   [null, null]|   1555073314733|     1555073314663|trafficfull|AFU_beijing_xhm_t...|   0|e8a517a4-bf72-46f...|1555073314663|   AFU_beijing_xhm|     4|       |[[[0, 102, 554, 2...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055566...|[[1555073315646, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073315801|     1555073315801|trafficfull|AFU_beijing_xhm_t...|   0|1256cd65-3100-448...|1555073315797|   AFU_beijing_xhm|     4|       |[[[-8, 79, 1638, ...|       |</span><br><span class="line">|           null| 25|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055520000, 1...|[[1555073271554, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                0|          0|           23|[[1555055520000, ...|   [null, null]|   1555073316495|     1555073316453|trafficfull|AFU_beijing_xhm_t...|   1|292ea3d7-cc3b-452...|1555073316453|   AFU_beijing_xhm|    -1|       |[[[27, 99, 1243, ...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055567...|[[1555073316558, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073316856|     1555073316856|trafficfull|AFU_beijing_xhm_t...|   0|caf82eb1-8f49-485...|1555073316856|   AFU_beijing_xhm|     4|       |[[[0, 68, 1695, 3...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055567...|[[1555073316313, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073317218|     1555073317218|trafficfull|AFU_beijing_xhm_t...|   0|868a5c14-903e-461...|1555073317129|   AFU_beijing_xhm|     4|       |[[[9, 129, 993, 3...|       |</span><br><span class="line">|           null| 31|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055564000, 1...|[[1555073302556, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                2|          0|            5|[[1555055558000, ...|   [null, null]|   1555073317592|     1555073317503|trafficfull|AFU_beijing_xhm_t...|   0|bb3b4831-f1db-4f9...|1555073317503|   AFU_beijing_xhm|    -1|       |[[[6, 91, 172, 56...|       |</span><br><span class="line">|           null|  0|        empty_photos|afu-beijing-xhm-c...|172.16.10.2|[0, 0, 1555055569...|[[1555073318311, ...|PROJAFU_beijing_x...|     0|      false|   AFU_beijing_xhm|                0|          0|           -1|                null|           [, ]|   1555073318529|     1555073318529|trafficfull|AFU_beijing_xhm_t...|   0|312c43a4-a247-464...|1555073318529|   AFU_beijing_xhm|     4|       |[[[2, 73, 1024, 1...|       |</span><br><span class="line">|           null| 31|  low_quality_faceId|afu-beijing-xhm-c...|172.16.10.2|[1555055564000, 1...|[[1555073302556, ...|PROJAFU_beijing_x...|     0|       true|   AFU_beijing_xhm|                2|          0|            7|[[1555055558000, ...|   [null, null]|   1555073320823|     1555073320721|trafficfull|AFU_beijing_xhm_t...|   0|da438ef2-daf6-472...|1555073320721|   AFU_beijing_xhm|    -1|       |[[[18, 105, 186, ...|       |</span><br><span class="line">|           null| 21|  low_quality_faceId|afu-hanghai-yxhqg...|172.16.10.2|[0, 0, 1555054302...|[[1555073285646, ...|PROJAFU_shanghai_...|     0|       true|AFU_shanghai_yxhqg|                2|          0|          -18|[[1555054311000, ...|   [null, null]|   1555073321796|     1555073321700|trafficfull|AFU_shanghai_yxhq...|   1|d16a278f-3ae9-44a...|1555073321700|AFU_shanghai_yxhqg|     4|       |[[[10, 58, 598, 3...|       |</span><br><span class="line">+---------------+---+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+------+-----------+------------------+-----------------+-----------+-------------+--------------------+---------------+----------------+------------------+-----------+--------------------+----+--------------------+-------------+------------------+------+-------+--------------------+-------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&quot;is_new_user = true &quot;).show(10)</span><br><span class="line">    </span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).where(&quot;is_new_user = true&quot;).show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">结果是一样的哈 </span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">   * Filters rows using the given SQL expression.</span><br><span class="line">   * &#123;&#123;&#123;</span><br><span class="line">   *   peopleDs.where(&quot;age &gt; 15&quot;)</span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line">   *</span><br><span class="line">   * @group typedrel</span><br><span class="line">   * @since 1.6.0</span><br><span class="line">   */</span><br><span class="line">  def where(conditionExpr: String): Dataset[T] = &#123;</span><br><span class="line">    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">where 底层调用的是 filter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filter 和where 里面 有好多中写法 ：</span><br><span class="line">个人喜欢使用 &apos;列名 +判断条件</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028103234302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是报错：  加一个隐式转换</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028103316281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&apos;is_new_user === &quot;true&quot;).show(10)</span><br><span class="line">  &#125;</span><br><span class="line">结果是：</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|age|              branch|           mall_id|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">|  0|low_quality_faceI...|AFU_shanghai_yxhqg|</span><br><span class="line">| 23|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 22|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 29|  low_quality_faceId|  AFU_beijing_cytj|</span><br><span class="line">| 21|  low_quality_faceId|AFU_shanghai_yxhqg|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">| 25|  low_quality_faceId|   AFU_beijing_xhm|</span><br><span class="line">+---+--------------------+------------------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191028103611462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">写法很多 ：</span><br><span class="line"> def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line">    df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(df.col(&quot;is_new_user&quot;) === &quot;true&quot;).show(10)</span><br><span class="line">  &#125;</span><br><span class="line">结果是一样的</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">我个人是喜欢 </span><br><span class="line">import spark.implicits._</span><br><span class="line">select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;)  +  .filter(&apos;is_new_user === &quot;true&quot;)    </span><br><span class="line"></span><br><span class="line">这样写代码量少一些</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def json(spark: SparkSession) = &#123;</span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;json&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.log&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val resultDF: Dataset[Row] = df.select(&quot;age&quot;,&quot;branch&quot;,&quot;mall_id&quot;).filter(&apos;is_new_user === &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">    resultDF.write</span><br><span class="line">      .mode(SaveMode.Overwrite)</span><br><span class="line">      .format(&quot;json&quot;)</span><br><span class="line">      .save(&quot;file:///C:/IdeaProjects/spark/out-sparksql-json&quot;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    json(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028104324951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">解析json 嵌套 + Sturct类型的 你会么？  给个思路 就是  exploded +打点</span><br></pre></td></tr></table></figure></div>
<p><strong>3.读csv数据</strong><br>csv文件打开是execel能看见的<br><img src="https://img-blog.csdnimg.cn/20191028104719825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df: DataFrame = spark.read.format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- _c0: string (nullable = true)</span><br><span class="line"></span><br><span class="line">+--------------------+</span><br><span class="line">|                 _c0|</span><br><span class="line">+--------------------+</span><br><span class="line">|pid	pid_type	stor...|</span><br><span class="line">|2637034	GLOBAL	30...|</span><br><span class="line">|127599	GLOBAL	303...|</span><br><span class="line">|2626026	GLOBAL	30...|</span><br><span class="line">|2643291	GLOBAL	30...|</span><br><span class="line">|182310	GLOBAL	303...|</span><br><span class="line">|182310	GLOBAL	303...|</span><br><span class="line">|856248	GLOBAL	303...|</span><br><span class="line">|29052	GLOBAL	3039...|</span><br><span class="line">|29052	GLOBAL	3039...|</span><br><span class="line">+--------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">所以这种处理结果不是我们想要的 </span><br><span class="line">所以处理 csv 文件的时候 需要一些 option 需要我们添加的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- pid	pid_type	store_id	store_name	floor	start_time	end_time	event_type	label_version	channel: string (nullable = true)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">|pid	pid_type	store_id	store_name	floor	start_time	end_time	event_type	label_version	channel|</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">|                                                                       2637034	GLOBAL	30...|</span><br><span class="line">|                                                                       127599	GLOBAL	303...|</span><br><span class="line">|                                                                       2626026	GLOBAL	30...|</span><br><span class="line">|                                                                       2643291	GLOBAL	30...|</span><br><span class="line">|                                                                       182310	GLOBAL	303...|</span><br><span class="line">|                                                                       182310	GLOBAL	303...|</span><br><span class="line">|                                                                       856248	GLOBAL	303...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">|                                                                       29052	GLOBAL	3039...|</span><br><span class="line">+-------------------------------------------------------------------------------------------+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">表 头出来了  但是不是我们想要的</span><br><span class="line">这个头 就一列  没有分开  所以 还得加option  把头拆开</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;\t&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">root</span><br><span class="line"> |-- pid: string (nullable = true)</span><br><span class="line"> |-- pid_type: string (nullable = true)</span><br><span class="line"> |-- store_id: string (nullable = true)</span><br><span class="line"> |-- store_name: string (nullable = true)</span><br><span class="line"> |-- floor: string (nullable = true)</span><br><span class="line"> |-- start_time: string (nullable = true)</span><br><span class="line"> |-- end_time: string (nullable = true)</span><br><span class="line"> |-- event_type: string (nullable = true)</span><br><span class="line"> |-- label_version: string (nullable = true)</span><br><span class="line"> |-- channel: string (nullable = true)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">|    pid|pid_type|store_id|    store_name|floor|start_time|end_time|event_type|       label_version|channel|</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">|2637034|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  17:38:44|17:39:32|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 127599|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  20:09:26|20:18:03|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|2626026|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  11:38:21|11:38:50|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|2643291|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  21:07:31|21:09:01|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 182310|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  10:41:34|10:41:55|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 182310|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  10:42:02|10:57:19|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">| 856248|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:49:23|14:56:18|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  13:12:00|13:13:57|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:14:28|14:14:55|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">|  29052|  GLOBAL|   3039A|Onitsuka Tiger|   3F|  14:30:38|14:30:52|         0|3b47b5f2c1d95c2fb...|ch11001|</span><br><span class="line">+-------+--------+--------+--------------+-----+----------+--------+----------+--------------------+-------+</span><br><span class="line">only showing top 10 rows</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def csv(spark: SparkSession) = &#123;</span><br><span class="line"></span><br><span class="line">    val df = spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">      .option(&quot;sep&quot;,&quot;\t&quot;)</span><br><span class="line">      .format(&quot;csv&quot;).load(&quot;file:///C:/IdeaProjects/spark/data/data.csv&quot;)</span><br><span class="line"></span><br><span class="line">    df.printSchema()</span><br><span class="line">    println(&quot;......&quot;)</span><br><span class="line">    df.show(10)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line">    df.select(&quot;pid&quot;,&quot;store_name&quot;).filter($&quot;store_id&quot; === &quot;3039A&quot;)</span><br><span class="line">      .write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;)</span><br><span class="line">      .save(&quot;file:///C:/IdeaProjects/spark/out-sparksql-csv&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">     csv(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028110000707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这些 option 参数 我是怎么知道的 ？去源码里找 </span><br><span class="line">CSVOptions 类下面</span><br></pre></td></tr></table></figure></div>
<p><strong>4.读jdbc数据</strong><br>MySQL中的数据是这样的<br><img src="https://img-blog.csdnimg.cn/20191028111043452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">  def jdbc(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val jdbcDF = spark.read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">        jdbcDF.show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是:</span><br><span class="line">+---------------+-----+---+</span><br><span class="line">|         domain|  url|cnt|</span><br><span class="line">+---------------+-----+---+</span><br><span class="line">|  www.baidu.com| url5|  5|</span><br><span class="line">|  www.baidu.com| url2|  2|</span><br><span class="line">|  www.baidu.com| url4|  4|</span><br><span class="line">|  www.baidu.com| url1|  1|</span><br><span class="line">|  www.baidu.com| url3|  3|</span><br><span class="line">|www.twitter.com| url6|  1|</span><br><span class="line">|www.twitter.com|url10| 11|</span><br><span class="line">|www.twitter.com| url9|  6|</span><br><span class="line">| www.google.com| url2|  2|</span><br><span class="line">| www.google.com| url6|  7|</span><br><span class="line">| www.google.com| url1|  1|</span><br><span class="line">| www.google.com| url8|  7|</span><br><span class="line">+---------------+-----+---+</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank" rel="noopener">JDBC To Other Databases</a><br>官网有好多写法 </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line">  def jdbc(spark: SparkSession) = &#123;</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val jdbcDF = spark.read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">        jdbcDF.show()</span><br><span class="line"></span><br><span class="line">    jdbcDF.filter(&apos;domain === &quot;www.google.com&quot;)</span><br><span class="line">      .write.format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://hadoop101:3306&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;hive_dwd.topn_2&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">      .save()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：写回MySQL</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show tables;</span><br><span class="line">+--------------------+</span><br><span class="line">| Tables_in_hive_dwd |</span><br><span class="line">+--------------------+</span><br><span class="line">| stat               |</span><br><span class="line">| topn               |</span><br><span class="line">| topn_2             |</span><br><span class="line">+--------------------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from topn_2;</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| domain         | url  | cnt  |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| www.google.com | url2 |    2 |</span><br><span class="line">| www.google.com | url6 |    7 |</span><br><span class="line">| www.google.com | url1 |    1 |</span><br><span class="line">| www.google.com | url8 |    7 |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">但是按照上面写 是不是太恶心了 参数 全都写死的 </span><br><span class="line">通过读取配置文件的方式  ：有很多种写法  这里列出一个</span><br><span class="line"></span><br><span class="line">object SparkSessionApp2 &#123;</span><br><span class="line"></span><br><span class="line">    def jdbc2(spark: SparkSession) = &#123;</span><br><span class="line">      import spark.implicits._</span><br><span class="line"></span><br><span class="line">      val config = ConfigFactory.load()</span><br><span class="line">      val url = config.getString(&quot;db.default.url&quot;)</span><br><span class="line">      val user = config.getString(&quot;db.default.user&quot;)</span><br><span class="line">      val password = config.getString(&quot;db.default.password&quot;)</span><br><span class="line">      val srcTable = config.getString(&quot;db.default.srctable&quot;)</span><br><span class="line">      val targetTable = config.getString(&quot;db.default.targettable&quot;)</span><br><span class="line"></span><br><span class="line">      val jdbcDF = spark.read</span><br><span class="line">        .format(&quot;jdbc&quot;)</span><br><span class="line">        .option(&quot;url&quot;, url)</span><br><span class="line">        .option(&quot;dbtable&quot;, srcTable)</span><br><span class="line">        .option(&quot;user&quot;, user)</span><br><span class="line">        .option(&quot;password&quot;, password)</span><br><span class="line">        .load()</span><br><span class="line"></span><br><span class="line">          jdbcDF.show()</span><br><span class="line"></span><br><span class="line">      jdbcDF.filter(&apos;domain === &quot;www.google.com&quot;)</span><br><span class="line">        .write.format(&quot;jdbc&quot;)</span><br><span class="line">        .option(&quot;url&quot;, url)</span><br><span class="line">        .option(&quot;dbtable&quot;, targetTable)</span><br><span class="line">        .option(&quot;user&quot;, user)</span><br><span class="line">        .option(&quot;password&quot;, password)</span><br><span class="line">        .save()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder()</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    jdbc2(spark)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191028113030955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from topn_3;</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| domain         | url  | cnt  |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">| www.google.com | url2 |    2 |</span><br><span class="line">| www.google.com | url6 |    7 |</span><br><span class="line">| www.google.com | url1 |    1 |</span><br><span class="line">| www.google.com | url8 |    7 |</span><br><span class="line">+----------------+------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark11-shuffle" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/28/Spark11-shuffle/">Spark11-shuffle</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/28/Spark11-shuffle/" class="article-date">
  <time datetime="2018-01-28T12:06:12.000Z" itemprop="datePublished">2018-01-28</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">shuffle：</span><br><span class="line">	算子：groupByKey、reduceByKey、countByKey、join(不是所有的join)</span><br><span class="line">	遇到宽依赖就会有shuffle的产生</span><br><span class="line">	遇到shuffle 就会切出新的stage</span><br><span class="line"></span><br><span class="line">数据倾斜：相同key的数据会分发到同一个task中去执行</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">找源码 要到 SparkEnv里面找</span><br><span class="line">Spark发展到现在经过几种类型的shufle</span><br><span class="line">1.  hash ： HashShuffleManager    现在源码里已经没有了 这个shuffle 存在一些弊端 所以不用了   1.2版本之前 默认是它</span><br><span class="line">2.   sort ： SortShuffleManager   &gt;1.2版本 底层默认是它</span><br><span class="line">3.tungsten-sort ：  SortShuffleManager   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">调优：代码、资源、数据倾斜(Skew)、Shuffle(这个影响面 不是太大的 )</span><br></pre></td></tr></table></figure></div>

<p><strong>HashShuffleManager</strong><br>为了看源码 idea里pom使用老点的版本</p>
<p><img src="https://img-blog.csdnimg.cn/20191025204517264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">HashShuffleManager    ：</span><br><span class="line">    每一个maptask 要为每个reduceTask 创建“东西”</span><br><span class="line">	东西：内存(bucket) + 落地的文件file(就是 bucket内存满了 会文件落地)</span><br><span class="line"></span><br><span class="line">产生的问题：</span><br><span class="line">	1.产生过多的file   maptask数 * reducetask数</span><br><span class="line">	    一般情况下 maptask 和reducetask数 都是比较大的 所以中间产生的磁盘文件数量惊人</span><br><span class="line"></span><br><span class="line">	那么bucket产生多少个呢？</span><br><span class="line">	   bucket用完是可以释放的</span><br><span class="line">	2.耗费过多的内存空间</span><br><span class="line">		每个maptask需要开启 reduce个数的 bucket   会开启 M*R 的bucket数量 虽然是可以释放</span><br><span class="line">		但是同一时间点 同时存在的bucket的数量  reducetask个数 * core的数量 </span><br><span class="line">	3.bucket多大？</span><br><span class="line">		官网 </span><br><span class="line">		spark.shuffle.file.buffer  默认32k</span><br><span class="line"></span><br><span class="line">HashShuffleManager    优化：</span><br><span class="line">优化后：</span><br><span class="line">一个core上连续执行的shufflemaptask可以共用一个输出文件</span><br><span class="line"></span><br><span class="line">core第一次的生成的文件 会形成 reduce个数 的shuffleFile文件 </span><br><span class="line">core第二次 就是运行下一个shufflemaptask 的时候 生成的文件会追加到第一次生成的shuffleBlock文件</span><br><span class="line"></span><br><span class="line">文件数 ： core数 *reduce task数</span><br><span class="line">优化后的文件数减少了好多 </span><br><span class="line">怎么开启这个优化呢？</span><br><span class="line">配置一个参数即可 spark.shuffle.consolidateFiles 为true</span><br><span class="line"></span><br><span class="line">这个shuffle 了解和面试即可 不作为重点</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025210825723.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.自己梳理下SortShuffleManner</span><br><span class="line">2.为什么说调优的时候 Shuffle不作为重点 因为 新版底层就两个 sort(默认) 和 tungsten-sort</span><br><span class="line">3.hash 、 sort 要观察shuffle产生的文件数到底是多少个？ 使用spark-shell就可以测试 eg ：wc为例  设置maptask的数量  和reducetask的数量</span><br><span class="line">( textFile算子可以设置 maptask数量   reduceByKey可以设置 reducetask数量)</span><br><span class="line">shuffle的数据是落地的 落地到哪里呢？spark.local.dir 参数 控制的落到那个文件夹</span><br><span class="line">  测试：</span><br><span class="line">  	1.普通的HashShuffleMannager</span><br><span class="line">  	2.consolidateFiles的HashShuffleManager</span><br><span class="line">  	3.sort</span><br><span class="line">  	shuffle生成的文件数</span><br><span class="line">  	参数的设置会吧 ！！！</span><br></pre></td></tr></table></figure></div>

<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>你首先要知道spark哪些地方需要优化？<br><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Tuning Spark</a></p>
<p>Because of the in-memory nature of most Spark computations, Spark programs can be <strong>bottlenecked</strong> by any resource in the cluster: CPU, network bandwidth, or memory. Most often, <strong>if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you also need to do some tuning, such as storing RDDs in serialized form, to decrease memory usage.</strong> This guide will cover two main topics: <strong>data serialization</strong>, which is crucial for good network performance and can also reduce memory use, and <strong>memory tuning.</strong> We also sketch several smaller topics.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bottlenecked ：瓶颈</span><br><span class="line">1.CPU, network bandwidth, or memory</span><br><span class="line"></span><br><span class="line">也就是说你公司集群资源不够就别搞spark了 --&gt;cpu+mem</span><br><span class="line">带宽 现在一般都是万兆的 千兆的就有点扯</span><br><span class="line"></span><br><span class="line">官方从两个方面：</span><br><span class="line">1.data serialization</span><br><span class="line">2.memory tuning</span><br><span class="line"></span><br><span class="line">1.数据序列化  可以内存的使用 就是数据体积变小了 占的内存少</span><br><span class="line">2.内存调优</span><br></pre></td></tr></table></figure></div>
<p><strong>Data Serialization</strong><br>看官网 写的很清楚<br>Serialization plays an important role in the performance of any distributed application.<br>因为算子里用到的东西 都是要经过序列化才可以<br><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Tuning Spark</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Java serialization: </span><br><span class="line">	Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.</span><br><span class="line">Kryo serialization: </span><br><span class="line">	Spark can also use the Kryo library (version 4) to serialize objects more quickly. </span><br><span class="line">	Kryo is significantly faster and more compact than Java serialization (often as much as 10x),</span><br><span class="line">	 but does not support all Serializable types and requires you to register the classes</span><br><span class="line">	  you’ll use in the program in advance for best performance.</span><br><span class="line"></span><br><span class="line">java：slow、large</span><br><span class="line">Kryo：</span><br><span class="line">		1. requires you to register the classes</span><br><span class="line">			quickly、compact </span><br><span class="line">		2.not register the classes</span><br><span class="line">			也能运行 性能刚好相反</span><br><span class="line"></span><br><span class="line">使用  </span><br><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) 来更换序列化的方式  因为默认是java的 </span><br><span class="line">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br><span class="line">把你自己定义的类 MyClass1 有几个就写进去几个  </span><br><span class="line"></span><br><span class="line">1.代码里是这样写的 但是建议代码里别写死 最好别写 我说的是 序列化的方式 而不是注册</span><br><span class="line">2,。最好  配置在 spark-default.conf 下面  ***</span><br><span class="line">或者</span><br><span class="line">3. spark-submit --conf k=v 也可以  ***</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"> spark.master                     local[2]</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"> spark.eventLog.dir               hdfs://hadoop101:8020/spark_directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br><span class="line">[double_happy@hadoop101 conf]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">测试 通过cache 看看页面  data 大小就ok了 </span><br><span class="line"> 看看ui  cache的data大小：</span><br><span class="line">1.data=&gt;rdd==&gt;rdd.cache   ==&gt;action     这种 默认  cache 存储级别是 内存</span><br><span class="line">2.data=&gt;rdd==&gt;rdd.persisit(mem_only_ser)==&gt;action  默认java序列化</span><br><span class="line">3.data=&gt;rdd==&gt;rdd.persisit(mem_only_ser)==&gt;action   把这个参数 spark.serializer  打开 设置 kryo   这种就是没有注册</span><br><span class="line">4.data=&gt;rdd==&gt;rdd.persisit(mem_only_ser)==&gt;action  把这个参数 spark.serializer设置 kryo  +register  注册</span><br></pre></td></tr></table></figure></div>
<p><strong>Determining Memory Consumption</strong><br>你怎么知道一个数据集或者对象的内存占用了多少了呢？</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">两种方法：</span><br><span class="line">1.The best way to size the amount of memory consumption a dataset will require is to create an RDD, </span><br><span class="line">	put it into cache, and look at the    “Storage” page in the web UI. </span><br><span class="line">	The page will tell you how much memory the RDD is occupying.</span><br><span class="line"></span><br><span class="line">2.To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method. </span><br><span class="line"></span><br><span class="line">第二种就是使用 SizeEstimator类</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">-r-xr-xr-x 1 double_happy double_happy  14M Sep 21 12:33 ip.txt</span><br><span class="line">scala&gt; import org.apache.spark.util.SizeEstimator</span><br><span class="line">import org.apache.spark.util.SizeEstimator</span><br><span class="line"></span><br><span class="line">scala&gt;  SizeEstimator.estimate(&quot;file:///home/double_happy/data/ip.txt&quot;)</span><br><span class="line">res0: Long = 120</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">我的文件是14m  预估占用内存是 120m   </span><br><span class="line"></span><br><span class="line">测试第一种方法</span><br><span class="line">scala&gt; import org.apache.spark.util.SizeEstimator</span><br><span class="line">import org.apache.spark.util.SizeEstimator</span><br><span class="line"></span><br><span class="line">scala&gt;  SizeEstimator.estimate(&quot;file:///home/double_happy/data/ip.txt&quot;)</span><br><span class="line">res0: Long = 120</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd = sc.textFile(&quot;file:///home/double_happy/data/ip.txt&quot;)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[String] = file:///home/double_happy/data/ip.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cache</span><br><span class="line">res1: rdd.type = file:///home/double_happy/data/ip.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">[Stage 0:&gt;                                                          (0 + 2) / 2[Stage 0:=============================&gt;                             (1 + 1) / 2                                                                               res2: Long = 115395</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191026114018994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为什么第二种方法多这么多？</span><br><span class="line">页面上的肯定是膨胀的变大是必然的  为什么 第二种多这么多？可以去看源码 查查资料</span><br></pre></td></tr></table></figure></div>


<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">If you have less than 32 GB of RAM, set the JVM flag -XX:+UseCompressedOops to make pointers be four bytes instead of eight. You can add these options in spark-env.sh.</span><br><span class="line"></span><br><span class="line">如果你的内存小于32G 设置JVM  -XX:+UseCompressedOops 会好一些</span><br></pre></td></tr></table></figure></div>

<p><strong>Garbage Collection Tuning</strong><br>需要先了解内存管理 和 jvm 才能 gc调优</p>
<p><a href="http://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning" target="_blank" rel="noopener">Advanced GC Tuning</a></p>
<p>gc我没有复习 所以这块之后补上。</p>
<p><strong>Data Locality</strong><br> 数据本地化 了解即可 很难实现<br>   <strong>If data and the code that operates on it are together then computation tends to be fast.</strong><br>   <img src="https://img-blog.csdnimg.cn/20191026115743915.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>But if code and data are separated, <strong>one must move to the other.</strong> Typically it is faster to ship serialized code from place to place than a chunk of data because code size is much smaller than data. Spark builds its scheduling around this general principle of data locality.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">one must move to the other 就是这两个总有一个要移动的</span><br><span class="line">1.代码是 driver端发过去的 ， 首先代码移动是没有难度的 ，但是</span><br><span class="line">你的作业申请到资源后 就把作业定到申请的executor上去了 所以 正常情况下 是数据的移动  不存在代码的移动</span><br></pre></td></tr></table></figure></div>
<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from <strong>closest to farthest</strong>:</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">levels of locality：</span><br><span class="line">	PROCESS_LOCAL</span><br><span class="line">	NODE_LOCAL </span><br><span class="line">	NO_PREF </span><br><span class="line">	RACK_LOCAL </span><br><span class="line">	ANY </span><br><span class="line"></span><br><span class="line">这个等级spark默认从近--&gt;远 选取 知道可以运行   毕竟越近越好嘛 生产上 达到NO_PREF 就挺好的 </span><br><span class="line"></span><br><span class="line">可以把这个理解成 妹子找对象  富二代 --&gt;高富帅--&gt;xxx---&gt;屌丝---&gt;男的     最后30岁了 依旧单身</span><br></pre></td></tr></table></figure></div>
<p><strong>总结shuffle参数：</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1.spark.shuffle.consolidateFiles     hashshuffle优化开启参数</span><br><span class="line">2.spark.shuffle.file.buffer.kb       提高buffer 默认32k 可以调64 再128   相当于map端的</span><br><span class="line"></span><br><span class="line">reduce端的也有  实际上 reduce 是 拉去file 到一个缓存区的 </span><br><span class="line">3.spark.reducer.maxMbInFlight     reduce的缓冲区大小 默认48m 稍微调大一点 96m</span><br><span class="line">既然有map端的和reduce端的 那么中间 应该也有的</span><br><span class="line"></span><br><span class="line">4.spark.shuffle.io.maxRetries   最大次数是默认是3 就是shuffle 或者reduce端失败了 中间这块要去map端再去做一遍</span><br><span class="line">5.spark.shuffle.io.retryWait   这是上面 要等待多久去retry一次呢 默认5s</span><br><span class="line">6.spark.shuffle.sort.bypassMergeThreshold  这是 sort shuffle 阈值是200 合并数据 就是sort需要排序的 但是在有些场景不需要排序 由这参数控制的</span><br><span class="line">7.spark.shuffle.spill.compress   shuffle的时候需不需压缩</span><br><span class="line">8.spark.io.compression.codec  需要压缩 你得配置一个 codec吧 </span><br><span class="line">9.spark.shuffle.compress 和 7 有什么区别呀 ？map output files 和 compress data spilled during shuffles的区别</span><br><span class="line">10.spark.shuffle.manager  官网上没有 源码里的 默认走的是sortshuffle </span><br><span class="line"></span><br><span class="line">这些参数就是 shuffle过程中我们要用到的调优参数</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark10-内存管理" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/26/Spark10-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/">Spark10--内存管理</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/26/Spark10-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" class="article-date">
  <time datetime="2018-01-26T12:05:30.000Z" itemprop="datePublished">2018-01-26</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ spark-submit --help</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master yarn </span><br><span class="line">提交到yarn的时候 肯定是有一堆executor进程的对吧  那么 到底有几个呢？每个executor 多少core？每个executor 多少内存？</span><br><span class="line">这些是提交作业的时候都要配置的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> on yarn 模式</span><br><span class="line"> --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G)</span><br><span class="line">--executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode）</span><br><span class="line">                              					</span><br><span class="line"> per executor到底使用几个cpu core呢？</span><br><span class="line"></span><br><span class="line"> --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line"></span><br><span class="line">之前我就遇到一个问题就是 </span><br><span class="line">我的数据量 没有我给的executor内存大 为什么程序跑不出来呢？还oom呢？</span><br><span class="line"></span><br><span class="line">说明对spark内存管理不了解</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">Tuning Spark</a><br>优化章节<br><a href="http://spark.apache.org/docs/latest/tuning.html#memory-management-overview" target="_blank" rel="noopener">Memory Management Overview</a></p>
<p>Memory usage in Spark largely falls under one of two categories: <strong>execution and storage</strong>. <strong>Execution</strong> memory refers to that used for computation in <strong>shuffles, joins, sorts and aggregations</strong>, while <strong>storage</strong> memory refers to that used for <strong>caching and propagating internal data across the cluster.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">内存分为：</span><br><span class="line">    execution ： computation in shuffles, joins, sorts and aggregations  用于计算的</span><br><span class="line">	storage ：caching and propagating internal data   用于存储</span><br><span class="line"></span><br><span class="line">所以spark的executor内存是经过划分的 你给他1G 用于计算的达不到1G</span><br><span class="line">查看源码：</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">SparkEnv类：</span><br><span class="line"></span><br><span class="line">    val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</span><br><span class="line">    val memoryManager: MemoryManager =</span><br><span class="line">      if (useLegacyMemoryManager) &#123;</span><br><span class="line">        new StaticMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        UnifiedMemoryManager(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.memory.useLegacyMode 什么意思呢？去官网找一下</span><br><span class="line"></span><br><span class="line">决定你spark采用什么样的内存管理机制</span><br><span class="line">是否使用历史遗留版本  false</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark Configuration</a></p>
<p><img src="https://img-blog.csdnimg.cn/20191025155826318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>1.SparkEnv类 进去搜索memoryManager<br><img src="https://img-blog.csdnimg.cn/20191025161627818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>2.点进去StaticMemoryManager    </p>
<p><img src="https://img-blog.csdnimg.cn/20191025161814167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>3.点进去getMaxExecutionMemory 或者getMaxStorageMemory  点不进去 说明这个方法就在这个类里面<br>搜索getMaxExecutionMemory<br><img src="https://img-blog.csdnimg.cn/20191025161953166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StaticMemoryManager    历史遗留版本   静态内存管理</span><br><span class="line">UnifiedMemoryManager    统一内存管理</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">静态内存管理机制： 存储和执行是单独的</span><br><span class="line">  StaticMemoryManager &#123;</span><br><span class="line">	getMaxExecutionMemory&#123;</span><br><span class="line"></span><br><span class="line">    val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</span><br><span class="line">   ......</span><br><span class="line">	val memoryFraction = conf.getDouble(&quot;spark.shuffle.memoryFraction&quot;, 0.2) //默认0.2</span><br><span class="line">    val safetyFraction = conf.getDouble(&quot;spark.shuffle.safetyFraction&quot;, 0.8) //默认0.8</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong   //1000m*0.2*0.8 = 160m</span><br><span class="line"></span><br><span class="line">  你传进来1g 真正用来计算的Execution 才 160m </span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	getMaxStorageMemory&#123;</span><br><span class="line">    val memoryFraction = conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6)</span><br><span class="line">    val safetyFraction = conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9)</span><br><span class="line">    (systemMaxMemory * memoryFraction * safetyFraction).toLong   //1000m*0.6*0.9 = 540m</span><br><span class="line"></span><br><span class="line">    你传进来1g 用来存储的的Storage 540m</span><br><span class="line">    如果你整个作业不需要 cache 不需要缓存 那么这个部分的内存就浪费掉了</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">systemMaxMemory 假设是传进来的内存 实际上比传进来的内存小一点</span><br><span class="line">所以你传进来的内存 是有个占比 有安全系数占比</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"> 统一内存管理:存储和执行内存是公用的 ==》会有相互借内存的</span><br><span class="line">UnifiedMemoryManager&#123;</span><br><span class="line">	val maxMemory = getMaxMemory(conf)&#123;</span><br><span class="line">		val systemMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</span><br><span class="line">		val reservedMemory = 300m</span><br><span class="line">		。。。。。</span><br><span class="line">		//1000m - 300m </span><br><span class="line">		val usableMemory = systemMemory - reservedMemory</span><br><span class="line">   		val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)</span><br><span class="line">    	(usableMemory * memoryFraction).toLong  //(1000m - 300m)*0.6 = 420m</span><br><span class="line"></span><br><span class="line">    	你真正能使用的内存 420m 这是存储端和执行端公有的 就这么多</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	//存储的占了0.5</span><br><span class="line">	onHeapStorageRegionSize =</span><br><span class="line">        (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">getMaxMemory  :Return the total amount of memory shared between execution and storage, in bytes.</span><br><span class="line">所以最终：</span><br><span class="line">Storage ： (1000m - 300m)*0.6*0.5 = 210m</span><br><span class="line">Execution ： 210m</span><br></pre></td></tr></table></figure></div>
<p>新版内存管理：<br>In Spark, execution and storage share a unified region (M). When <strong>no execution memory is used</strong>, <strong>storage can acquire all the available memory and vice versa.</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">no execution memory is used 那么storage 会获取所有资源</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Spark1.0版本：</span><br><span class="line">静态资源管理：</span><br><span class="line">	execution | storage        </span><br><span class="line">	1.如果storage用来做cache的很少 storage就剩余很多内存资源</span><br><span class="line">	那么execution 做sort、join、shuffle的如果 这部分的内存资源不够 只能 spill to disk 。</span><br><span class="line">	2.反过来</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">到了Spark1.6版本：</span><br><span class="line">统一内存管理：</span><br><span class="line">	名字都变了</span><br><span class="line">	execution|storage     默认各占50%</span><br><span class="line">	这块 execution的优先级高，storage如果cache的满了 把cache的数据spill to disk，</span><br><span class="line">	如果execution的不够用，那么他会去storage拿资源，极限情况下，只会给storage留一丢丢资源，不会让storage很没面子。但是 execution就是有借无还，谁让他优先级高呢。</span><br><span class="line">	还有一点就是 execution借完了之后 storage此时也需要cache 为什么execution不能还给storage内存呢？</span><br><span class="line">	如果此时execution 正在shuffle 还了内存 execution 会出现问题的 所以不能还</span><br></pre></td></tr></table></figure></div>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark009-spark-shell执行流程" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/26/Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/">Spark009--spark-shell执行流程</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/26/Spark009-spark-shell%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/" class="article-date">
  <time datetime="2018-01-26T12:04:44.000Z" itemprop="datePublished">2018-01-26</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="spark-shell脚本"><a href="#spark-shell脚本" class="headerlink" title="spark-shell脚本"></a>spark-shell脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-shell </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"># Shell script for starting the Spark Shell REPL</span><br><span class="line"></span><br><span class="line">cygwin=false     </span><br><span class="line">case &quot;$(uname)&quot; in     </span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"># Enter posix mode for bash</span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE=&quot;Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Scala REPL options:</span><br><span class="line">  -I &lt;file&gt;                   preload &lt;file&gt;, enforcing line-by-line interpretation&quot;</span><br><span class="line"></span><br><span class="line"># SPARK-4161: scala does not assume use of the java classpath,</span><br><span class="line"># so we need to add the &quot;-Dscala.usejavacp=true&quot; flag manually. We</span><br><span class="line"># do this specifically for the Spark shell because the scala REPL</span><br><span class="line"># has its own class loader, and any additional classpath specified</span><br><span class="line"># through spark.driver.extraClassPath is not automatically propagated.</span><br><span class="line">SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true&quot;</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # Workaround for issue involving JLine and Cygwin</span><br><span class="line">    # (see http://sourceforge.net/p/jline/bugs/40/).</span><br><span class="line">    # If you&apos;re using the Mintty terminal emulator in Cygwin, may need to set the</span><br><span class="line">    # &quot;Backspace sends ^H&quot; setting in &quot;Keys&quot; section of the Mintty options</span><br><span class="line">    # (see https://github.com/sbt/sbt/issues/562).</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot;</span><br><span class="line">    &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line">  fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in</span><br><span class="line"># binary distribution of Spark where Scala is not installed</span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=&quot;&quot;</span><br><span class="line"></span><br><span class="line"># restore stty settings (echo in particular)</span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=&quot;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ &quot;$saved_stty&quot; != &quot;&quot; ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># to reenable echo if we are interrupted before completing.</span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"># save terminal settings</span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"># clear on error so we don&apos;t later try to restore them</span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line"></span><br><span class="line"># record the exit status lest it be overwritten:</span><br><span class="line"># then reenable echo and propagate the code.</span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.  cygwin=false  //windows 电脑操作linux东西 要安装cygwin  </span><br><span class="line">2.  case &quot;$(uname)&quot; in       //uname 知道我们是什么操作系统</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025135328497.png" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ uname -r        //内核的版本</span><br><span class="line">3.10.0-514.26.2.el7.x86_64</span><br><span class="line">[double_happy@hadoop101 bin]$ uname -a      //打印所有的信息</span><br><span class="line">Linux hadoop101 3.10.0-514.26.2.el7.x86_64 #1 SMP Tue Jul 4 15:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>
<p><strong>case in</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">      模式1)</span><br><span class="line">      		command</span><br><span class="line">      ;;</span><br><span class="line">      模式2)</span><br><span class="line">      		command</span><br><span class="line">      ;;</span><br><span class="line">      *)</span><br><span class="line">      	command</span><br><span class="line">      	;;</span><br><span class="line">      	esac</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat test.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">cygwin=false     </span><br><span class="line">case &quot;$(uname)&quot; in     </span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line">echo $cygwin</span><br><span class="line">[double_happy@hadoop101 script]$ sh test.sh </span><br><span class="line">false</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat case.sh </span><br><span class="line">read -p &quot;press key , then press return:&quot; KEY</span><br><span class="line">case $KEY in</span><br><span class="line">        [a-z]|[A-Z])</span><br><span class="line">echo &quot;this is a letter..&quot;;;</span><br><span class="line">        [0-9])</span><br><span class="line">echo &quot;this is a digit...&quot; ;;</span><br><span class="line">        *)</span><br><span class="line">echo &quot;other..&quot; ;;</span><br><span class="line">esac</span><br><span class="line">[double_happy@hadoop101 script]$ sh case.sh </span><br><span class="line">press key , then press return:1</span><br><span class="line">this is a digit...</span><br><span class="line">[double_happy@hadoop101 script]$ sh case.sh </span><br><span class="line">press key , then press return:a</span><br><span class="line">this is a letter..</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>
<p><strong>if -z</strong><br><a href="https://www.cnblogs.com/new-journey/p/11017659.html" target="_blank" rel="noopener">shell 参数</a></p>
<p>if [ -z “${SPARK_HOME}” ]; then<br>  source “$(dirname “$0”)”/find-spark-home<br>fi</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[ -z STRING ]  “STRING” 的长度为零则为真。  </span><br><span class="line">dirname 是什么？</span><br><span class="line">获取当前的路径</span><br><span class="line"></span><br><span class="line">find-spark-home是在bin目录下  这脚本里有 就是去export SPARK_HOME</span><br><span class="line"> source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home    目的就是找 SPARK_HOME</span><br><span class="line"> 所以你环境里配置了 SPARK_HOME if那个shell 就直接跳过去 就不用找SPARK_HOME</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat dirname.sh </span><br><span class="line">HOME=`cd $(dirname &quot;$0&quot;);pwd`</span><br><span class="line">echo $HOME</span><br><span class="line">[double_happy@hadoop101 script]$ sh dirname.sh </span><br><span class="line">/home/double_happy/script</span><br><span class="line">[double_happy@hadoop101 script]$</span><br></pre></td></tr></table></figure></div>

<p><strong>main</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">main &quot;$@&quot;  main方法里 ：</span><br><span class="line"></span><br><span class="line"> &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot;</span><br><span class="line"></span><br><span class="line">知道 spark-shell底层调用的是  spark-submit</span><br></pre></td></tr></table></figure></div>
<p><strong>$@</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat main.sh </span><br><span class="line">function main()&#123;</span><br><span class="line">        echo &quot;.....&quot;$@</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 script]$ sh main.sh abc </span><br><span class="line">.....abc</span><br><span class="line">[double_happy@hadoop101 script]$ sh main.sh abc 123 456</span><br><span class="line">.....abc 123 456</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line">$@ :就是把一堆输入的参数  带走</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat main.sh </span><br><span class="line">function main()&#123;</span><br><span class="line">        echo &quot;.....&quot;$@</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 script]$ mv main.sh spark-shell</span><br><span class="line">[double_happy@hadoop101 script]$ chmod +x spark-shell </span><br><span class="line">[double_happy@hadoop101 script]$ ./spark-shell --master yarn --jars mysql.driver.jar</span><br><span class="line">.....--master yarn --jars mysql.driver.jar</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">明白了吗 spark-shell的参数 就是这么传进去的</span><br></pre></td></tr></table></figure></div>
<h2 id="spark-submit脚本"><a href="#spark-submit脚本" class="headerlink" title="spark-submit脚本"></a>spark-submit脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-submit </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># disable randomized hash for string in Python 3.3+</span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br><span class="line"></span><br><span class="line">exec 是做什么的？</span><br><span class="line">就是一个执行的命令</span><br><span class="line">例如在当前shell中执行 exec ls  表示执行ls这条命令来替换当前的shell ，即为执行完后会退出当前shell。</span><br><span class="line"></span><br><span class="line">为了避免这个结果的影响，一般将exec命令放到一个shell脚本中，用主脚本调用这个脚本，调用处可以用bash  xx.sh(xx.sh为存放exec命令的脚本)，这样会为xx.sh建立一个子shell去执行，当执行exec后该子脚本进程就被替换成相应的exec的命令。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025145541791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>直接退出了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 script]$ cat exec.sh </span><br><span class="line">exec ls</span><br><span class="line">[double_happy@hadoop101 script]$ sh exec.sh </span><br><span class="line">azkaban-job  case.sh  dirname.sh  exec.sh  flume-agent  spark-shell  test.sh</span><br><span class="line">[double_happy@hadoop101 script]$ </span><br><span class="line"></span><br><span class="line">明白了吗？把exec 封装在一个shell 里 去调用别的脚本</span><br></pre></td></tr></table></figure></div>
<h2 id="spark-class脚本"><a href="#spark-class脚本" class="headerlink" title="spark-class脚本"></a>spark-class脚本</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ cat spark-class </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;&quot;/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"># Find the java binary</span><br><span class="line">if [ -n &quot;$&#123;JAVA_HOME&#125;&quot; ]; then</span><br><span class="line">  RUNNER=&quot;$&#123;JAVA_HOME&#125;/bin/java&quot;</span><br><span class="line">else</span><br><span class="line">  if [ &quot;$(command -v java)&quot; ]; then</span><br><span class="line">    RUNNER=&quot;java&quot;</span><br><span class="line">  else</span><br><span class="line">    echo &quot;JAVA_HOME is not set&quot; &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Find Spark jars.</span><br><span class="line">if [ -d &quot;$&#123;SPARK_HOME&#125;/jars&quot; ]; then</span><br><span class="line">  SPARK_JARS_DIR=&quot;$&#123;SPARK_HOME&#125;/jars&quot;</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR=&quot;$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d &quot;$SPARK_JARS_DIR&quot; ] &amp;&amp; [ -z &quot;$SPARK_TESTING$SPARK_SQL_TESTING&quot; ]; then</span><br><span class="line">  echo &quot;Failed to find Spark jars directory ($SPARK_JARS_DIR).&quot; 1&gt;&amp;2</span><br><span class="line">  echo &quot;You need to build Spark with the target \&quot;package\&quot; before running this program.&quot; 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH=&quot;$SPARK_JARS_DIR/*&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># Add the launcher build dir to the classpath if requested.</span><br><span class="line">if [ -n &quot;$SPARK_PREPEND_CLASSES&quot; ]; then</span><br><span class="line">  LAUNCH_CLASSPATH=&quot;$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># For tests</span><br><span class="line">if [[ -n &quot;$SPARK_TESTING&quot; ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># The launcher library will print arguments separated by a NULL character, to allow arguments with</span><br><span class="line"># characters that would be otherwise interpreted by the shell. Read that in a while loop, populating</span><br><span class="line"># an array that will be used to exec the final command.</span><br><span class="line">#</span><br><span class="line"># The exit code of the launcher is appended to the output, so the parent shell removes it from the</span><br><span class="line"># command array and checks the value to see if the launcher succeeded.</span><br><span class="line">build_command() &#123;</span><br><span class="line">  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">  printf &quot;%d\0&quot; $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Turn off posix mode since it does not allow process substitution</span><br><span class="line">set +o posix</span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d &apos;&apos; -r ARG; do</span><br><span class="line">  CMD+=(&quot;$ARG&quot;)</span><br><span class="line">done &lt; &lt;(build_command &quot;$@&quot;)</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"># Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes</span><br><span class="line"># the code that parses the output of the launcher to get confused. In those cases, check if the</span><br><span class="line"># exit code is an integer, and if it&apos;s not, handle it as a special error case.</span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo &quot;$&#123;CMD[@]&#125;&quot; | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=(&quot;$&#123;CMD[@]:0:$LAST&#125;&quot;)</span><br><span class="line">exec &quot;$&#123;CMD[@]&#125;&quot;</span><br><span class="line">[double_happy@hadoop101 bin]$</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">. &quot;$&#123;SPARK_HOME&#125;&quot;/bin/load-spark-env.sh  这是在做什么？</span><br><span class="line">就是把环境里的scala版本找到</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">build_command() &#123;</span><br><span class="line">  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">  printf &quot;%d\0&quot; $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RUNNER=java 去看一下就知道</span><br></pre></td></tr></table></figure></div>
<p>整个spark-shell流程：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">spark-shell &#123;</span><br><span class="line"></span><br><span class="line">&quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.repl.Main \</span><br><span class="line"> --name &quot;Spark shell&quot; \</span><br><span class="line"> &quot;$@&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">==&gt; spark-submit&#123;</span><br><span class="line">	exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class \</span><br><span class="line">	 org.apache.spark.deploy.SparkSubmit \</span><br><span class="line">	 &quot;$@&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">==&gt;spark-class&#123;</span><br><span class="line">	build_command() &#123;</span><br><span class="line">	  &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;</span><br><span class="line">	  printf &quot;%d\0&quot; $?</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark-shell最底层就是使用Jave来启动 org.apache.spark.launcher.Main类</span><br><span class="line">REPL  ==》交互式解释器</span><br><span class="line"></span><br><span class="line">看org.apache.spark.launcher.Main 源码 需要这个依赖</span><br><span class="line"></span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-launcher_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line"> &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">idea ：</span><br><span class="line">control + shift +n </span><br><span class="line">control + shift +f</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025152410447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark008-补充Spark007" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/25/Spark008-%E8%A1%A5%E5%85%85Spark007/">Spark008--补充Spark007</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/25/Spark008-%E8%A1%A5%E5%85%85Spark007/" class="article-date">
  <time datetime="2018-01-25T12:03:56.000Z" itemprop="datePublished">2018-01-25</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><img src="https://img-blog.csdnimg.cn/20191024155059801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上次的结果输出的文本是这样的，客户说我需要压缩的格式呢？<br>生产上出来的数据非常非常大 肯定是需要压缩的<br>eg：5分钟数据量达到10多个G</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class</span><br><span class="line">   * supporting the key and value types K and V in this RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note We should make sure our tasks are idempotent when speculation is enabled, i.e. do</span><br><span class="line">   * not use output committer that writes data directly.</span><br><span class="line">   * There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad</span><br><span class="line">   * result of using direct output committer with speculation enabled.</span><br><span class="line">   */</span><br><span class="line">  def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      conf: JobConf = new JobConf(self.context.hadoopConfiguration),</span><br><span class="line">      codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">codec 就是指定压缩的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    /**</span><br><span class="line">      * Android</span><br><span class="line">      *   xxxx.log</span><br><span class="line">      *</span><br><span class="line">      * iOS</span><br><span class="line">      *   xxx.log</span><br><span class="line">      */</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],</span><br><span class="line">        classOf[RuozedataMultipleTextOutputFormat],</span><br><span class="line">        classOf[GzipCodec])      //这块加上压缩的格式</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024155642185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>学学底层的实现：</strong> 前面的文章好像有写 忘记了</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">1.saveAsHadoopFile</span><br><span class="line"> def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      codec: Class[_ &lt;: CompressionCodec]): Unit = self.withScope &#123;</span><br><span class="line">    saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">      new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2.点进去</span><br><span class="line"> saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">      new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line"></span><br><span class="line">3.</span><br><span class="line">  def saveAsHadoopFile(</span><br><span class="line">      path: String,</span><br><span class="line">      keyClass: Class[_],</span><br><span class="line">      valueClass: Class[_],</span><br><span class="line">      outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">      conf: JobConf = new JobConf(self.context.hadoopConfiguration),</span><br><span class="line">      codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit = self.withScope &#123;</span><br><span class="line">    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).</span><br><span class="line">    val hadoopConf = conf</span><br><span class="line">    hadoopConf.setOutputKeyClass(keyClass)</span><br><span class="line">    hadoopConf.setOutputValueClass(valueClass)</span><br><span class="line">    conf.setOutputFormat(outputFormatClass)</span><br><span class="line">    for (c &lt;- codec) &#123;</span><br><span class="line">      hadoopConf.setCompressMapOutput(true)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,</span><br><span class="line">        CompressionType.BLOCK.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Use configured output committer if already set</span><br><span class="line">    if (conf.getOutputCommitter == null) &#123;</span><br><span class="line">      hadoopConf.setOutputCommitter(classOf[FileOutputCommitter])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // When speculation is on and output committer class name contains &quot;Direct&quot;, we should warn</span><br><span class="line">    // users that they may loss data if they are using a direct output committer.</span><br><span class="line">    val speculationEnabled = self.conf.getBoolean(&quot;spark.speculation&quot;, false)</span><br><span class="line">    val outputCommitterClass = hadoopConf.get(&quot;mapred.output.committer.class&quot;, &quot;&quot;)</span><br><span class="line">    if (speculationEnabled &amp;&amp; outputCommitterClass.contains(&quot;Direct&quot;)) &#123;</span><br><span class="line">      val warningMessage =</span><br><span class="line">        s&quot;$outputCommitterClass may be an output committer that writes data directly to &quot; +</span><br><span class="line">          &quot;the final location. Because speculation is enabled, this output committer may &quot; +</span><br><span class="line">          &quot;cause data loss (see the case in SPARK-10063). If possible, please use an output &quot; +</span><br><span class="line">          &quot;committer that does not have this behavior (e.g. FileOutputCommitter).&quot;</span><br><span class="line">      logWarning(warningMessage)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    FileOutputFormat.setOutputPath(hadoopConf,</span><br><span class="line">      SparkHadoopWriterUtils.createPathFromString(path, hadoopConf))</span><br><span class="line">    saveAsHadoopDataset(hadoopConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4. 3里面关键</span><br><span class="line">   for (c &lt;- codec) &#123;</span><br><span class="line">      hadoopConf.setCompressMapOutput(true)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress&quot;, &quot;true&quot;)</span><br><span class="line">      hadoopConf.setMapOutputCompressorClass(c)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.codec&quot;, c.getCanonicalName)</span><br><span class="line">      hadoopConf.set(&quot;mapreduce.output.fileoutputformat.compress.type&quot;,</span><br><span class="line">        CompressionType.BLOCK.toString)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"> hadoopConf.setCompressMapOutput(true) 点进去看看</span><br><span class="line"></span><br><span class="line"> public void setCompressMapOutput(boolean compress) &#123;</span><br><span class="line">    setBoolean(JobContext.MAP_OUTPUT_COMPRESS, compress);</span><br><span class="line">  &#125;</span><br><span class="line">再点进去</span><br><span class="line"></span><br><span class="line">public static final String MAP_OUTPUT_COMPRESS = &quot;mapreduce.map.output.compress&quot;;</span><br><span class="line"></span><br><span class="line">mapreduce.map.output.compress 这个参数不就是设置map端的输出压缩</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec 这个参数 </span><br><span class="line">设置最终输出的压缩 前面的文章写过</span><br><span class="line"></span><br><span class="line">所以这底层的代码就是Mapreduce代码 **** 就是Spark给封装好的</span><br></pre></td></tr></table></figure></div>

<h2 id="补充知识点"><a href="#补充知识点" class="headerlink" title="补充知识点"></a>补充知识点</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">      *  map vs mapPartitions(优先选择)  transformation</span><br><span class="line">      *</span><br><span class="line">      *  RDD 1w个元素  需要你把这个RDD的元素写入到MySQL （前提没有使用数据库连接池那种）</span><br><span class="line">      * 1w个元素 我要写1w次MySQL</span><br><span class="line">      * </span><br><span class="line">      *  RDD 1w个元素  10分区   10次</span><br><span class="line">      *</span><br><span class="line">map vs mapPartitions ：</span><br><span class="line">这两个写入db哪个好？   优先级角度选 mapPartitions 是没有问题的 能解决80%生产上的操作</span><br><span class="line">分情况的 应该说都不好  你应该想说 mapPartitions好 但是</span><br><span class="line">当你数据量很大 分区数很少 那么一个分区里的数据量很大 写的时候 可能oom </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      *</span><br><span class="line">      *  foreach vs foreachPartition   action</span><br><span class="line">      *</span><br><span class="line">      * rdd==&gt; transformations ==&gt; action</span><br><span class="line">      *  真正生产上把RDD的数据写入到DB，是使用foreachPartition</span><br><span class="line">      */</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03ToMySQL &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_)</span><br><span class="line">        .sortBy(-_._2)</span><br><span class="line">        .foreachPartition(partition =&gt;&#123;</span><br><span class="line">          var connection : Connection = null</span><br><span class="line">          var pstmt:PreparedStatement = null</span><br><span class="line">          try&#123;</span><br><span class="line">            connection = MySQLUtils.getConnection()</span><br><span class="line">            val sql = &quot;insert into topn(domain,url,cnt) values (?,?,?)&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            //真正的数据是分区里的元素</span><br><span class="line">            partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.execute()  </span><br><span class="line">            &#125;)</span><br><span class="line">          &#125;catch &#123;</span><br><span class="line">            case  e:Exception =&gt; e.printStackTrace()</span><br><span class="line">          &#125;finally &#123;</span><br><span class="line">            MySQLUtils.closeResource(pstmt,connection)  //这块只关闭connection可以么? 可以的 java知识</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">object MySQLUtils &#123;</span><br><span class="line">  def getConnection():Connection = &#123;</span><br><span class="line">    Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    getConnection(&quot;hadoop101&quot;, &quot;3306&quot;, &quot;hive_dwd&quot;, &quot;root&quot;, &quot;wsx123$%^&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  def getConnection(host: String, port: String, database: String, user: String, password: String):Connection = &#123;</span><br><span class="line">    Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</span><br><span class="line">    DriverManager.getConnection(s&quot;jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/$&#123;database&#125;&quot;, user, password)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">    * 关闭资源</span><br><span class="line">    *</span><br><span class="line">    * @param resources 可变数组</span><br><span class="line">    */</span><br><span class="line">  def closeResource(resources: AutoCloseable*): Unit = &#123;</span><br><span class="line">    for (resource &lt;- resources) &#123;</span><br><span class="line">      resource.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from topn;</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| domain          | url   | cnt  |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| www.baidu.com   | url2  |    2 |</span><br><span class="line">| www.baidu.com   | url5  |    5 |</span><br><span class="line">| www.baidu.com   | url1  |    1 |</span><br><span class="line">| www.baidu.com   | url4  |    4 |</span><br><span class="line">| www.baidu.com   | url3  |    3 |</span><br><span class="line">| www.twitter.com | url10 |   11 |</span><br><span class="line">| www.twitter.com | url6  |    1 |</span><br><span class="line">| www.twitter.com | url9  |    6 |</span><br><span class="line">| www.google.com  | url2  |    2 |</span><br><span class="line">| www.google.com  | url6  |    7 |</span><br><span class="line">| www.google.com  | url1  |    1 |</span><br><span class="line">| www.google.com  | url8  |    7 |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">能使用scalikejdbc 把数据写入MySQL更好哈   前面scala篇有讲</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">问题指出：</span><br><span class="line">  partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.execute()  </span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">如果你一个partition 一个元素执行一次  里有1w个元素呢？   性能不好</span><br><span class="line"></span><br><span class="line">肯定是要批处理的   一个批次给它搞一个事务 把自动提交给关掉</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03ToMySQL &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_)</span><br><span class="line">        .sortBy(-_._2)</span><br><span class="line">        .foreachPartition(partition =&gt;&#123;</span><br><span class="line">          var connection : Connection = null</span><br><span class="line">          var pstmt:PreparedStatement = null</span><br><span class="line">          try&#123;</span><br><span class="line">            connection = MySQLUtils.getConnection()</span><br><span class="line">            connection.setAutoCommit(false)</span><br><span class="line"></span><br><span class="line">     /*       //先把之前的数据删掉   </span><br><span class="line">            val sqlDelete = s&quot;delete from topn where access_time = $&#123;args(0)&#125;&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sqlDelete)</span><br><span class="line">            pstmt.execute()*/</span><br><span class="line"></span><br><span class="line">            //再插入数据</span><br><span class="line">            val sql = &quot;insert into topn(domain,url,cnt) values (?,?,?)&quot;</span><br><span class="line">            pstmt = connection.prepareStatement(sql)</span><br><span class="line">            //真正的数据是分区里的元素</span><br><span class="line">            partition.foreach(x =&gt;&#123;</span><br><span class="line">              pstmt.setString(1,x._1._1)</span><br><span class="line">              pstmt.setString(2,x._1._2)</span><br><span class="line">              pstmt.setInt(3,x._2)</span><br><span class="line">              pstmt.addBatch()</span><br><span class="line">            &#125;)</span><br><span class="line">            pstmt.executeBatch()   //执行批次</span><br><span class="line">            connection.commit()  //提交事务</span><br><span class="line">          &#125;catch &#123;</span><br><span class="line">            case  e:Exception =&gt; e.printStackTrace()</span><br><span class="line">          &#125;finally &#123;</span><br><span class="line">            MySQLUtils.closeResource(pstmt,connection)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; truncate table topn;       //代码里加上删除之前的数据 可以解决 最好别truncate 只是学习时方便</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from topn; </span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| domain          | url   | cnt  |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">| www.baidu.com   | url5  |    5 |</span><br><span class="line">| www.baidu.com   | url2  |    2 |</span><br><span class="line">| www.baidu.com   | url4  |    4 |</span><br><span class="line">| www.baidu.com   | url1  |    1 |</span><br><span class="line">| www.baidu.com   | url3  |    3 |</span><br><span class="line">| www.twitter.com | url6  |    1 |</span><br><span class="line">| www.twitter.com | url10 |   11 |</span><br><span class="line">| www.twitter.com | url9  |    6 |</span><br><span class="line">| www.google.com  | url2  |    2 |</span><br><span class="line">| www.google.com  | url6  |    7 |</span><br><span class="line">| www.google.com  | url1  |    1 |</span><br><span class="line">| www.google.com  | url8  |    7 |</span><br><span class="line">+-----------------+-------+------+</span><br><span class="line">12 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></div>
<h2 id="Submitting-Applications"><a href="#Submitting-Applications" class="headerlink" title="Submitting Applications"></a>Submitting Applications</h2><p>工作当中是再idea里开发的 在生产上是在Submitting Applications</p>
<p><img src="https://img-blog.csdnimg.cn/20191024192531620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.Spark-shell 底层调用的是Spark-submit</span><br><span class="line"></span><br><span class="line">Spark-submit怎么使用呢？</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications" target="_blank" rel="noopener">Submitting Applications</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">idea里面：</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">        val conf = new SparkConf()       //提交到集群上的时候 setAppName 和setMater全都去掉</span><br><span class="line">        val sc = new SparkContext(conf)</span><br><span class="line">        val input = sc.textFile(args(0))</span><br><span class="line">        val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">          val splits = x.split(&quot;,&quot;)</span><br><span class="line">          val site = splits(0)</span><br><span class="line">          val url = splits(1)</span><br><span class="line">          ((site, url), 1)</span><br><span class="line">        &#125;).reduceByKey(_+_).saveAsTextFile(args(1))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">打包提交到集群上去</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 bin]$ ./spark-submit --help</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (&quot;client&quot;) or</span><br><span class="line">                              on one of the worker machines inside the cluster (&quot;cluster&quot;)</span><br><span class="line">                              (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application&apos;s main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place</span><br><span class="line">                              on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit.</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  Print the version of current Spark.</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster mode</span><br><span class="line">                              (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,</span><br><span class="line">                              or all available cores on the worker in standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: &quot;default&quot;).</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">                              If dynamic allocation is enabled, the initial number of</span><br><span class="line">                              executors will be at least NUM.</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above. This keytab will be copied to</span><br><span class="line">                              the node running the Application Master via the Secure</span><br><span class="line">                              Distributed Cache, for renewing the login tickets and the</span><br><span class="line">                              delegation tokens periodically.</span><br><span class="line">      </span><br><span class="line">[double_happy@hadoop101 bin]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[]  可选  &lt;&gt; 必选</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark-submit  \</span><br><span class="line">--name InterviewApp03 \</span><br><span class="line">--class com.ruozedata.spark.spark05.InterviewApp03 \</span><br><span class="line">--master local[2] \</span><br><span class="line">/home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">/data_spark/input/  /data_spark/output </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">抛出一个问题 我的路径前面没有添加 hdfs:xxx:8020/路径  为什么我的不用加 ？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024200131809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 lib]$ hadoop fs -text /data_spark/output/par*</span><br><span class="line">19/10/24 20:05:35 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">19/10/24 20:05:35 INFO compress.CodecPool: Got brand-new decompressor [.bz2]</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line">((www.baidu.com,url1),1)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line">((www.google.com,url1),1)</span><br><span class="line">((www.baidu.com,url3),3)</span><br><span class="line">((www.google.com,url2),2)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url6),1)</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url2),2)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">[double_happy@hadoop101 lib]$</span><br></pre></td></tr></table></figure></div>
<p>If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a Spark cluster. <strong>To do this, create an assembly jar (or “uber” jar) containing your code and its dependencies.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">你的代码 依赖hadoop spark 但是这些集群本身 就有 打包的时候 要的是瘦包 如果需要第三方的包</span><br><span class="line">可以使用  --jars  或者 package到application 里 </span><br><span class="line"></span><br><span class="line">个人不建议使用assembly 打包</span><br><span class="line"></span><br><span class="line">到ss 打包再讨论</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line"></span><br><span class="line">第三方包 加入这个命令</span><br></pre></td></tr></table></figure></div>
<h2 id="Loading-Configuration-from-a-File"><a href="#Loading-Configuration-from-a-File" class="headerlink" title="Loading Configuration from a File"></a>Loading Configuration from a File</h2><p>The <strong>spark-submit script</strong> can load default Spark configuration values from a properties file and pass them on to your application. By default, it will <strong>read options from conf/spark-defaults.conf in the Spark directory.</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">spark-submit 默认会加载 conf/spark-defaults.conf   in the Spark directory </span><br><span class="line"></span><br><span class="line">当然也可以人为指定 ：</span><br><span class="line"> --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"># spark.master                     spark://master:7077</span><br><span class="line"># spark.eventLog.enabled           true</span><br><span class="line"># spark.eventLog.dir               hdfs://namenode:8021/directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br><span class="line">[double_happy@hadoop101 conf]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">带来的好处是什么？</span><br><span class="line">默认情况下 你每次开启spark-shell  --master local[2]</span><br><span class="line"> 每次都要手动加参数 </span><br><span class="line"> 可以再 spark-defalut.xml 里加上即可。</span><br><span class="line"></span><br><span class="line">如果你的业务线非常多 你就多写几个spark-defalut.xml  通过 --files 传过去你想要的模式</span><br></pre></td></tr></table></figure></div>
<p>以上是最基本的操作</p>
<p>它默认走的是 spark-defalut.xml 那么底层的实现一定是走的默认参数的</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>上面跑的程序<br><img src="https://img-blog.csdnimg.cn/20191024203040986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">成功完之后 SC被干掉了 UI上面还能看到么？</span><br><span class="line">看不见了 如果半夜三点程序挂了 或者在调优的场景下  你程序跑完 UI就没了呀 该怎么解决呢？</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/monitoring.html#web-interfaces" target="_blank" rel="noopener">Monitoring</a></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;c&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;d&quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((d,1), (b,2), (a,2), (c,2))                  </span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024203854192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:<br>        A list of scheduler stages and tasks<br>        A summary of RDD sizes and memory usage<br>        Environmental information.<br>        Information about the running executors</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.页面上展示 有多少个job （就是代码里有多少个action）</span><br><span class="line">2.stages ---&gt;一个 job里面  有多少个stage    点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204201773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.一个stage有几个task呢？点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204355319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Information about the running executors：</span><br><span class="line">关于你运行的executors信息 </span><br><span class="line">1.你运行这个程序 设置了多少个executors 活的有多少 死的有多少</span><br><span class="line">2.对调优很重要 </span><br><span class="line">你肯定要看 ：</span><br><span class="line">   1.shuffle的数据量有多少 </span><br><span class="line">   2.经历多少算子</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024204640805.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这副图运行时间慢的原因是 ：<br>emmm 总有贪小便宜的人 去我云服务器上挖矿 悄踏玛真的很烦 因为我把端口全部开放了</p>
<p> If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc).</p>
<p>这句话挺重要的 因为之前在公司里 我喜欢用 yarn client模式 而我提交的任务比较多<br>达到特别多的时候 再提交任务 是排不上的哈 提交不上的 </p>
<p><strong>Note that this information is only available for the duration of the application by default</strong>. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Note that this information is only available for the duration of the application by default</span><br><span class="line"></span><br><span class="line">1.默认情况下 这个页面只能 在 application 生命周期内有效  所以运行完 sc.stop 之后</span><br><span class="line">你的UI界面就看不见了 </span><br><span class="line">2.spark.eventLog.enabled 这个参数设置为true  开启spark日志 当你再打开ui界面 可以看的到 运行完的application</span><br></pre></td></tr></table></figure></div>
<p>那么怎么构建 总结的UI呢？<br><img src="https://img-blog.csdnimg.cn/20191024224445859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.spark.eventLog.enabled true   开关打开</span><br><span class="line">2.spark.eventLog.dir hdfs://namenode/shared/spark-logs    </span><br><span class="line">记录spark运行过程中日志记录在这个目录  这是spark作业触发的</span><br><span class="line"></span><br><span class="line">3.使用start-history-server.sh 展示记录的spark日志  也需要一个目录  </span><br><span class="line">spark.history.fs.logDirectory 用这个参数 </span><br><span class="line">这个参数 是配置spark-env.sh里</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">这个跟配置压缩一样 打开开关 + 指定codec</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 conf]$ cat spark-defaults.conf</span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Default system properties included when running spark-submit.</span><br><span class="line"># This is useful for setting default environmental settings.</span><br><span class="line"></span><br><span class="line"># Example:</span><br><span class="line"> spark.master                     local[2]</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"> spark.eventLog.dir               hdfs://hadoop101:8020/spark_directory</span><br><span class="line"># spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"># spark.driver.memory              5g</span><br><span class="line"># spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br></pre></td></tr></table></figure></div>
<p><a href="http://spark.apache.org/docs/latest/monitoring.html#viewing-after-the-fact" target="_blank" rel="noopener">参数配置</a></p>
<p>This creates a web interface at http://<server-url>:18080 by default, listing <strong>incomplete</strong> and <strong>completed</strong> applications and attempts.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.completed 和 incomplete spark怎么区别呢 需要一个刷新时间 </span><br><span class="line">用这个参数 spark.history.fs.update.interval</span><br><span class="line"></span><br><span class="line">2.如果配置成功之后日志日积月累多了 数据量会很大 所以需要定期删除的</span><br><span class="line">spark.history.fs.cleaner.enabled     是否需要清理呢</span><br><span class="line">spark.history.fs.cleaner.interval         清理周期是多少</span><br><span class="line">spark.history.fs.cleaner.maxAge        一次清理几天的</span><br></pre></td></tr></table></figure></div>
<p>配置一下 ：<br><img src="https://img-blog.csdnimg.cn/20191024230341733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>启动：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.hdfs 上创建 log 文件夹</span><br><span class="line">2.启动./sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">测试查看是否成功</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ ./start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.deploy.history.HistoryServer-1-hadoop101.out</span><br><span class="line">[double_happy@hadoop101 sbin]$ tail -200f /home/double_happy/app/spark/logs/spark-double_happy-org.apache.spark.deploy.history.HistoryServer-1-hadoop101.out</span><br><span class="line">Spark Command: /usr/java/java/bin/java -cp /home/double_happy/app/spark/conf/:/home/double_happy/app/spark/jars/*:/home/double_happy/app/hadoop/etc/hadoop/ -Dspark.history.fs.logDirectory=hdfs://hadoop101:8020/spark_directory -Xmx1g org.apache.spark.deploy.history.HistoryServer</span><br><span class="line">========================================</span><br><span class="line">19/10/24 23:06:35 INFO HistoryServer: Started daemon with process name: 6633@hadoop101</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for TERM</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for HUP</span><br><span class="line">19/10/24 23:06:35 INFO SignalUtils: Registered signal handler for INT</span><br><span class="line">19/10/24 23:06:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing view acls to: double_happy</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing modify acls to: double_happy</span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing view acls groups to: </span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: Changing modify acls groups to: </span><br><span class="line">19/10/24 23:06:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(double_happy); groups with view permissions: Set(); users  with modify permissions: Set(double_happy); groups with modify permissions: Set()</span><br><span class="line">19/10/24 23:06:35 INFO FsHistoryProvider: History server ui acls disabled; users with admin permissions: ; groups with admin permissions</span><br><span class="line">19/10/24 23:06:37 INFO Utils: Successfully started service on port 18080.</span><br><span class="line">19/10/24 23:06:37 INFO HistoryServer: Bound HistoryServer to 0.0.0.0, and started at http://hadoop101:18080</span><br></pre></td></tr></table></figure></div>
<p>说明启动ok </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">测试：</span><br><span class="line">1.spark-shell 运行了一个东西</span><br><span class="line">2.spark-submit 提交了两次</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.</span><br><span class="line">scala&gt;  sc.parallelize(List(&quot;a&quot;,&quot;c&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;d&quot;)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((d,1), (b,2), (a,2), (c,2))</span><br><span class="line"></span><br><span class="line">scala&gt; sc.stop</span><br><span class="line"></span><br><span class="line">2.</span><br><span class="line">spark-submit  \</span><br><span class="line">&gt; --name InterviewApp03 \</span><br><span class="line">&gt; --class com.ruozedata.spark.spark05.InterviewApp03 \</span><br><span class="line">&gt; --master local[2] \</span><br><span class="line">&gt; /home/double_happy/lib/spark-core-1.0.jar \</span><br><span class="line">&gt; /data_spark/input/  /data_spark/output</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024231709105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Note that in all of these UIs, the tables are sortable by <strong>clicking their headers,</strong> making it easy to <strong>identify slow tasks, data skew, etc</strong>.</p>
<p>Note<br>1.<strong>The history server displays both completed and incomplete Spark jobs.</strong> If an application makes multiple attempts after failures, the failed attempts will be displayed, as well as any ongoing incomplete attempt or the final successful attempt.</p>
<p>2.Incomplete applications are only updated intermittently. The time between updates is defined by the interval between checks for changed files (spark.history.fs.update.interval). On larger clusters, the update interval may be set to large values. The way to view a running application is actually to view its own web UI.</p>
<p>3.Applications which exited without registering themselves as completed will be listed as incomplete —even though they are no longer running. This can happen if an application crashes.</p>
<p><strong>4.One way to signal the completion of a Spark job is to stop the Spark Context explicitly (sc.stop()), or in Python using the with SparkContext() as sc: construct to handle the Spark Context setup and tear down.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4.就是你代码里 sc.stop() 写了  程序完成后会显示在 completed 里面 如果不写会显示在incomplete </span><br><span class="line">所以 为了 好区分正在运行的作业还是 完成的作业 sc.stop() 要加上的</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191025111143777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191025111221278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.本地运行的作业 全部以 local开头的 </span><br><span class="line">2.ui上显示很多信息   点进去</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102423185850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>这个页面不就回来了么 程序已经运行完了   测试成功。</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Download 之后就是个Json文件 也可以去HDFS上去看我配置的log目录 也可以下载的</span><br><span class="line">整个历史页面 就是靠 Json文件 来渲染的</span><br><span class="line"></span><br><span class="line">所以这个东西有了 spark调优就方便了很多</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024232531503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="start-history-server-sh"><a href="#start-history-server-sh" class="headerlink" title="start-history-server.sh"></a>start-history-server.sh</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[double_happy@hadoop101 sbin]$ cat start-history-server.sh </span><br><span class="line">#!/usr/bin/env bash</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"># Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line"># contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line"># this work for additional information regarding copyright ownership.</span><br><span class="line"># The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line"># (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line"># the License.  You may obtain a copy of the License at</span><br><span class="line">#</span><br><span class="line">#    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line">#</span><br><span class="line"># Unless required by applicable law or agreed to in writing, software</span><br><span class="line"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"># See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># Starts the history server on the machine this script is executed on.</span><br><span class="line">#</span><br><span class="line"># Usage: start-history-server.sh</span><br><span class="line">#</span><br><span class="line"># Use the SPARK_HISTORY_OPTS environment variable to set history server configuration.</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;/sbin/spark-config.sh&quot;</span><br><span class="line">. &quot;$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh&quot;</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;/sbin&quot;/spark-daemon.sh start org.apache.spark.deploy.history.HistoryServer 1 &quot;$@&quot;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">去idea里找到 HistoryServer类：</span><br><span class="line">1.这个类一定有main方法</span><br><span class="line"></span><br><span class="line"> def main(argStrings: Array[String]): Unit = &#123;</span><br><span class="line">    Utils.initDaemon(log)</span><br><span class="line">    new HistoryServerArguments(conf, argStrings)</span><br><span class="line">    initSecurity()</span><br><span class="line">    val securityManager = createSecurityManager(conf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. new HistoryServerArguments(conf, argStrings)  点进去 </span><br><span class="line">里面的  解析</span><br><span class="line">// This mutates the SparkConf, so all accesses to it must be made after this line</span><br><span class="line">   Utils.loadDefaultSparkProperties(conf, propertiesFile)</span><br><span class="line"></span><br><span class="line">3.def loadDefaultSparkProperties(conf: SparkConf, filePath: String = null): String = &#123;</span><br><span class="line">    val path = Option(filePath).getOrElse(getDefaultPropertiesFile())</span><br><span class="line">    Option(path).foreach &#123; confFile =&gt;</span><br><span class="line">      getPropertiesFromFile(confFile).filter &#123; case (k, v) =&gt;</span><br><span class="line">        k.startsWith(&quot;spark.&quot;)</span><br><span class="line">      &#125;.foreach &#123; case (k, v) =&gt;</span><br><span class="line">        conf.setIfMissing(k, v)</span><br><span class="line">        sys.props.getOrElseUpdate(k, v)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    path</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">4.getDefaultPropertiesFile</span><br><span class="line"></span><br><span class="line">def getDefaultPropertiesFile(env: Map[String, String] = sys.env): String = &#123;</span><br><span class="line">    env.get(&quot;SPARK_CONF_DIR&quot;)</span><br><span class="line">      .orElse(env.get(&quot;SPARK_HOME&quot;).map &#123; t =&gt; s&quot;$t$&#123;File.separator&#125;conf&quot; &#125;)</span><br><span class="line">      .map &#123; t =&gt; new File(s&quot;$t$&#123;File.separator&#125;spark-defaults.conf&quot;)&#125;</span><br><span class="line">      .filter(_.isFile)</span><br><span class="line">      .map(_.getAbsolutePath)</span><br><span class="line">      .orNull</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">能知道 加载 spark-defaults.conf 文件 明白了吗 </span><br><span class="line">详细脚本实现 自己看源码</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024233636507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">源码里有写：</span><br><span class="line">spark.history.retainedApplications   ： 默认50个</span><br><span class="line"></span><br><span class="line">The number of applications to retain UI data for in the cache. If this cap is exceeded, then the oldest applications will be removed from the cache. If an application is not in the cache, it will have to be loaded from disk if it is accessed from the UI.</span><br><span class="line"></span><br><span class="line">1.The number of applications to retain UI data for in the cache</span><br><span class="line"> 是在内存中的</span><br><span class="line"> 这个参数 不是 ui上面只能展示50个意思哈   是内存里只放50个 超过了 removed from the cache </span><br><span class="line"> 2. 看解释 很清楚</span><br></pre></td></tr></table></figure></div>
<h2 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h2><p><img src="https://img-blog.csdnimg.cn/2019102510374444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Shared-Variables"><a href="#Shared-Variables" class="headerlink" title="Shared Variables"></a>Shared Variables</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables" target="_blank" rel="noopener">Shared Variables</a><br>Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a remote cluster node, it works on separate copies of all the variables used in the function. <strong>These variables are copied to each machine</strong>, and <strong>no updates</strong> to the variables on the remote machine are propagated back to the driver program. Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: <strong>broadcast variables and accumulators.</strong></p>
<p>Accumulators are variables that are <strong>only “added”</strong> to through an associative and commutative operation and can therefore <strong>be efficiently supported in parallel.</strong><br>Spark natively supports accumulators of <strong>numeric types</strong>, and <strong>programmers can add support for new types.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accumulators：</span><br><span class="line">使用场景：ETl处理的时候 把正确的条数 和总的条数  统计出来 </span><br><span class="line">1.原生的支持数值类型 ，开发者开发可以支持别的类型</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val accum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res1: Long = 10</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102510514440.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">现在的计数器都是AccumulatorV2 版本  官网上写了如何自定义累加器 生产上我没用用过</span><br></pre></td></tr></table></figure></div>
<p>案例：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    var cnts = 0</span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,7,8),3)</span><br><span class="line">    data.foreach(x =&gt; &#123;</span><br><span class="line">      cnts += 1</span><br><span class="line">      println(s&quot;cnts:$cnts&quot;)</span><br><span class="line">    &#125;)</span><br><span class="line">    println(cnts+&quot;~~~~~~~&quot;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">cnts:1</span><br><span class="line">cnts:1</span><br><span class="line">cnts:2</span><br><span class="line">cnts:2</span><br><span class="line">cnts:3</span><br><span class="line">cnts:1</span><br><span class="line">cnts:2</span><br><span class="line">cnts:3</span><br><span class="line">0~~~~~~~</span><br><span class="line"></span><br><span class="line">这个结果什么意思？是想要的结果么？</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	上面这个 不是并行的 也就是 多个分区 是无法计算的 </span><br><span class="line">需要使用计数器 </span><br><span class="line">   计数器一定是在action算子之后使用   一定是要触发action的要不然拿不到结果 </span><br><span class="line">那么触发多个action可以么？</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">broadcast variables ：</span><br><span class="line">类似的</span><br><span class="line">1.前面解析ip库的时候，使用的mapreduce的分布式缓存 </span><br><span class="line">2.在sql里的 </span><br><span class="line"></span><br><span class="line">a join b on a.id=b.id ==&gt; shuffle  (普通的join 必然有shuffle的)  </span><br><span class="line">会按照join的条件 作为key(就是id)，其他的值作为value </span><br><span class="line"> 经过shuffle到reduce端 把相同的key聚在一块 来做的</span><br><span class="line"></span><br><span class="line">大数据集和小数据集join ==&gt;采用 mapjoin</span><br><span class="line"> 小表放到缓存中，不会有真正的join发生，底层其实就是一个匹配  匹配上拿出来 匹配不上就滚蛋的</span><br><span class="line"> </span><br><span class="line"> 那么 上面的这些可以使用 broadcast variables 来实现</span><br></pre></td></tr></table></figure></div>
<p>Broadcast variables allow the programmer to keep a read-only variable <strong>cached on each machine rather than shipping a copy of it with tasks.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">eg:这个代码  </span><br><span class="line">val xx = new HashMap() // 10M</span><br><span class="line">rdd.map(x=&gt;&#123;</span><br><span class="line">    ....xx</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">如果有一万个task  每个task里 额外的  1w*10M</span><br><span class="line"></span><br><span class="line">Broadcast variables cached on each machine （理解为executor就可以）而不是 cp到tasks</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">object RDDOperationApp02 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    //广播的时候 要使用 kv的 最佳实践</span><br><span class="line">    val rdd1 = sc.parallelize(Array((&quot;23&quot;,&quot;smart&quot;),(&quot;9&quot;,&quot;愤怒的麻雀&quot;))).collectAsMap()</span><br><span class="line">    val rdd2 = sc.parallelize(Array((&quot;23&quot;,&quot;郑州&quot;),(&quot;9&quot;,&quot;蜀国&quot;),(&quot;14&quot;,&quot;魔都&quot;)))</span><br><span class="line">    val rdd1_bc = sc.broadcast(rdd1)</span><br><span class="line">    rdd2.map(x=&gt;(x._1,x)).mapPartitions(x =&gt; &#123;</span><br><span class="line">      val bc_value = rdd1_bc.value</span><br><span class="line">      for((k,v)&lt;- x if(bc_value.contains(k)))</span><br><span class="line">        yield (k, bc_value.get(k).getOrElse(&quot;&quot;), v._2)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(9,愤怒的麻雀,蜀国)</span><br><span class="line">(23,smart,郑州)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Scala中的yield的主要作用是记住每次迭代中的有关值，并逐一存入到一个数组中。</span><br><span class="line">要将结果存放到数组的变量或表达式必须放在yield&#123;&#125;里最后位置</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(Array((&quot;23&quot;,&quot;smart&quot;),(&quot;9&quot;,&quot;愤怒的麻雀&quot;))).collectAsMap()</span><br><span class="line">rdd1: scala.collection.Map[String,String] = Map(23 -&gt; smart, 9 -&gt; 愤怒的麻雀)</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(Array((&quot;23&quot;,&quot;郑州&quot;),(&quot;9&quot;,&quot;蜀国&quot;),(&quot;14&quot;,&quot;魔都&quot;)))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1_bc = sc.broadcast(rdd1)</span><br><span class="line">rdd1_bc: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,String]] = Broadcast(3)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.map(x=&gt;(x._1,x)).mapPartitions(x =&gt; &#123;</span><br><span class="line">     |   val bc_value = rdd1_bc.value</span><br><span class="line">     |   for((k,v)&lt;- x if(bc_value.contains(k)))</span><br><span class="line">     |     yield (k, bc_value.get(k).getOrElse(&quot;&quot;), v._2)</span><br><span class="line">     | &#125;).foreach(println)</span><br><span class="line">(23,smart,郑州)</span><br><span class="line">(9,愤怒的麻雀,蜀国)</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">查看页面 是没有shuffle的 没有join的 就是mapjoin</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191025114937388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>sparkcore之后sparksql 以及sparkstreaming 、sss、spark调优  的重要的文章是进行私密的 我写博客的目的是为了做笔记 为了学习</p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark007-综合案例" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/24/Spark007-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/">Spark007--综合案例</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/24/Spark007-%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2018-01-24T12:03:00.000Z" itemprop="datePublished">2018-01-24</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.数据如下</span><br><span class="line">    a,1,3</span><br><span class="line">    a,2,4</span><br><span class="line">    b,1,1</span><br><span class="line">根据第一列统计出</span><br><span class="line">    a,3,7</span><br><span class="line">    b,1,1</span><br><span class="line">用RDD实现</span><br><span class="line"></span><br><span class="line">分析：</span><br><span class="line">1）使用逗号对数据进行拆分 (a,&lt;1,3&gt;)  a=_.1  &lt;1,3&gt;=_.2</span><br><span class="line">2）reduceByKey((a,b)=&gt;a+b)   =&gt;  _.2._1 + _.2._2</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark04</span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object InterviewApp01 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.parallelize(List(</span><br><span class="line">      List(&quot;a&quot;,1,3),</span><br><span class="line">      List(&quot;a&quot;,2,4),</span><br><span class="line">      List(&quot;b&quot;,1,1)</span><br><span class="line">    ))</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val key = x(0).toString</span><br><span class="line">      val v1 = x(1).toString.toInt</span><br><span class="line">      val v2 = x(2).toString.toInt</span><br><span class="line">      (key, (v1,v2))</span><br><span class="line">    &#125;).reduceByKey((x,y)=&gt;&#123;</span><br><span class="line">      (x._1 + y._1, x._2+y._2)</span><br><span class="line">    &#125;).map(x=&gt;List(x._1, x._2._1,x._2._2)).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">List(a, 3, 7)</span><br><span class="line">List(b, 1, 1)</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2.广告投放 收费标准：</span><br><span class="line">	看到就收费</span><br><span class="line">	点击 才收费</span><br><span class="line">eg：</span><br><span class="line">&quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;    </span><br><span class="line"> uid,导航,1,0       （1表示看到了 0表示没有点进去）</span><br><span class="line"></span><br><span class="line">需求1：人和“一个东西”的展示量以及点击量</span><br><span class="line">eg：1000000 一起看 2 1</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">使用reduceBykey和groupBykey都实现一下：</span><br><span class="line"></span><br><span class="line">package com.ruozedata.spark.spark04</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object InterviewApp02 &#123;</span><br><span class="line">    def main(args: Array[String]): Unit = &#123;</span><br><span class="line">      val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">      val input = sc.parallelize(List(</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,1,0 1表示看到了 0表示没有点进去</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">        * 需求：人和“一个东西”的展示量以及点击量</span><br><span class="line">        * 1）组合key：人和所谓的一个东西</span><br><span class="line">        *</span><br><span class="line">        * 1000000 一起看 2 1</span><br><span class="line">        */</span><br><span class="line">      val processRDD = input.flatMap(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;,&quot;)</span><br><span class="line">        val id = splits(0).toInt</span><br><span class="line">        val word = splits(1)</span><br><span class="line">        val show = splits(2).toInt</span><br><span class="line">        val clicks = splits(3).toInt</span><br><span class="line"></span><br><span class="line">        val words = word.split(&quot;\\|&quot;)</span><br><span class="line">        words.map(x =&gt; ((id, x), (show, clicks)))</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">        * 在每个task/partition按照key先进行一个本地的聚合mapSideCombine: Boolean = true</span><br><span class="line">        * 预聚合之后，在每个task之上对于相同key的数据只有一条</span><br><span class="line">        *</span><br><span class="line">        *</span><br><span class="line">        * 调优 前 vs 后</span><br><span class="line">        * 是否能按照调优之前和调优之后作业的执行时间来对比?</span><br><span class="line">        * 时间之外还有其他的：读进来多少数据，shuffle出去多少数据，shuffle读写花费多少时间</span><br><span class="line">        * </span><br><span class="line">        * 所以优先选择reduceByKey </span><br><span class="line">        * 	1.shuffle数据少</span><br><span class="line">        * 去4040页面查看就知道了</span><br><span class="line">        */</span><br><span class="line">      processRDD.reduceByKey((x,y)=&gt;(x._1+y._1, x._2+y._2)).printInfo()</span><br><span class="line"></span><br><span class="line">      // 数据全部进行shuffle操作</span><br><span class="line">      processRDD</span><br><span class="line">        .groupByKey().mapValues(x=&gt;&#123;</span><br><span class="line">        val totalShows = x.map(_._1).sum</span><br><span class="line">        val totalClicks = x.map(_._2).sum</span><br><span class="line">        (totalShows, totalClicks)</span><br><span class="line">      &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">结果是：</span><br><span class="line">((1000000,军旅),(10,5))</span><br><span class="line">((1000000,一起看),(10,5))</span><br><span class="line">((1000000,士兵突击),(10,5))</span><br><span class="line">((1000001,电视剧),(5,5))</span><br><span class="line">((1000001,我的团长我的团),(5,5))</span><br><span class="line">((1000001,一起看),(5,5))</span><br><span class="line">((1000001,军旅),(5,5))</span><br><span class="line">((1000000,电视剧),(10,5))</span><br><span class="line">-------------------------</span><br><span class="line">((1000000,一起看),(10,5))</span><br><span class="line">((1000000,军旅),(10,5))</span><br><span class="line">((1000001,电视剧),(5,5))</span><br><span class="line">((1000000,士兵突击),(10,5))</span><br><span class="line">((1000001,军旅),(5,5))</span><br><span class="line">((1000001,我的团长我的团),(5,5))</span><br><span class="line">((1000001,一起看),(5,5))</span><br><span class="line">((1000000,电视剧),(10,5))</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191023170856895.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023171046603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>一定要注意数据结构</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp02 &#123;</span><br><span class="line">    def main(args: Array[String]): Unit = &#123;</span><br><span class="line">      val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">      val input = sc.parallelize(List(</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,1,0 1表示看到了 0表示没有点进去</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,0&quot;, // uid,导航,</span><br><span class="line">        &quot;1000000,一起看|电视剧|军旅|士兵突击,1,1&quot;,</span><br><span class="line">        &quot;1000001,一起看|电视剧|军旅|我的团长我的团,1,1&quot;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">      /**</span><br><span class="line">        * 需求：人和“一个东西”的展示量以及点击量</span><br><span class="line">        * 1）组合key：人和所谓的一个东西</span><br><span class="line">        *</span><br><span class="line">        * 1000000 一起看 2 1</span><br><span class="line">        * flatMap 注意 为什么不用map *** </span><br><span class="line">        */</span><br><span class="line">      val processRDD = input.flatMap(x =&gt; &#123;</span><br><span class="line">        val splits = x.split(&quot;,&quot;)</span><br><span class="line">        val id = splits(0).toInt</span><br><span class="line">        val word = splits(1)</span><br><span class="line">        val show = splits(2).toInt</span><br><span class="line">        val clicks = splits(3).toInt</span><br><span class="line"></span><br><span class="line">        val words = word.split(&quot;\\|&quot;)</span><br><span class="line">        words.map(x =&gt; ((id, x), (show, clicks)))</span><br><span class="line">      &#125;)</span><br><span class="line">      // 数据全部进行shuffle操作</span><br><span class="line">      processRDD</span><br><span class="line">        .groupByKey().printInfo()</span><br><span class="line">      sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((1000000,一起看),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000000,军旅),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000001,电视剧),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line">((1000000,士兵突击),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000001,军旅),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line">((1000001,我的团长我的团),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line">((1000000,电视剧),CompactBuffer((1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1), (1,0), (1,1)))</span><br><span class="line">((1000001,一起看),CompactBuffer((1,1), (1,1), (1,1), (1,1), (1,1)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">groupByKey 没有预聚合 还记得wc那张图么 上一篇的。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">3.  分组排序/组内排序</span><br><span class="line">  求每个域名访问量最大的url的Top N</span><br><span class="line"></span><br><span class="line">数据格式：</span><br><span class="line">www.baidu.com,url1</span><br><span class="line">www.baidu.com,url2</span><br><span class="line">www.baidu.com,url2</span><br><span class="line">www.baidu.com,url3</span><br><span class="line">www.baidu.com,url3</span><br><span class="line">www.baidu.com,url3</span><br><span class="line">www.baidu.com,url4</span><br></pre></td></tr></table></figure></div>
<p>我们一步步来：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    processRDD.printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((www.google.com,url6),1)</span><br><span class="line">((www.baidu.com,url1),1)</span><br><span class="line">((www.baidu.com,url2),1)</span><br><span class="line">((www.google.com,url6),1)</span><br><span class="line">((www.baidu.com,url2),1)</span><br><span class="line">((www.google.com,url2),1)</span><br><span class="line">。。。。。</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>分组 ： 如何分组？ 不分组行不行？</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">        processRDD.reduceByKey(_+_)</span><br><span class="line">          .groupBy(_._1._1)</span><br><span class="line">          .printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(www.baidu.com,CompactBuffer(</span><br><span class="line">((www.baidu.com,url1),1), </span><br><span class="line">((www.baidu.com,url3),3),</span><br><span class="line"> ((www.baidu.com,url5),5),</span><br><span class="line">  ((www.baidu.com,url2),2),</span><br><span class="line">   ((www.baidu.com,url4),4)))</span><br><span class="line">   </span><br><span class="line">(www.twitter.com,CompactBuffer(((www.twitter.com,url9),6), ((www.twitter.com,url10),11), ((www.twitter.com,url6),1)))</span><br><span class="line">(www.google.com,CompactBuffer(((www.google.com,url6),7), ((www.google.com,url8),7), ((www.google.com,url1),1), ((www.google.com,url2),2)))</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>结果：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">       processRDD.reduceByKey(_+_)</span><br><span class="line">          .groupBy(_._1._1)</span><br><span class="line">            .mapValues(x =&gt; &#123;</span><br><span class="line">              x.toList.sortBy(-_._2)  // toList是一个很大的安全隐患    </span><br><span class="line">                .map(x =&gt; (x._1._2, x._2)).take(TOPN)</span><br><span class="line">            &#125;).printInfo()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">(www.baidu.com,List((url5,5), (url4,4)))</span><br><span class="line">(www.twitter.com,List((url10,11), (url9,6)))</span><br><span class="line">(www.google.com,List((url6,7), (url8,7)))</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<p>  x.toList.sortBy(-<em>.</em>2)  // toList是一个很大的安全隐患，为什么这么说呢？<br>  x来了一亿条数据 list就炸掉了 所以这样 虽然能出结果 但是不能用<br>  如何规避掉呢 这块就使用rdd算子 不用scala的高级函数</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">如何解决呢？</span><br><span class="line">1.方法：</span><br><span class="line">	分而治之的思路</span><br><span class="line">		类似mapreduce 思想  一个文件 拆成多个inputsplits 每个split单独处理 之后reduce聚合</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    // 分而治之的思路</span><br><span class="line">        val sites = Array(&quot;www.baidu.com&quot;,&quot;www.google.com&quot;,&quot;www.twitter.com&quot;)</span><br><span class="line">        for(site &lt;- sites) &#123;</span><br><span class="line">          processRDD.filter(_._1._1 == site)</span><br><span class="line">            .reduceByKey(_+_).sortBy(-_._2)</span><br><span class="line">            .take(TOPN).foreach(println)</span><br><span class="line">        &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line"></span><br><span class="line">有什么问题？</span><br><span class="line">1.会产生好多job  去ui上看 </span><br><span class="line">2.val sites = Array(&quot;www.baidu.com&quot;,&quot;www.google.com&quot;,&quot;www.twitter.com&quot;) 不要这么写</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">方法2; 优化方法1</span><br><span class="line"></span><br><span class="line">object InterviewApp03 &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val TOPN = 2</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/site.log&quot;)</span><br><span class="line">    val processRDD = input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;,&quot;)</span><br><span class="line">      val site = splits(0)</span><br><span class="line">      val url = splits(1)</span><br><span class="line">      ((site, url), 1)</span><br><span class="line">    &#125;)</span><br><span class="line">    // 分而治之的思路</span><br><span class="line">    val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">    sites.map(x=&gt;&#123;</span><br><span class="line">      processRDD.filter(_._1._1 == x).reduceByKey(_+_).sortBy(-_._2) .take(TOPN).foreach(println)</span><br><span class="line">    &#125;)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">((www.baidu.com,url5),5)</span><br><span class="line">((www.baidu.com,url4),4)</span><br><span class="line">((www.twitter.com,url10),11)</span><br><span class="line">((www.twitter.com,url9),6)</span><br><span class="line">((www.google.com,url6),7)</span><br><span class="line">((www.google.com,url8),7)</span><br><span class="line"></span><br><span class="line">问题：</span><br><span class="line"> val sites = processRDD.map(_._1._1).distinct().collect()  // 数组</span><br><span class="line">生产上能直接collect么？不能 这代码怎么改进呢？</span><br></pre></td></tr></table></figure></div>

<h2 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">reduceByKey走的是 defaultPartitioner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = &#123;</span><br><span class="line">    val rdds = (Seq(rdd) ++ others)</span><br><span class="line">    val hasPartitioner = rdds.filter(_.partitioner.exists(_.numPartitions &gt; 0))</span><br><span class="line">    if (hasPartitioner.nonEmpty) &#123;</span><br><span class="line">      hasPartitioner.maxBy(_.partitions.length).partitioner.get</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      if (rdd.context.conf.contains(&quot;spark.default.parallelism&quot;)) &#123;</span><br><span class="line">        new HashPartitioner(rdd.context.defaultParallelism)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        new HashPartitioner(rdds.map(_.partitions.length).max)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">RDD的5大特性中有一条是 Partitioner</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024101322660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)</span><br><span class="line">      : RDD[(K, V)] = self.withScope</span><br><span class="line">  &#123;</span><br><span class="line">    val part = new RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">    new ShuffledRDD[K, V, V](self, part)</span><br><span class="line">      .setKeyOrdering(if (ascending) ordering else ordering.reverse)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">RangePartitioner：</span><br><span class="line"></span><br><span class="line">object PartitionerApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">  </span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,30,100,300,400,500),3)</span><br><span class="line">  //Kafka分区策略</span><br><span class="line">    data.zipWithIndex().sortByKey()</span><br><span class="line">      .mapPartitionsWithIndex((index, partition)=&gt;&#123;</span><br><span class="line">        partition.map(x=&gt;s&quot;分区是$index, 元素是$&#123;x._1&#125;&quot;)</span><br><span class="line">      &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是：</span><br><span class="line">分区是0, 元素是1</span><br><span class="line">分区是0, 元素是2</span><br><span class="line">分区是0, 元素是3</span><br><span class="line">分区是0, 元素是4</span><br><span class="line">分区是1, 元素是5</span><br><span class="line">分区是1, 元素是6</span><br><span class="line">分区是1, 元素是30</span><br><span class="line">分区是1, 元素是100</span><br><span class="line">分区是2, 元素是300</span><br><span class="line">分区是2, 元素是400</span><br><span class="line">分区是2, 元素是500</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def groupByKey(): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    groupByKey(defaultPartitioner(self))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">defaultPartitioner --》HashPartitioner</span><br><span class="line"></span><br><span class="line">object PartitionerApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">   </span><br><span class="line">    val data = sc.parallelize(List(1,2,3,4,5,6,30,100,300,400,500),3)</span><br><span class="line">    data.zipWithIndex().groupByKey()</span><br><span class="line">      .mapPartitionsWithIndex((index, partition)=&gt;&#123;</span><br><span class="line">        partition.map(x=&gt;s&quot;分区是$index, 元素是$&#123;x._1&#125;&quot;)</span><br><span class="line">      &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果是;</span><br><span class="line">分区是0, 元素是300</span><br><span class="line">分区是0, 元素是30</span><br><span class="line">分区是0, 元素是6</span><br><span class="line">分区是0, 元素是3</span><br><span class="line">分区是1, 元素是100</span><br><span class="line">分区是1, 元素是4</span><br><span class="line">分区是1, 元素是1</span><br><span class="line">分区是1, 元素是400</span><br><span class="line">分区是2, 元素是500</span><br><span class="line">分区是2, 元素是5</span><br><span class="line">分区是2, 元素是2</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>
<h2 id="多路径输出-定制化业务"><a href="#多路径输出-定制化业务" class="headerlink" title="多路径输出 ***定制化业务"></a>多路径输出 ***定制化业务</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">日志格式：</span><br><span class="line">uid1	Andriod	v3	215.197.96.120	4780	2019-09-11 08:37:33	9232	app19-20-18	香港			2019	09	11</span><br><span class="line">uid4	Symbain	71.10.97	168.170.39.193	189	2019-09-11 01:44:36	9232	app19-20-18	湖北	武汉	联通	2019	09	11</span><br><span class="line">uid2	linux	v2	168.170.39.193	189	2019-09-11 03:08:19	9232	app15-1-14-11	湖北	武汉	联通	2019	09	11</span><br><span class="line">uid3	linux	v2	168.170.39.193	189	2019-09-11 03:51:14	9232	app15-1-14-11	湖北	武汉	联通	2019	09	11</span><br><span class="line">uid6	mac	71.10.97	171.125.131.128	4780	2019-09-11 06:54:29	9232	app19-20-18	山西	忻州	联通	2019	09	11</span><br><span class="line">uid10	mac	71.10.97	171.125.131.128	4780	2019-09-11 07:09:27	189	app15-1-14-11	山西	忻州	联通	2019	09	11</span><br><span class="line"></span><br><span class="line">eg：给你一个完整的日志 做成 客户定制版  （可以去各大云平台cdn上查看产品）</span><br><span class="line">这个功能很重要 定制是收钱的（这个功能 公司一年收益是很多的 ）</span><br><span class="line"></span><br><span class="line">       1.假设按照不同的 品牌进行落盘</span><br><span class="line">       Andriod的数据都落在 Andriod的文件夹下</span><br><span class="line">       Symbain的数据都落在 Symbain的文件夹下</span><br><span class="line">      2. 输出的日志 客户想要什么字段的日志就输出什么字段的日志 （而不是把全部的日志都输出）</span><br></pre></td></tr></table></figure></div>

<p> <strong>1.假设按照不同的 品牌进行落盘</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输出的时候相当于根据某一个字段进行输出</span><br></pre></td></tr></table></figure></div>
<p>我们一步一步的来：</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    /**</span><br><span class="line">      * Android</span><br><span class="line">      *   xxxx.log</span><br><span class="line">      *</span><br><span class="line">      * iOS</span><br><span class="line">      *   xxx.log</span><br><span class="line">      */</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(8))</span><br><span class="line">      .saveAsTextFile(output)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">这个结果肯定是不对的 都放到一个目录下的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102412035911.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">所有日志到在一个文件夹下：</span><br><span class="line">part-00000：mac、linux、Symbain  的 数据</span><br><span class="line">part-00001：Andriod </span><br><span class="line">part-00002 ：空</span><br><span class="line">part-00003：空</span><br><span class="line">part-00004：windows</span><br><span class="line">	</span><br><span class="line">	不仅仅是日志到在一个文件夹下 而且有的同一个文件下有别的品牌的数据</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1.我们是要把某一个字段作为输出文件夹名 </span><br><span class="line">这就是多目录输出 </span><br><span class="line">2.mapreduce里面有这个类  MultipleTextOutputFormat</span><br><span class="line">所以我们自己实现一个类继承这个类就可以</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * This class extends the MultipleOutputFormat, allowing to write the output</span><br><span class="line"> * data to different output files in Text output format.</span><br><span class="line"> */</span><br><span class="line">@InterfaceAudience.Public</span><br><span class="line">@InterfaceStability.Stable</span><br><span class="line">public class MultipleTextOutputFormat&lt;K, V&gt;</span><br><span class="line">    extends MultipleOutputFormat&lt;K, V&gt; &#123;</span><br><span class="line"></span><br><span class="line">  private TextOutputFormat&lt;K, V&gt; theTextOutputFormat = null;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  protected RecordWriter&lt;K, V&gt; getBaseRecordWriter(FileSystem fs, JobConf job,</span><br><span class="line">      String name, Progressable arg3) throws IOException &#123;</span><br><span class="line">    if (theTextOutputFormat == null) &#123;</span><br><span class="line">      theTextOutputFormat = new TextOutputFormat&lt;K, V&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">    return theTextOutputFormat.getRecordWriter(fs, job, name, arg3);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class MyDataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    补充里面的实现方法</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024121310993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>没有自己想要的方法，继续看MultipleTextOutputFormat的父类<br><img src="https://img-blog.csdnimg.cn/20191024121544286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> class MyDataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;   </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line"> 1.</span><br><span class="line">   s&quot;$key/$name&quot;   </span><br><span class="line">   key和name是什么东西呢？一会debug看一下</span><br><span class="line">2.这个东西写好了之后怎么用呢？</span><br><span class="line"> 第一个测试代码基础上 就不能使用saveAsTextFile</span><br><span class="line"> 因为你要定向输出到某一个类里面去 需要设置FileOutputFormat</span><br><span class="line"></span><br><span class="line">这和在mapreduce里是一样的 </span><br><span class="line">这块就要使用 saveAsHadoopFile</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024122713780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class</span><br><span class="line"> * supporting the key and value types K and V in this RDD. Compress with the supplied codec.</span><br><span class="line"> */</span><br><span class="line">def saveAsHadoopFile(</span><br><span class="line">    path: String,</span><br><span class="line">    keyClass: Class[_],</span><br><span class="line">    valueClass: Class[_],</span><br><span class="line">    outputFormatClass: Class[_ &lt;: OutputFormat[_, _]],</span><br><span class="line">    codec: Class[_ &lt;: CompressionCodec]): Unit = self.withScope &#123;</span><br><span class="line">  saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass,</span><br><span class="line">    new JobConf(self.context.hadoopConfiguration), Some(codec))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],classOf[RuozedataMultipleTextOutputFormat])</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">查看结果：</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024123422488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191024123557180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>怎么才能去掉呢？<br><img src="https://img-blog.csdnimg.cn/2019102412383967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">知道了kv代表了什么。那么如何去掉文件里的多出那一列key值呢？</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102412411829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">默认generateActualKey 返回是最终输出的key  所以我们自定义的类里 重写这个方法 就ok了</span><br><span class="line">  /**</span><br><span class="line">   * Generate the actual key from the given key/value. The default behavior is that</span><br><span class="line">   * the actual key is equal to the given key</span><br><span class="line">   * </span><br><span class="line">   * @param key</span><br><span class="line">   *          the key of the output data</span><br><span class="line">   * @param value</span><br><span class="line">   *          the value of the output data</span><br><span class="line">   * @return the actual key derived from the given key/value</span><br><span class="line">   */</span><br><span class="line">  protected K generateActualKey(K key, V value) &#123;</span><br><span class="line">    return key;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">你mapreduce代码里 不输出值 用什么？使用NullWritable.get()   不可能使用 “” 或者 null 你可以测试一下使用它们输出是什么。</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">object MulitOutputApp &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val output = &quot;file:///C:/IdeaProjects/spark/out/mulit&quot;</span><br><span class="line">    val input = sc.textFile(&quot;file:///C:/IdeaProjects/spark/data/access.log&quot;)</span><br><span class="line">    input.map(x =&gt; &#123;</span><br><span class="line">      val splits = x.split(&quot;\t&quot;)</span><br><span class="line">      (splits(1), x) // (platform , 完整的日志)</span><br><span class="line">    &#125;).partitionBy(new HashPartitioner(5))</span><br><span class="line">      .saveAsHadoopFile(output,classOf[String],classOf[String],classOf[RuozedataMultipleTextOutputFormat])</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123;</span><br><span class="line">    override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123;</span><br><span class="line">      s&quot;$key/$name&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def generateActualKey(key: Any, value: Any): AnyRef = &#123;</span><br><span class="line">      NullWritable.get()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">查看结果</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191024124752801.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>基本功能实现完成 </p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark06-依赖关系" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/24/Spark06-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/">Spark06--依赖关系</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/24/Spark06-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/" class="article-date">
  <time datetime="2018-01-24T12:02:17.000Z" itemprop="datePublished">2018-01-24</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <h2 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rdd ==&gt; transformation s ==&gt; action</span><br><span class="line">就是rdd经过一系列的转换 最后触发action</span><br><span class="line">eg：</span><br><span class="line">textFile(path) ==&gt; map ==&gt; filter ==&gt; ...  ==&gt; collect</span><br><span class="line">每一步转换都会形成一个rdd </span><br><span class="line">RDDA   RDDB   RDDC</span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">一个rdd 三个分区 经过一个map之后 分区不会发生变化的 再filter 分区也是三个</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.你对rdd做一个map操作  其实是对rdd内部的所有数据做map操作  ----RDD篇</span><br><span class="line">2.窄依赖操作 默认不会造成分区的个数发生变化</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023093733887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>对于这个场景他们之间是有一个<strong>依赖关系</strong>的</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">注意：</span><br><span class="line">1.假如说 RDDB 分区里 6，8 这元素在计算的时候挂了</span><br><span class="line">那么spark再重新计算的时候 它只需要重新计算这一个分区就可以了</span><br><span class="line">2.这个分区里的数据怎么来的呢？</span><br><span class="line">直接从上一个rddA分区 里拿过来 计算就可以 其他分区不会做处理  所以这里面存在依赖关系的</span><br><span class="line">3.6和8这个元素的这个分区 到底从RDDA的哪一个分区过来的 </span><br><span class="line">这个是必然知道的 再spark里叫Lineage</span><br><span class="line">4.Lineage ： 一个rdd 是如何从父RDD计算的来的</span><br><span class="line">5.RDD里的五大特性的其中一个特性 是可以得到依赖关系的 </span><br><span class="line"></span><br><span class="line">eg：因为你每次transformation的时候会把这个依赖关系记录下来的   这样就知道父rdd是谁</span><br><span class="line">就是自己数据坏了 去爸爸那计算恢复 总有源头可以计算恢复 </span><br><span class="line">这个机制</span><br><span class="line">就是Spark性能高的一个非常重要的原因</span><br><span class="line"></span><br><span class="line">6. 性能 + 容错 （容错也体现在 数据坏了 重新算一下就ok）</span><br><span class="line">7. 整个过程就是一个计算链</span><br><span class="line">8. 如果转换非常多 </span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">	这一个链路 100个转换 算到第99个数据坏了 ，如果要重头算 也是挺麻烦的一件事 </span><br><span class="line">	core里面 提供 checkpoint（根本用不到 了解即可）</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023095648257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>(1)idea中debug是可以看到依赖关系的<br><img src="https://img-blog.csdnimg.cn/20191023101103284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>所以整个过程中 你的RDD是怎么来的 spark是知道的</p>
<p>(2) spark-shell中</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.map(_*2)</span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; val c = b.filter(_ &gt; 6)</span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: Array[Int] = Array(8, 10)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023101642501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023101727428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">那么这个过程中到底产生多少个RDD呢？</span><br><span class="line"></span><br><span class="line">scala&gt; c.collect</span><br><span class="line">res0: Array[Int] = Array(8, 10)</span><br><span class="line"></span><br><span class="line">scala&gt; c.toDebugString</span><br><span class="line">res1: String =</span><br><span class="line">(2) MapPartitionsRDD[2] at filter at &lt;console&gt;:25 []</span><br><span class="line"> |  MapPartitionsRDD[1] at map at &lt;console&gt;:25 []</span><br><span class="line"> |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">parallelize  --》ParallelCollectionRDD</span><br><span class="line">map  --》MapPartitionsRDD</span><br><span class="line">filter  ---》MapPartitionsRDD</span><br><span class="line"></span><br><span class="line">那么这几个东西哪里来的呢？看源码</span><br><span class="line"></span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">parallelize 返回的是一个RDD 而真正的类型是 ParallelCollectionRDD 其他同理</span><br></pre></td></tr></table></figure></div>
<h2 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile() 这一个过程产生多少个RDD呢？</span><br><span class="line"></span><br><span class="line">scala&gt; sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).collect()</span><br><span class="line">res2: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">结果出来了 到页面上看一下。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102310260984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">这个过程产生了多少rdd呢？</span><br><span class="line">scala&gt; sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1)).reduceByKey(_+_).toDebugString</span><br><span class="line">res3: String =</span><br><span class="line">(2) ShuffledRDD[12] at reduceByKey at &lt;console&gt;:25 []</span><br><span class="line"> +-(2) MapPartitionsRDD[11] at map at &lt;console&gt;:25 []</span><br><span class="line">    |  MapPartitionsRDD[10] at flatMap at &lt;console&gt;:25 []</span><br><span class="line">    |  file:///home/double_happy/data/double_happy.txt MapPartitionsRDD[9] at textFile at &lt;console&gt;:25 []</span><br><span class="line">    |  file:///home/double_happy/data/double_happy.txt HadoopRDD[8] at textFile at &lt;console&gt;:25 []</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">textFile ：  HadoopRDD +  MapPartitionsRDD</span><br><span class="line">flatMap  ： MapPartitionsRDD</span><br><span class="line">map  ： MapPartitionsRDD</span><br><span class="line">reduceByKey  ： ShuffledRDD</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">textFile  过程：</span><br><span class="line"></span><br><span class="line">1.textFile</span><br><span class="line"> def textFile(</span><br><span class="line">      path: String,</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">2.hadoopFile</span><br><span class="line"> def hadoopFile[K, V](</span><br><span class="line">      path: String,</span><br><span class="line">      inputFormatClass: Class[_ &lt;: InputFormat[K, V]],</span><br><span class="line">      keyClass: Class[K],</span><br><span class="line">      valueClass: Class[V],</span><br><span class="line">      minPartitions: Int = defaultMinPartitions): RDD[(K, V)]</span><br><span class="line"></span><br><span class="line">hadoopFile 我没用拷贝全 但是足够了 返回值是一个 kv类型的   真正的返回的是HadoopRDD</span><br><span class="line"></span><br><span class="line">3.hadoopFile 就是 mapreduce里的 读取文本文件的mapper过程</span><br><span class="line"></span><br><span class="line">通过：mapper</span><br><span class="line">TextInputFormat</span><br><span class="line">mapper: LongWritable（每行数据的偏移量）  Text(每行数据的内容)</span><br><span class="line">那么 RDD[(K, V) 就是 （偏移量，Text）</span><br><span class="line"></span><br><span class="line">4.</span><br><span class="line">    hadoopFile(path, classOf[TextInputFormat],</span><br><span class="line">     classOf[LongWritable], classOf[Text],</span><br><span class="line">      minPartitions)</span><br><span class="line">      .map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">      </span><br><span class="line">这个HadoopRDD 之后的map操作 所以会产生MapPartitionsRDD</span><br><span class="line">map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">就是把读取的内容拿出来</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">	rdd里的分区数据对应hdfs上一个文件有多少个block</span><br><span class="line"></span><br><span class="line">所以HadoopRDD底层实现可以去看一下 </span><br><span class="line"></span><br><span class="line"> override def getPartitions: Array[Partition] = &#123;</span><br><span class="line">    val jobConf = getJobConf()</span><br><span class="line">    // add the credentials here as this can be called before SparkContext initialized</span><br><span class="line">    SparkHadoopUtil.get.addCredentials(jobConf)</span><br><span class="line">    val inputFormat = getInputFormat(jobConf)</span><br><span class="line">    val inputSplits = inputFormat.getSplits(jobConf, minPartitions)</span><br><span class="line">    val array = new Array[Partition](inputSplits.size)</span><br><span class="line">    for (i &lt;- 0 until inputSplits.size) &#123;</span><br><span class="line">      array(i) = new HadoopPartition(id, i, inputSplits(i))</span><br><span class="line">    &#125;</span><br><span class="line">    array</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">getInputFormat(jobConf).getSplits(jobConf, minPartitions) 明白了吗</span><br></pre></td></tr></table></figure></div>
<h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">窄依赖</span><br><span class="line">       一个父RDD的partition至多被子RDD的partition使用一次</span><br><span class="line">       OneToOneDependency</span><br><span class="line">       都在一个stage中完成</span><br><span class="line">宽依赖   &lt;= 会产生shuffle 会有新的stage</span><br><span class="line">       一个父RDD的partition会被子RDD的partition使用多次</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/20191023111033655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105631974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105652721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023105850276.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20191023110813166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果经过宽依赖之后的RDD的某一个分区数据挂掉</span><br><span class="line">需要去父RDD重新计算 会把父亲所有分区都会算一下才行 </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">1.所有分区都要重算</span><br><span class="line">从容错的角度来说，在开发过程，能使用窄依赖就使用窄依赖 emm这就话 不全对</span><br><span class="line">在某些情况下 会把窄依赖改成宽依赖 来实现。</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023111336288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="解析wc过程"><a href="#解析wc过程" class="headerlink" title="解析wc过程"></a>解析wc过程</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;)</span><br><span class="line"> val words = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line"> val pair = words.map((_,1))</span><br><span class="line"> val result = pair.reduceByKey(_+_)</span><br><span class="line">   result.collect()</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023114118106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">为什么会多一个conbine操作呢？</span><br><span class="line">reduceBykey算子底层封装好的</span><br><span class="line"></span><br><span class="line">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123;</span><br><span class="line">    combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> def combineByKeyWithClassTag[C](</span><br><span class="line">      createCombiner: V =&gt; C,</span><br><span class="line">      mergeValue: (C, V) =&gt; C,</span><br><span class="line">      mergeCombiners: (C, C) =&gt; C,</span><br><span class="line">      partitioner: Partitioner,</span><br><span class="line">      mapSideCombine: Boolean = true,</span><br><span class="line">      serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)] = self.withScope &#123;</span><br><span class="line">    require(mergeCombiners != null, &quot;mergeCombiners must be defined&quot;) // required as of Spark 0.9.0</span><br><span class="line">    if (keyClass.isArray) &#123;</span><br><span class="line">      if (mapSideCombine) &#123;</span><br><span class="line">        throw new SparkException(&quot;Cannot use map-side combining with array keys.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      if (partitioner.isInstanceOf[HashPartitioner]) &#123;</span><br><span class="line">        throw new SparkException(&quot;HashPartitioner cannot partition array keys.&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    val aggregator = new Aggregator[K, V, C](</span><br><span class="line">      self.context.clean(createCombiner),</span><br><span class="line">      self.context.clean(mergeValue),</span><br><span class="line">      self.context.clean(mergeCombiners))</span><br><span class="line">    if (self.partitioner == Some(partitioner)) &#123;</span><br><span class="line">      self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        val context = TaskContext.get()</span><br><span class="line">        new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">      &#125;, preservesPartitioning = true)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new ShuffledRDD[K, V, C](self, partitioner)</span><br><span class="line">        .setSerializer(serializer)</span><br><span class="line">        .setAggregator(aggregator)</span><br><span class="line">        .setMapSideCombine(mapSideCombine)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">combineByKeyWithClassTag中的 mapSideCombine: Boolean = true</span><br><span class="line"></span><br><span class="line">map端输出 设置Combine为true </span><br><span class="line"></span><br><span class="line">reduceBykey这个算子有Combine，那么groupBykey算子有么？</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123;</span><br><span class="line">    // groupByKey shouldn&apos;t use map side combine because map side combine does not</span><br><span class="line">    // reduce the amount of data shuffled and requires all map side data be inserted</span><br><span class="line">    // into a hash table, leading to more objects in the old gen.</span><br><span class="line">    val createCombiner = (v: V) =&gt; CompactBuffer(v)</span><br><span class="line">    val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v</span><br><span class="line">    val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2</span><br><span class="line">    val bufs = combineByKeyWithClassTag[CompactBuffer[V]](</span><br><span class="line">      createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)</span><br><span class="line">    bufs.asInstanceOf[RDD[(K, Iterable[V])]]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mapSideCombine = false </span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">reduceByKey</span><br><span class="line">	有map端输出预聚合功能的</span><br><span class="line">groupBykey</span><br><span class="line">	全数据shuffle的 ，没有预聚合</span><br></pre></td></tr></table></figure></div>
<h2 id="shuffle-operations"><a href="#shuffle-operations" class="headerlink" title="shuffle operations"></a>shuffle operations</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations" target="_blank" rel="noopener">shuffle operations</a></p>
<p>The shuffle is Spark’s mechanism for <strong>re-distributing data</strong> so that it’s grouped differently across partitions. This typically involves <strong>copying data across executors and machines</strong>, making the shuffle a complex and costly operation.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re-distributing data：</span><br><span class="line">数据重新分区 --就是shuffle过程  我画的wc那个图</span><br></pre></td></tr></table></figure></div>
<p>Operations which can cause a shuffle include repartition operations like repartition and coalesce, ‘ByKey operations (except for counting) like groupByKey and reduceByKey, and join operations like cogroup and join.</p>
<p>上面这句话是不严谨的 之后测试证实。</p>
<p><strong>The Shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O.</strong> </p>
<p>这块官网好好读读</p>
<h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">    val lines = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;)</span><br><span class="line">    val words = lines.flatMap(_.split(&quot;,&quot;))</span><br><span class="line">    val pair = words.map((_,1))</span><br><span class="line">    </span><br><span class="line">   val result2 = pair.groupByKey()</span><br><span class="line">    val result1 = pair.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">假设pair之后还有其他的业务逻辑</span><br><span class="line">这里是：</span><br><span class="line">	groupByKey</span><br><span class="line">	reduceByKey</span><br><span class="line"></span><br><span class="line">到pair为止 大家都是公用的 这块就有必要使用cache机制 </span><br><span class="line"></span><br><span class="line">如果不做这个操作和做了 有什么区别呢？</span><br></pre></td></tr></table></figure></div>
<p>（1）没有做cache测试</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[16] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res4: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br><span class="line">查看4040页面</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023145325933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再执行一遍 re.collect 页面还是这样的 </p>
<p>（2）做cache</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res0: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res1: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.cache</span><br><span class="line">res2: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; </span><br><span class="line">查看页面</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023145545411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>再执行一边</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/2019102314564550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>会发现执行了两次 就根本不是一个东西了 ，做了cache已经把我们的东西持久化到默认的存储级别里去了，下次就会去缓存里读取数据了</strong><br><img src="https://img-blog.csdnimg.cn/20191023145840158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.不做cache 如果你对同一个操作执行多次 下一次会从头开始执行</span><br><span class="line">2.如果做了cache （lazy 的操作并不会触发）</span><br><span class="line">3.cache后 默认的存储级别后 为什么数据量会变大了呢？</span><br><span class="line">	之后再说。</span><br></pre></td></tr></table></figure></div>
<h2 id="persist和cache的区别"><a href="#persist和cache的区别" class="headerlink" title="persist和cache的区别"></a>persist和cache的区别</h2><p>You can mark an RDD to be persisted using the persist() or cache() methods on it. <strong>The first time it is computed in an action,</strong> it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.<br> If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the <strong>RDD.unpersist() method.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The first time it is computed in an action </span><br><span class="line">就是说 sparkcore里的</span><br><span class="line">persist、cache 执行是在遇到action算子 才触发</span><br><span class="line"></span><br><span class="line">在迭代次数比较多的场景下 使用</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val re = sc.textFile(&quot;file:///home/double_happy/data/double_happy.txt&quot;).flatMap(_.split(&quot;,&quot;)).map((_,1))</span><br><span class="line">re: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res0: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res1: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.cache</span><br><span class="line">res2: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res3: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.reduceByKey(_+_).collect</span><br><span class="line">res4: Array[(String, Int)] = Array((xx,2), (aa,1), (flink,1), (jj,1), (rr,1), (spark,1), (flume,1), (ruozedata,1), (nihao,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.unpersist()</span><br><span class="line">res5: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023150726195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cache 、 persist   是 lazy的 </span><br><span class="line">2.unpersist  是 eager的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br><span class="line">   */</span><br><span class="line">  def cache(): this.type = persist()</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">object StorageLevel &#123;</span><br><span class="line">  val NONE = new StorageLevel(false, false, false, false)</span><br><span class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</span><br><span class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</span><br><span class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</span><br><span class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</span><br><span class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</span><br><span class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</span><br><span class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</span><br><span class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</span><br><span class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</span><br><span class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</span><br><span class="line">  val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</span><br><span class="line"></span><br><span class="line">class StorageLevel private(</span><br><span class="line">    private var _useDisk: Boolean,</span><br><span class="line">    private var _useMemory: Boolean,</span><br><span class="line">    private var _useOffHeap: Boolean,</span><br><span class="line">    private var _deserialized: Boolean,    // _deserialized 不序列化</span><br><span class="line">    private var _replication: Int = 1)</span><br><span class="line">  extends Externalizable</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.cache</span><br><span class="line">    ==&gt; persist</span><br><span class="line">        ==&gt; persist(MEMORY_ONLY)</span><br><span class="line">2.cache、persist 默认都走的是MEMORY_ONLY</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023151458914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>换一种存储级别<br><img src="https://img-blog.csdnimg.cn/20191023152323222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>数据是不是小了</strong> 序列化的会节省空间</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; re.collect</span><br><span class="line">res7: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt; re.unpersist()</span><br><span class="line">res8: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; import org.apache.spark.storage.StorageLevel</span><br><span class="line">import org.apache.spark.storage.StorageLevel</span><br><span class="line"></span><br><span class="line">scala&gt; re.persist(StorageLevel.MEMORY_ONLY_SER)</span><br><span class="line">res9: re.type = MapPartitionsRDD[3] at map at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; re.collect</span><br><span class="line">res10: Array[(String, Int)] = Array((spark,1), (flink,1), (flume,1), (nihao,1), (ruozedata,1), (xx,1), (xx,1), (jj,1), (rr,1), (aa,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>

<p><strong>Which Storage Level to Choose?</strong><br><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#which-storage-level-to-choose" target="_blank" rel="noopener">Which Storage Level to Choose?</a></p>
<p>一定要做序列化么？这和压缩是一个道理<br><strong>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency.</strong></p>
<h2 id="coalesce和repartition"><a href="#coalesce和repartition" class="headerlink" title="coalesce和repartition"></a>coalesce和repartition</h2><p>repartition</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> /**</span><br><span class="line">   * Return a new RDD that has exactly numPartitions partitions.</span><br><span class="line">   *</span><br><span class="line">   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses</span><br><span class="line">   * a shuffle to redistribute data.</span><br><span class="line">   *</span><br><span class="line">   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,</span><br><span class="line">   * which can avoid performing a shuffle.</span><br><span class="line">   */</span><br><span class="line">  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123;</span><br><span class="line">    coalesce(numPartitions, shuffle = true)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Can increase or decrease the level of parallelism in this RDD</span><br><span class="line"> shuffle = true</span><br><span class="line">就是</span><br><span class="line">repartition 无论增大还是减少 分区数 它都走shuffle</span><br><span class="line"></span><br><span class="line">增大分区数 我们使用repartition  </span><br><span class="line">repartition 底层调用coalesce</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">package com.ruozedata.spark.spark03</span><br><span class="line"></span><br><span class="line">import com.ruozedata.spark.homework.utils.ContextUtils</span><br><span class="line">import com.ruozedata.spark.homework.utils.ImplicitAspect._</span><br><span class="line">object CoalesceAndRepartitionApp &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val sc = ContextUtils.getSparkContext(this.getClass.getSimpleName)</span><br><span class="line">    val data = sc.parallelize(List(1 to 9: _*),3)</span><br><span class="line"></span><br><span class="line">    data.mapPartitionsWithIndex((index,partition) =&gt; &#123;</span><br><span class="line">      partition.map(x=&gt;s&quot;分区是$index,元素是$x&quot;)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    val repartitionRDD = data.repartition(4)</span><br><span class="line">    repartitionRDD.mapPartitionsWithIndex((index,partition) =&gt; &#123;</span><br><span class="line">      partition.map(x=&gt;s&quot;分区是$index,元素是$x&quot;)</span><br><span class="line">    &#125;).printInfo()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">分区是1,元素是4</span><br><span class="line">分区是0,元素是1</span><br><span class="line">分区是1,元素是5</span><br><span class="line">分区是1,元素是6</span><br><span class="line">分区是0,元素是2</span><br><span class="line">分区是0,元素是3</span><br><span class="line">分区是2,元素是7</span><br><span class="line">分区是2,元素是8</span><br><span class="line">分区是2,元素是9</span><br><span class="line">-------------------------</span><br><span class="line">分区是1,元素是3</span><br><span class="line">分区是0,元素是2</span><br><span class="line">分区是1,元素是6</span><br><span class="line">分区是0,元素是5</span><br><span class="line">分区是1,元素是9</span><br><span class="line">分区是0,元素是8</span><br><span class="line">分区是3,元素是1</span><br><span class="line">分区是3,元素是4</span><br><span class="line">分区是3,元素是7</span><br><span class="line">-------------------------</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data = sc.parallelize(List(1 to 9: _*),3)</span><br><span class="line">data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; data.repartition(4)</span><br><span class="line">res11: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[10] at repartition at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; data.repartition(4).collect</span><br><span class="line">res12: Array[Int] = Array(2, 7, 3, 4, 8, 5, 9, 1, 6)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191023154639524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.能够说明 repartition是走shuffle的</span><br></pre></td></tr></table></figure></div>

<p><strong>coalesce</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">   * Return a new RDD that is reduced into `numPartitions` partitions.</span><br><span class="line">   *</span><br><span class="line">   * This results in a narrow dependency, e.g. if you go from 1000 partitions</span><br><span class="line">   * to 100 partitions, there will not be a shuffle, instead each of the 100</span><br><span class="line">   * new partitions will claim 10 of the current partitions. If a larger number</span><br><span class="line">   * of partitions is requested, it will stay at the current number of partitions.</span><br><span class="line">   *</span><br><span class="line">   * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1,</span><br><span class="line">   * this may result in your computation taking place on fewer nodes than</span><br><span class="line">   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span><br><span class="line">   * you can pass shuffle = true. This will add a shuffle step, but means the</span><br><span class="line">   * current upstream partitions will be executed in parallel (per whatever</span><br><span class="line">   * the current partitioning is).</span><br><span class="line">   *</span><br><span class="line">   * @note With shuffle = true, you can actually coalesce to a larger number</span><br><span class="line">   * of partitions. This is useful if you have a small number of partitions,</span><br><span class="line">   * say 100, potentially with a few partitions being abnormally large. Calling</span><br><span class="line">   * coalesce(1000, shuffle = true) will result in 1000 partitions with the</span><br><span class="line">   * data distributed using a hash partitioner. The optional partition coalescer</span><br><span class="line">   * passed in must be serializable.</span><br><span class="line">   */</span><br><span class="line">  def coalesce(numPartitions: Int, shuffle: Boolean = false,</span><br><span class="line">               partitionCoalescer: Option[PartitionCoalescer] = Option.empty)</span><br><span class="line">              (implicit ord: Ordering[T] = null)</span><br><span class="line">      : RDD[T] = withScope &#123;</span><br><span class="line">    require(numPartitions &gt; 0, s&quot;Number of partitions ($numPartitions) must be positive.&quot;)</span><br><span class="line">    if (shuffle) &#123;</span><br><span class="line">      /** Distributes elements evenly across output partitions, starting from a random partition. */</span><br><span class="line">      val distributePartition = (index: Int, items: Iterator[T]) =&gt; &#123;</span><br><span class="line">        var position = (new Random(index)).nextInt(numPartitions)</span><br><span class="line">        items.map &#123; t =&gt;</span><br><span class="line">          // Note that the hash code of the key will just be the key itself. The HashPartitioner</span><br><span class="line">          // will mod it with the number of total partitions.</span><br><span class="line">          position = position + 1</span><br><span class="line">          (position, t)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; : Iterator[(Int, T)]</span><br><span class="line"></span><br><span class="line">      // include a shuffle step so that our upstream tasks are still distributed</span><br><span class="line">      new CoalescedRDD(</span><br><span class="line">        new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        new HashPartitioner(numPartitions)),</span><br><span class="line">        numPartitions,</span><br><span class="line">        partitionCoalescer).values</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new CoalescedRDD(this, numPartitions, partitionCoalescer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">Return a new RDD that is reduced into `numPartitions` partitions.</span><br><span class="line">1.coalesce 用来 reduced numPartitions </span><br><span class="line"></span><br><span class="line">shuffle: Boolean = false,</span><br><span class="line"></span><br><span class="line">2。coalesce 默认是不走shuffle的</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; data.partitions.size</span><br><span class="line">res15: Int = 3</span><br><span class="line"></span><br><span class="line">scala&gt; data.coalesce(4).partitions.size</span><br><span class="line">res16: Int = 3</span><br><span class="line"></span><br><span class="line">scala&gt; data.coalesce(2).partitions.size</span><br><span class="line">res17: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt;data.coalesce(4,true).partitions.size    //这样就走shuffle啦</span><br><span class="line">res18: Int = 4</span><br></pre></td></tr></table></figure></div>

<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">repartition调用的是coalesce算子，shuffle默认为true    会产生新的 stage</span><br><span class="line">coalesce  shuffle默认为false    传shuffle为true，就和repartition一样</span><br></pre></td></tr></table></figure></div>
<p>Operations which can cause a shuffle include repartition operations like repartition and coalesce, </p>
<p>所以这句话 coalesce 默认是不会产生shuffle的 官网这话不严谨。</p>

      
    </div>
    
  </div>
  
  
</article>

  
    <article id="post-Spark005-核心架构" class="article global-container article-type-post" itemscope itemprop="blogPost">
  
    <header class="article-header">
      
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/23/Spark005-%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84/">Spark005--核心架构</a>
    </h1>
  

    </header>
  
  <div class="article-meta">
    <a href="/2018/01/23/Spark005-%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84/" class="article-date">
  <time datetime="2018-01-23T12:01:31.000Z" itemprop="datePublished">2018-01-23</time>
</a>
    
    
  </div>
  

  <div class="article-inner">
    
    <div class="article-content article-content-cloud" itemprop="articleBody">
      
        <p><a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">spark核心架构官网</a></p>
<h2 id="核心术语Glossary"><a href="#核心术语Glossary" class="headerlink" title="核心术语Glossary"></a>核心术语Glossary</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">核心术语</span><br><span class="line">    Application *****</span><br><span class="line">        a driver program</span><br><span class="line">        executors on the cluster.</span><br><span class="line">    Application jar</span><br><span class="line">    Driver program  *****</span><br><span class="line">        main  </span><br><span class="line">        sc </span><br><span class="line">    Cluster manager</span><br><span class="line">    Deploy mode</span><br><span class="line">        YARN: RM NM(container)</span><br><span class="line">            cluster: Driver是跑在container</span><br><span class="line">            client：Driver就运行在你提交机器的本地</span><br><span class="line">                client是不是一定要是集群内的？gateway</span><br><span class="line">    Worker node</span><br><span class="line">    Executor  *****</span><br><span class="line">        process</span><br><span class="line">        runs tasks</span><br><span class="line">        keeps data in memory or disk storage across them</span><br><span class="line">        Each application has its own executors.</span><br><span class="line">            A：executor1 2 3</span><br><span class="line">            B：executor1 2 3</span><br><span class="line">    Task	*****</span><br><span class="line">        A unit of work that will be sent to one executor</span><br><span class="line">        RDD: partitions == task</span><br><span class="line">    Job    *****</span><br><span class="line">        action ==&gt; job</span><br><span class="line">    Stage</span><br><span class="line"></span><br><span class="line">eg：</span><br><span class="line">spark-shell  应用程序</span><br><span class="line">一个application：1到n个job</span><br><span class="line">一个job ： 1到n个stage构成</span><br><span class="line">一个stage： 1到n个task  task与partition一一对应</span><br></pre></td></tr></table></figure></div>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>User program built on Spark. Consists of a driver program and executors on the cluster.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.User program built on Spark</span><br><span class="line">2.a driver program</span><br><span class="line">3. executors on the cluster.</span><br></pre></td></tr></table></figure></div>
<h2 id="Application-jar"><a href="#Application-jar" class="headerlink" title="Application jar"></a>Application jar</h2><p>A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. <strong>The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</strong></p>
<h2 id="Driver-program"><a href="#Driver-program" class="headerlink" title="Driver program"></a>Driver program</h2><p>The process running the main() function of the application and creating the SparkContext</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. running the main() function of the application </span><br><span class="line">2.  creating the SparkContext</span><br></pre></td></tr></table></figure></div>
<h2 id="Cluster-manager"><a href="#Cluster-manager" class="headerlink" title="Cluster manager"></a>Cluster manager</h2><p>An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</p>
<p>所以你的代码里 上传到集群的时候 不要写 死  appname 和 master<br>到时候通过 spark-submit 来指定就 可以选择 外部的cluster </p>
<h2 id="Deploy-mode"><a href="#Deploy-mode" class="headerlink" title="Deploy mode"></a>Deploy mode</h2><p><strong>Distinguishes where the driver process runs.</strong> In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.</p>
<p>一切以yarn为主</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YARN: RM NM(container)</span><br><span class="line">           cluster: Driver是跑在container  （跑在NM 里的container的 ）</span><br><span class="line">           client：Driver就运行在你提交机器的本地</span><br><span class="line">               client是不是一定要是集群内的？gateway</span><br></pre></td></tr></table></figure></div>
<h2 id="Worker-node"><a href="#Worker-node" class="headerlink" title="Worker node"></a>Worker node</h2><p>Any node that can run application code in the cluster</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yarn模式下 就是 nm </span><br><span class="line"></span><br><span class="line">run application code</span><br></pre></td></tr></table></figure></div>
<h2 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h2><p>a process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</p>
<p>一个进程 jps 就能看到<br><strong>对应yarn上 就跑在 nm的container里的</strong> </p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 1.  runs tasks</span><br><span class="line">2.  keeps data in memory or disk storage across them</span><br><span class="line">3. Each application has its own executors.</span><br><span class="line">            A：executor1 2 3</span><br><span class="line">            B：executor1 2 3</span><br><span class="line">这6个东西 是跑在container里的</span><br></pre></td></tr></table></figure></div>
<h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>A unit of work that will be sent to one executor</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.task 是发送到 executor里的</span><br><span class="line">2.RDD: partitions == tasks</span><br><span class="line">RDD是由多个partition所构成的，</span><br><span class="line">一个partition就对应一个task</span><br></pre></td></tr></table></figure></div>
<h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.一个action算子对应一个job</span><br></pre></td></tr></table></figure></div>
<h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><p>Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.一组task的集合 叫一个stage</span><br><span class="line">2.遇到一个shuffle算子  就会被拆成两个stage </span><br><span class="line"></span><br><span class="line">那遇到两个shuffle算子 会被拆成几个stage呢？ 3个哈  </span><br><span class="line">（跟一个桌子切掉一个桌角 还有几个角一样的哈 你别把桌子对角切就行 ）</span><br></pre></td></tr></table></figure></div>
<h2 id="总结案例"><a href="#总结案例" class="headerlink" title="总结案例"></a>总结案例</h2><div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eg：</span><br><span class="line">spark-shell  应用程序</span><br><span class="line">一个application：1到n个job</span><br><span class="line">一个job ： 1到n个stage构成</span><br><span class="line">一个stage： 1到n个task  task与partition一一对应</span><br></pre></td></tr></table></figure></div>

<h2 id="Components"><a href="#Components" class="headerlink" title="Components  ***"></a>Components  ***</h2><p>Spark applications run as independent sets of processes on a cluster<br> <strong>independent sets of processes：就是executor</strong></p>
<p>Spark applications run as <strong>independent sets of processes on a cluster</strong>, coordinated by the SparkContext object in your main program (called the driver program).</p>
<p>Specifically, to run on a cluster, <strong>the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications.</strong> <strong>Once connected, Spark acquires executors on nodes in the cluster,</strong> <strong>which are processes that run computations and store data for your application.</strong> Next<strong>, <strong>it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors</strong></strong>. Finally, <strong>SparkContext sends tasks to the executors to run.</strong></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. which allocate resources across applications.  which  ===》  cluster managers</span><br><span class="line">2.  Once connected, Spark acquires executors on nodes in the cluster</span><br><span class="line">     一旦连接上 spark 就yarn集群的 nm 的contatiner 里启动executor</span><br><span class="line">3.which are processes that run computations and store data for your application ：  </span><br><span class="line">        which  ===》executor</span><br><span class="line">4.it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors </span><br><span class="line">    it ==》 sc</span><br></pre></td></tr></table></figure></div>

<p><img src="https://img-blog.csdnimg.cn/2019102214595656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.driver ： main方法里有个 sc</span><br></pre></td></tr></table></figure></div>



<p>There are several useful things to note about this architecture:</p>
<p>1.<strong>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.</strong> <strong>This has the benefit of isolating applications from each other,</strong> <strong>on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs).</strong> However, <strong>it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system</strong>.<br>2.Spark is agnostic to the underlying cluster manager. <strong>As long as it can acquire executor processes, and these communicate with each other</strong>, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. which stay up for the duration of the whole application and run tasks in multiple threads </span><br><span class="line">		which  ==》executor processes</span><br><span class="line">		1.stay up for the duration of the whole application</span><br><span class="line">		2.run tasks in multiple threads</span><br><span class="line">2.This has the benefit of isolating applications from each other</span><br><span class="line">		Each application gets its own executor processes 就是每个application有自己独立的 executor processes</span><br><span class="line">		带来的好处就是 applications之间是隔离的</span><br><span class="line">3.cannot be shared across different Spark applications</span><br><span class="line">	without writing it to an external storage system    ===》Alluxio这个框架 现在可以实现 多个application之间共享</span><br><span class="line"></span><br><span class="line">4. agnostic  不关注</span><br><span class="line">    As long as it can acquire executor processes, and these communicate with each other</span><br><span class="line">    指的是 driver 和 executor之间的通信</span><br></pre></td></tr></table></figure></div>


<p>3.<strong>The driver program must listen for and accept incoming connections from its executors throughout its lifetime</strong> (e.g., see spark.driver.port in the network config section). As such, the driver program must be <strong>network addressable</strong> from the worker nodes.<br>4.Because the driver schedules tasks on the cluster, <strong>it should be run close to the worker nodes</strong>, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from <strong>nearby than to run a driver far away from the worker nodes</strong>.</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.The driver program must listen for and accept incoming connections from its executors throughout its lifetime</span><br><span class="line"> driver一定要知道你的executor在哪台机器上的</span><br></pre></td></tr></table></figure></div>
<p><img src="https://img-blog.csdnimg.cn/20191022152821973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9kb3VibGVoYXBweS5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这块可以看到 executor 在那台机器的哪个端口上</p>
<div class="highlight-box"autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" contenteditable="true"data-rel="PLAIN"><figure class="iseeu highlight /plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2.As such, the driver program must be network addressable from the worker nodes. </span><br><span class="line"></span><br><span class="line">所以yarn的client模式下 </span><br><span class="line">必须保证这台本地的机器能与yarn通了 ，client是不是一定要是集群内的？不一定的哈 能连上yarn就可以  </span><br><span class="line">     最开始的文章 gateway什么意思呢？你亲我一下我就告诉你 （这是对未来女朋友说的）</span><br><span class="line"></span><br><span class="line">3.driver进程最好离executor进程 近一点</span><br></pre></td></tr></table></figure></div>
      
    </div>
    
  </div>
  
  
</article>

  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>


</section>
        <aside id="sidebar">
  
    <div class="widget-box">
  <div class="avatar-box">
    <img class="avatar" src="/images/default-avatar.jpg" title="图片来自网络"></img>
    <h3 class="avatar-name">
      
        DoubleHappy
      
    </h3>
    <p class="avatar-slogan">
      特别耐撕的大数据，资深的打酱油攻城狮。
    </p>
  </div>
</div>


  
    

  
    

  
    
  
    
  <div class="widget-box">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li></ul>
    </div>
  </div>

  
    
  <div class="widget-box">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/01/05/Azkaban%E8%B0%83%E5%BA%A6-double-happy/">Azkaban调度--double_happy</a>
          </li>
        
          <li>
            <a href="/2019/01/04/Zookeeper%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E4%B8%8E%E7%9B%91%E6%8E%A7-Curator/">Zookeeper基本使用与监控(Curator)</a>
          </li>
        
          <li>
            <a href="/2018/04/17/SparkSQL-TextFile%E8%BE%93%E5%87%BA%E5%A4%9A%E5%88%97/">SparkSQL--TextFile输出多列</a>
          </li>
        
          <li>
            <a href="/2018/03/17/%E9%9B%85%E6%81%A9%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98-double-happy/">雅恩资源调优---double_happy</a>
          </li>
        
          <li>
            <a href="/2018/02/22/SS04/">SS04</a>
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-box">
    <h3 class="widget-title">友链</h3>
    <div class="widget">
      
        <a style="display: block;" href="https://sxwanggit126.github.io/" title target='_blank'
        >一路眼瞎</a>
      
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  <div class="foot-box global-width">
    &copy; 2019 DoubleHappy &nbsp;&nbsp;
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    &nbsp;|&nbsp;主题 <a href="https://github.com/yiluyanxia/hexo-theme-antiquity" target="_blank" rel="noopener">antiquity</a>
    <br>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">不蒜子告之   阁下是第<span id="busuanzi_value_site_pv"></span>个访客</span>
  </div>
</footer>
      <script src="https://code.jquery.com/jquery-2.0.3.min.js"></script>
<script>
if (!window.jQuery) {
var script = document.createElement('script');
script.src = "/js/jquery-2.0.3.min.js";
document.body.write(script);
}
</script>

  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



    </div>
    <nav id="mobile-nav" class="mobile-nav-box">
  <div class="mobile-nav-img mobile-nav-top"></div>
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
  <div class="mobile-nav-img  mobile-nav-bottom"></div>
</nav>    
  </div>
</body>
</html>